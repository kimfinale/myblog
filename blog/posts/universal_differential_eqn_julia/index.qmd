---
title: "Universal differential equation using Julia"
author: "Jong-Hoon Kim"
date: "2024-01-12"
categories: [universal differential equation, julia, sub-exponential growth]
image: "ude_obs.png"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE) 
```

### Universal differential equation (UDE)

The [UDE](https://docs.sciml.ai/Overview/stable/showcase/missing_physics/) refers to an approach to embed the machine learning into differential equations. The resulting UDE has some parts of the equation replaced by **universal approximators** i.e., neural network (NN). The UDE model approach allows us to approximate a wide, if not infinite, variety of functional relationships. As an example, I will test how well the UDE model approach will approximate a [sub-exponential growth model](https://www.jonghoonk.com/posts/sub-exponential-growth/index.html), which is challenging to fit if we use an exponential growth model.

I am using Julia for the UDE approach as it appeared that the Julia is the most advanced in this regard.

```{julia}
# SciML (Scientific Machine Learning) Tools
using OrdinaryDiffEq, SciMLSensitivity
using Optimization, OptimizationOptimisers, OptimizationOptimJL

# Standard Libraries
using LinearAlgebra, Statistics

# External Libraries
using ComponentArrays, Lux, Zygote, Plots, StableRNGs
gr()

# Set a random seed for reproducible behaviour
rng = StableRNG(1111)
```

### Data generation

The SIR model with a sub-exponential growth is used.

```{julia}
function sir_subexp!(du, u, p, t)
    α, β, γ = p 
    du[1] = - β*u[1]*u[2]^α
    du[2] = + β*u[1]*u[2]^α - γ*u[2]
    du[3] = + γ*u[2]
end

# Define the experimental parameter
tspan = (0.0, 20.0);
# u0 = 5.0f0 * rand(rng, 2)
u0 = [0.99, 0.01, 0.0];
p_ = [0.8, 0.4, 0.2];
prob = ODEProblem(sir_subexp!, u0, tspan, p_);
solution = solve(prob, Tsit5(), abstol = 1e-12, reltol = 1e-12, saveat = 1.0);

# Add noise in terms of the mean
X = Array(solution);
t = solution.t;

xbar = mean(X, dims=2);   
noise_magnitude = 5e-2;
Xn = X .+ (noise_magnitude * xbar) .* randn(rng, eltype(X), size(X));

plot(solution, alpha = 0.75, color = :black, label = ["True Data" nothing]);
scatter!(t, transpose(Xn), color = :red, label = ["Noisy Data" nothing])
```

### UDE model

```{julia}
# Let's define our Universal Differential eqution
rbf(x) = exp.(-(x .^ 2));

# Multilayer FeedForward
const U = Lux.Chain(Lux.Dense(3, 5, rbf), Lux.Dense(5, 5, rbf), 
Lux.Dense(5, 5, rbf), Lux.Dense(5, 1))
# Get the initial parameters and state variables of the model
p, st = Lux.setup(rng, U)
const _st = st

# Define the hybrid model
function ude_dynamics!(du, u, p, t, p_true)
    û = U(u, p, _st)[1] # Network prediction
    du[1] = dS = - û[1]
    du[2] = dI = + û[1] - p_true[3]*u[2] 
    du[3] = dR = + p_true[3]*u[2] 
end

# Closure with the known parameter
nn_dynamics!(du, u, p, t) = ude_dynamics!(du, u, p, t, p_)
# Define the problem
prob_nn = ODEProblem(nn_dynamics!, Xn[:, 1], tspan, p)

# I don't understand the details of the algorithm
# sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))
# I just adopted what's provided in the web page: 
# https://docs.sciml.ai/Overview/stable/showcase/missing_physics/
    
function predict(θ, X = Xn[:, 1], T = t)
    _prob = remake(prob_nn, u0 = X, tspan = (T[1], T[end]), p = θ)
    Array(solve(_prob, Tsit5(), saveat = T,
                abstol = 1e-6, reltol = 1e-6,
                sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))))
end

function loss(θ)
    Xhat = predict(θ)
    mean(abs2, Xn .- Xhat)
end

losses = Float64[];

callback = function (p, l)
    push!(losses, l)
    if length(losses) % 50 == 0
        println("Current loss after $(length(losses)) iterations: $(losses[end])")
    end
    return false
end

adtype = Optimization.AutoZygote();
optf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype);
optprob = Optimization.OptimizationProblem(optf, ComponentVector{Float64}(p));

mxiter = 2000
res1 = Optimization.solve(optprob, ADAM(), callback = callback, maxiters = mxiter);
println("Training loss after $(length(losses)) iterations: $(losses[end])")

# You can optimize further by using LBFGS
optprob2 = Optimization.OptimizationProblem(optf, res1.u)
res2 = Optimization.solve(optprob2, Optim.LBFGS(), callback = callback, maxiters = 1000);
println("Final training loss after $(length(losses)) iterations: $(losses[end])")

# Rename the best candidate
p_trained = res2.u

# Plot the losses
pl_losses = plot(1:mxiter, losses[1:mxiter], yaxis = :log10, xaxis = :log10,
                 xlabel = "Iterations", ylabel = "Loss", label = "ADAM", color = :blue)
plot!((mxiter+1):length(losses), losses[(mxiter+1):end], yaxis = :log10, xaxis = :log10,
      xlabel = "Iterations", ylabel = "Loss", label = "BFGS", color = :red)


## Analysis of the trained network
# Plot the data and the approximation
ts = first(solution.t):(mean(diff(solution.t)) / 2):last(solution.t)
Xhat = predict(p_trained, Xn[:, 1], ts)
# Trained on noisy data vs real solution
pl_trajectory = plot(ts, transpose(Xhat), xlabel = "t", 
                     ylabel = "S(t), I(t), R(t)", color = :red,
                     label = ["UDE Approximation" nothing])
scatter!(solution.t, transpose(Xn), color = :black, label = ["Measurements" nothing])


# Ideal unknown interactions of the predictor
# Ybar = [-p_[2] * (Xhat[1, :] .* Xhat[2, :])'; p_[3] * (Xhat[1, :] .* Xhat[2, :])']
# Ybar = [p_[2] .* Xhat[1,:] .* (Xhat[2,:].^p_[1])]
Ybar = transpose([p_[2] * Xhat[1,i] * (Xhat[2,i].^p_[1]) for i ∈ 1:41, j ∈ 1:1])
# Neural network guess
Yhat = U(Xhat, p_trained, st)[1]

pl_reconstruction = plot(ts, transpose(Yhat), xlabel = "t", 
                         ylabel = "U(S,I,R)", color = :red,
                         label = ["UDE Approximation" nothing]);

plot!(ts, transpose(Ybar), color = :black, label = ["True Interaction" nothing])


# Plot the error
pl_reconstruction_error = plot(ts, norm.(eachcol(Ybar - Yhat)), yaxis = :log, xlabel = "t",
                               ylabel = "L2-Error", label = nothing, color = :red);
pl_missing = plot(pl_reconstruction, pl_reconstruction_error, layout = (2, 1));

pl_overall = plot(pl_trajectory, pl_missing)

```
