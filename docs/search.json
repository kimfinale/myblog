[
  {
    "objectID": "blog/posts/variable_selection/index.html",
    "href": "blog/posts/variable_selection/index.html",
    "title": "Variables selection in statistical models",
    "section": "",
    "text": "When building statistical models, one of the most critical steps is variable selection. This process involves choosing the appropriate predictors or features that will be included in your model. The goal is to create a model that is both accurate and interpretable, avoiding overfitting and underfitting. I’ll explore what I am learning regarding issues related to variable selection, the common methods used, and best practices to ensure robust model performance.\nThe following contents are mainly based on the work by Sauebrei et al, which I am learning. As such, the following contents should not be used as a definitive guide to variable selection.\nVariable selection should start by defining what kind of model one is developing. The work by Shumeli, To explain or to predict?, discusses that there may be three kinds of statistical models: descriptive, explanatory, and predictive models. The first sentence of its abstract reads, “Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description.”\nThen we can list the variables based on the prior information. In this stage, drawing a directed acyclic graph (DAG) may help.\nOnce the list is made, one can employ a\n\nForward selection (FS)\nBackward elimination (BE)\nStepwise method that combines FS with additional BE. BE method is preferred because we already starts with a plausible model\nBest subset selection\n\nDifferent information criteria have been proposed, among which Akaike’s information criterion (AIC) is in principle preferable for predictive models and Schwartz’s Bayesian information criterion (BIC) for descriptive models.\n\nBurnham KP, Anderson DR. Model selection and multimodel inference: a practical information-theoretic approach. New York: Springer; 2002.\n\ntrue data generating model??\n\nFilter Methods: These methods apply statistical measures to assign a score to each feature. Features are ranked by their scores, and the top-ranked ones are selected.\n\nCorrelation Coefficient: Measures the linear relationship between each feature and the target variable. Features with high correlation (positive or negative) are preferred.\nChi-Square Test: Used for categorical variables, this test assesses whether there is a significant association between the feature and the target variable.\nANOVA F-Value: For continuous variables, the ANOVA F-value tests the null hypothesis that all means are equal.\n\nWrapper Methods: These methods evaluate feature subsets based on model performance. The main types of wrapper methods include:\n\nForward Selection: Starts with no variables and adds one variable at a time based on model performance improvement.\nBackward Elimination: Starts with all variables and removes the least significant variable at each step.\nRecursive Feature Elimination (RFE): Builds the model and eliminates the least important feature iteratively until the optimal number of features is reached.\n\nEmbedded Methods: These methods perform variable selection during the model training process and are typically specific to certain types of models.\n\nLASSO (Least Absolute Shrinkage and Selection Operator): Adds a penalty equal to the absolute value of the magnitude of coefficients. This penalty can shrink some coefficients to zero, effectively performing variable selection.\nRidge Regression: Adds a penalty equal to the square of the magnitude of coefficients. While it doesn’t perform variable selection outright, it can reduce the impact of less important variables.\nTree-Based Methods: Models like Random Forests or Gradient Boosting Trees inherently perform variable selection by considering the importance of variables based on their contribution to reducing impurity or error.\nBest Practices for Variable Selection 1. Understand Your Data: Before diving into variable selection techniques, it’s crucial to have a deep understanding of your data. Perform exploratory data analysis (EDA) to identify potential relationships and understand the context of each variable.\n\nUse Domain Knowledge: Leverage domain expertise to inform your variable selection process. Certain variables might be inherently more important based on the context of the problem.\nCross-Validation: Always use cross-validation to assess the performance of your model with the selected variables. This ensures that your model’s performance is consistent across different subsets of the data.\nCombine Methods: Often, the best approach is to combine multiple methods. For instance, you might start with filter methods to narrow down the field and then use wrapper methods to fine-tune the selection.\nRegularly Re-Evaluate: As new data becomes available or the problem context changes, re-evaluate your variable selection. What was relevant at one time may no longer be the best choice.\n\nConclusion Variable selection is a pivotal step in building effective statistical models. By carefully choosing the right variables, you can improve model accuracy, interpretability, and efficiency. Whether using filter methods, wrapper methods, or embedded methods, the key is to balance model complexity with performance. Combining these methods with domain knowledge and cross-validation will ensure that your model is robust and reliable. As data science continues to evolve, so too will the strategies for variable selection, making it an exciting area of ongoing research and application.\nUse prior information. Try to draw a directed acyclic graph (DAG)\n\nForward selection (FS)\nBackward elimination (BE)\nStepwise method that combines FS with additional BE. BE method is preferred because we already starts with a plausible model\nBest subset selection\n\nDifferent information criteria have been proposed, among which Akaike’s information criterion (AIC) is in principle preferable for predictive models and Schwartz’s Bayesian information criterion (BIC) for descriptive models.\n\nBurnham KP, Anderson DR. Model selection and multimodel inference: a practical information-theoretic approach. New York: Springer; 2002.\n\ntrue data generating model??\n\nChange-in-estimate\nLasso\nElastic net\nBoosting - Component-wise boosting is a forward stagewise regression procedure applicable to generalised linear models\nResampling-based: 9.1: bootstrap inclusion frequencies (BIF) 9.2: stability selection\n\nPost-selection inference\nSince the main aim of descriptive models is the interpretation of the estimated regression coefficients, point estimates should be accompanied by confidence intervals and sometimes also by \\(p\\) values.\nGeneralised additive model (GAM) State-of-the-art\nTo explain or to predict?\nClssic guide to the generalized linear mixed model is Bolker et al.\nReporting guidelines for the GLIMM\n\nData Fields\nage: Age of the subject age_grp: sex: household size: Number of people living together excluding oneself school/occupation"
  },
  {
    "objectID": "blog/posts/vacc_eff_waining_multiple_stages/index.html",
    "href": "blog/posts/vacc_eff_waining_multiple_stages/index.html",
    "title": "Waning vaccine efficacy on susceptibility",
    "section": "",
    "text": "In this article, I examined the process of modifying the disease transmission model (for instance, the \\(SEIR\\) model) to include vaccination and the waning of vaccine-induced immunity as it might happen in a clinical trial. A straightforward method to represent this involves assuming vaccinated individuals (i.e., those in the vaccinated, \\(V\\), compartment) possess partial immunity and subsequently transition back to the susceptible, \\(S\\), compartment. This approach effectively simulates the diminishing effectiveness of vaccines using two parameters. I focus on determining these parameters through the analysis of existing vaccine efficacy data, utilizing the typhoid vaccine clinical trial as a case study. The adaptation of the model to account for the progressive reduction in vaccine-derived immunity across various compartments was conducted prior post.\n\nVaccine efficacy data\nVaccine efficacy data come from a clinical trial conducted in Malawi from 2018 through 2023, for which the detailed information is available in the study.\n\n# df &lt;- data.frame(year=c(1,2), efficacy=c(83.4,73.0)) # Shakya (2021) Lancet Glob Health\n\n# vaccine efficacy cumulative over the four years\n# Patel (2024) Lancet\ntab2_vitt &lt;- data.frame(group = \"Vi-TT\",\n                   age = c(\"&lt;2yo\",\"2-4yo\",\"&gt;=5yo\",\"all_ITT\",\"all_PP\"),\n                   person_years = c(6586,15007,38907,60500,59942), \n                   typhoid_culture = c(4,5,15,24,22),\n                   IR_obs = c(60.7,33.3,38.6,39.7,36.7),\n                   VE_obs = c(70.6,79.6,79.3,78.3,80.0))\n\ntab2_mena &lt;- data.frame(group = \"MenA\",\n                   age = c(\"&lt;2yo\",\"2-4yo\",\"&gt;=5yo\",\"all_ITT\",\"all_PP\"),\n                   person_years = c(6773,15297,38151,60220,59662), \n                   typhoid_culture = c(14,25,71,110,109),\n                   IR_obs = c(206.7,163.4,186.1,182.7,182.7),\n                   VE_obs = NA)\n\ntab2 &lt;- rbind(tab2_vitt, tab2_mena)\n# Check the sanity of the data\ntab2$IR_cal &lt;- round(tab2$typhoid_culture/tab2$person_years*1e5, digits=1)\ntab2$IR_cal == tab2$IR_obs\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\ntab2$VE_cal &lt;- NA\ntab2[tab2$group == \"Vi-TT\",]$VE_cal &lt;- \n  round(100*(1 - tab2[tab2$group == \"Vi-TT\",]$IR_cal/\n               tab2[tab2$group == \"MenA\",]$IR_cal),digits=1) \ntab2$VE_cal == tab2$VE_obs\n\n [1]  TRUE  TRUE  TRUE  TRUE FALSE    NA    NA    NA    NA    NA\n\ntab3_vitt &lt;- data.frame(group = \"Vi-TT\",\n                   age = \"all_ITT\",\n                   person_years = c(14058,28104,42135,56121,60500),\n                   year_since_vacc = c(1,2,3,4,4.61),\n                   typhoid_culture = c(6,12,15,23,24),\n                   IR_obs = c(42.7,42.7,35.6,41.0,39.7),\n                   VE_obs=c(83.4,80.7,80.1,77.1,78.3))\ntab3_mena &lt;- data.frame(group = \"MenA\",\n                   age = \"all_ITT\",\n                   person_years = c(14036,28021,41983,55889,60220),\n                   year_since_vacc = c(1,2,3,4,4.61),\n                   typhoid_culture = c(36,62,75,100,110),\n                   IR_obs = c(256.5,221.3,178.6,178.9,182.7),\n                   VE_obs = NA)\ntab3 &lt;- rbind(tab3_vitt, tab3_mena)\n# Check the sanity of the data\ntab3$IR_cal &lt;- round(tab3$typhoid_culture/tab3$person_years*1e5, digits=1)\ntab3$VE_cal &lt;- NA\ntab3[tab3$group == \"Vi-TT\",]$VE_cal &lt;- \n  round(100*(1 - tab3[tab3$group == \"Vi-TT\",]$IR_cal/\n               tab3[tab3$group == \"MenA\",]$IR_cal),digits=1) \ntab3$VE_cal == tab3$VE_obs\n\n [1] TRUE TRUE TRUE TRUE TRUE   NA   NA   NA   NA   NA\n\n\n\n\nModel of typhoid transmission for a clinical trial\n\nseirv_nstg_trial &lt;- function(u, p, t){\n  \n  S &lt;- u[1]; E &lt;- u[2]; I &lt;- u[3]; R &lt;- u[4]; C &lt;- u[5]\n  # # vaccinated cohort\n  nstg &lt;- p[1]\n  for (i in 1:nstg) {\n    eval(str2lang(paste0(\"V\",i,\" &lt;- \", \"u[5+\",i,\"]\")));\n  }# vaccinated and partially protected\n \n  VS &lt;- u[5 + nstg + 1] # vaccinated but immunity waned and fully susceptible\n  VE &lt;- u[5 + nstg + 2] \n  VI &lt;- u[5 + nstg + 3] \n  VR &lt;- u[5 + nstg + 4] \n  VC &lt;- u[5 + nstg + 5]\n  \n  pop &lt;- S + E + I + R\n  \n  v_sum &lt;- paste(sapply(1:nstg, function(x) paste0(\"V\",x)), collapse=\"+\")\n  eval(str2lang(paste0(\"popV &lt;- VS+VE+VI+VR+\", v_sum)))\n  \n  epsilon &lt;- p[2] # 1/latent period\n  gamma &lt;- p[3] # 1/duration of infectiousness\n  beta &lt;- p[4] # transmission rate\n  mu &lt;- p[5] # death rate is applied and population size decreases over time\n  omega &lt;- p[6] # 1 / duration of natural immunity\n  omega_v &lt;- p[7] # 1 / duration of partial vaccine-derived immunity\n  ve &lt;- p[8] # vaccine efficacy\n  # vaccinated and unvaccinated population mix randomly\n  foi &lt;- beta*(I+VI)/(pop+popV) # force of infection\n  \n  muEI &lt;- epsilon\n  muIR &lt;- gamma\n  muRS &lt;- omega\n  muVS &lt;- nstg*omega_v\n  \n  # differential equations\n  dS &lt;- - foi*S + muRS*R - mu*S + mu*(pop+popV)\n  dE &lt;- foi*S - muEI*E - mu*E\n  dI &lt;- muEI*E - muIR*I - mu*I\n  dR &lt;- muIR*I - muRS*R - mu*R\n  dC &lt;- muEI*E\n\n  dV1 &lt;- - foi*(1-ve)*V1 - muVS*V1 - mu*V1\n  if (nstg &gt;= 2) {\n    for (i in 2:nstg) {\n      eval(str2lang(paste0(\"dV\",i,\" &lt;- - foi*(1-ve)*V\", i, \" + muVS*V\",\n                           i-1 , \" - muVS*V\", i,\" - mu*V\", i)))\n    }\n  }\n\n  eval(str2lang(paste0(\"dVS &lt;- - foi*VS + muVS*V\", \n                       nstg, \" + muRS*VR - mu*VS\"))) \n  \n  eval(str2lang(paste0(\"dVE &lt;- foi*((1-ve)*(\", \n                       v_sum, \")+VS) - muEI*VE - mu*VE\")))\n  \n  dVI &lt;- muEI*VE - muIR*VI - mu*VI\n  dVR &lt;- muIR*VI - muRS*VR - mu*VR\n  dVC &lt;- muEI*VE\n  \n  dvstr &lt;- paste(sapply(1:nstg, function(x) paste0(\"dV\", x)), collapse=\",\")\n  return(eval(str2lang(paste0(\"c(dS,dE,dI,dR,dC,\", \n                              dvstr, \",dVS,dVE,dVI,dVR,dVC)\"))))\n}\n\n\n\nSimulation of the variable-compartment \\(SEIRV\\) model\nSolutions for the steady states of the \\(SEIR\\) model. These are used to set the initial conditions as we assume that the disease transmission reached a steady state.\n\nSs &lt;- \"((gamma + mu) * (epsilon + mu)) / (beta * epsilon)\"\nEs &lt;- \"- ((gamma + mu) * (mu + omega) * ((gamma + mu) * (epsilon + mu) - beta * epsilon)) / (beta * epsilon * (gamma * (epsilon + mu + omega) + (epsilon + mu) * (mu + omega)))\"\n\nIs &lt;- \"((mu + omega) * (beta * epsilon - (gamma + mu) * (epsilon + mu))) / (beta * omega * (gamma + epsilon + mu) + beta * (gamma + mu) * (epsilon + mu))\"\nRs &lt;- \"(beta * gamma * epsilon - gamma * (gamma + mu) * (epsilon + mu)) / (beta * omega * (gamma + epsilon + mu) + beta * (gamma + mu) * (epsilon + mu))\"\n\nCommon parameters\n\nR0 &lt;- 3 # basic reproduction number\nepsilon &lt;- 1/5 # 1/epsilon = incubation period\ngamma &lt;- 1/11.8 # 1/gamma = duration of infectiousness\nbeta &lt;- R0*gamma # instantaneous transmission rate\nmu &lt;- 1/(65*365) # natural death rate\nomega &lt;- 0 # re-infection is not counted in the clinical trial\nomega_v &lt;- 1/(4*365) # natural immunity waning rate\nve &lt;- 0.834\nnstg &lt;- 1 # number of stages\nN &lt;- 1e6 # population size\nf &lt;- 1e-4 # fraction of vaccinated population\ntend &lt;- 10*365\n# parameters\nparams &lt;- c(nstg=nstg, epsilon=epsilon, gamma=gamma, beta=beta, \n            mu=mu, omega=omega, omega_v=omega_v, ve=ve,\n            N=N, f=f, tend=tend)\n\n# steady states for given parameters\nstates0 &lt;- list(S=Ss, E=Es, I=Is, R=Rs)\nsteadys0 &lt;- sapply(states0, function(x) eval(parse(text = x)))\n\nFunction to run the model\n\nrun_seirv_nstg_trial &lt;- function(params) {\n  de &lt;- diffeqr::diffeq_setup() # to use Julia's DifferentialEquations.jl\n  # initial distribution of the population across the states\n  nstg &lt;- params[[\"nstg\"]]\n  N &lt;- params[[\"N\"]]\n  f &lt;- params[[\"f\"]]\n  V0s &lt;- paste(sapply(1:nstg, function(x) paste0(\"V\", x,\"=0\")), collapse=\",\")\n  eval(str2lang(paste0(\"u0 &lt;- c(steadys0*N,C=0,\", \n                       V0s, \",VS=0,VE=0,VI=0,VR=0,VC=0)\")))\n  u0[c(\"V1\",\"VE\",\"VI\",\"VR\")] &lt;- as.numeric(steadys0*N*f/(1-f))\n  tend &lt;- params[[\"tend\"]]\n  tspan &lt;- c(0.0, tend)\n  \n  prob &lt;- de$ODEProblem(seirv_nstg_trial, u0, tspan, params)\n  sol &lt;- de$solve(prob, de$Tsit5(), saveat=1)\n  mat &lt;- sapply(sol$u, identity)\n  udf &lt;- as.data.frame(t(mat))\n  out &lt;- cbind(data.frame(t=sol$t), udf)\n\n  Vn &lt;- paste(sapply(1:nstg, function(x) paste0('\"V', x, '\"')), collapse=\",\")\n  names(out) &lt;- eval(str2lang(paste0('c(\"t\",\"S\",\"E\",\"I\",\"R\",\"C\",', Vn, \n                                     ',\"VS\",\"VE\",\"VI\",\"VR\",\"VC\")'))) \n  return(out)\n}\n\nDirect vaccine effectiveness in case vaccine-derived immunity does not wane over time\n\nnstg &lt;- params[[\"nstg\"]]\nparams[[\"omega_v\"]] &lt;- 0\nout &lt;- run_seirv_nstg_trial(params=params)\nnovacc &lt;- c(\"S\",\"E\",\"I\",\"R\")\nVn &lt;- paste(sapply(1:nstg, function(x) paste0('\"V', x, '\"')), collapse=\",\")\nvacc &lt;- eval(str2lang(paste0('c(', Vn, ',\"VS\",\"VE\",\"VI\",\"VR\")'))) \n  \ndve &lt;- rep(NA, nrow(out)) # direct vaccine effectiveness measured daily\nfor(i in 1:nrow(out)){\n  dve[i] &lt;- 1 - (out[i,\"VC\"]/sum(out[i,vacc]))/(out[i,\"C\"]/sum(out[i,novacc]))\n}\nplot(1:length(dve)/365, dve, type=\"l\", ylab=\"direct VE\", xlab=\"year\")\n\n\n\n\nDirect vaccine efficacy in case vaccine-derived immunity wanes over time with the average waiting time of 10 years\n\nnstg &lt;- params[[\"nstg\"]]\nparams[[\"omega_v\"]] &lt;- 1/10/365 # average waiting time = 10 years\nout &lt;- run_seirv_nstg_trial(params=params)\nnovacc &lt;- c(\"S\",\"E\",\"I\",\"R\")\nVn &lt;- paste(sapply(1:nstg, function(x) paste0('\"V', x, '\"')), collapse=\",\")\nvacc &lt;- eval(str2lang(paste0('c(', Vn, ',\"VS\",\"VE\",\"VI\",\"VR\")'))) \n  \ndve &lt;- rep(NA, nrow(out)) # direct vaccine effectiveness measured daily\nfor(i in 1:nrow(out)){\n  dve[i] &lt;- 1 - (out[i,\"VC\"]/sum(out[i,vacc]))/(out[i,\"C\"]/sum(out[i,novacc]))\n}\nplot(1:length(dve)/365, dve, type=\"l\", ylab=\"direct VE\", xlab=\"year\")\n\n\n\n\nFunction to measure vaccine effectiveness over time.\n\nmeasure_vacc_eff &lt;- function(p, params, times){\n  de &lt;- diffeqr::diffeq_setup()\n  params[[\"ve\"]] &lt;- p[1]\n  params[[\"omega_v\"]] &lt;- p[2]\n  nstg &lt;- params[[\"nstg\"]]\n  \n  out &lt;- run_seirv_nstg_trial(params)\n  novacc &lt;- c(\"S\",\"E\",\"I\",\"R\")\n  Vn &lt;- paste(sapply(1:nstg, function(x) paste0('\"V', x, '\"')), collapse=\",\")\n  vacc &lt;- eval(str2lang(paste0('c(', Vn, ',\"VS\",\"VE\",\"VI\",\"VR\")'))) \n  \n  ve_sim &lt;- rep(NA, length(times))\n  for (i in 1:length(times)) {\n    ve_sim[i] &lt;- 1 - \n      ((out[times[i],\"VC\"])/sum(out[times[i],vacc])) / \n      ((out[times[i],\"C\"])/sum(out[times[i],novacc]))\n  }\n  return(ve_sim)\n}\n\nDefine sum of squared difference (ssq) between the model and the data to evaluate the performance of the model.\n\n# the initial VE is not measured from the model\ntimes &lt;- round(tab3[tab3$group == \"Vi-TT\",]$year_since_vacc*365) # times to measure VE\nve_obs &lt;- tab3[tab3$group == \"Vi-TT\",]$VE_obs/100\nssq &lt;- function(p){\n  ve_sim &lt;- measure_vacc_eff(p, params=params, times=times)\n  sum((ve_obs - ve_sim)^2)  \n}\n# check for some predetermined ve and omega\n# ssq(p=c(0.8,1/3650)) must be smaller than ssq(p=c(0.4,1/365))\nssq(p=c(0.8,1/3650))\n\n[1] 0.05994089\n\nssq(p=c(0.4,1/3650))\n\n[1] 1.005789\n\n\nEstimate parameters by minimizing the ssq using the nlminb function.\n\nparams[[\"nstg\"]] &lt;- 1\nfit1 &lt;- nlminb(c(0.9, 1/(10*365)), objective=ssq, \n              lower=c(0.1,1/(1000*365)),\n              upper=c(0.99,0.99))\nfit1$par[1]\n\n[1] 0.8589978\n\n1/fit1$par[2]/365\n\n[1] 22.14073\n\nfit1$objective\n\n[1] 0.0003437906\n\nve_obs\n\n[1] 0.834 0.807 0.801 0.771 0.783\n\n(ve_sim &lt;- round(measure_vacc_eff(p=fit1$par, params=params, times=times), digits=3))\n\n[1] 0.828 0.815 0.799 0.782 0.772\n\n\n\nparams[[\"nstg\"]] &lt;- 2\nfit2 &lt;- nlminb(c(0.9, 1/(100*365)), objective=ssq, \n              lower=c(0.1,1/(100000*365)),\n              upper=c(0.99,0.99))\nfit2$par[1]\n1/fit2$par[2]/365\nfit2$objective\nve_obs\nround(measure_vacc_eff(p=fit2$par, params=params, times=times), digits=3)\n\nPlot data and simulation results\n\n# png(\"vacc_eff_obs_sim.png\")\nplot(times/365, ve_obs, col=\"firebrick\", \n     ylim=c(0.5,1),  xlab=\"year since vaccination\", ylab=\"direct vaccine effectiveness\",\n     type=\"p\", pch=0)\nlines(times/365, ve_sim, col=\"black\")\nlegend(\"topright\", \n  legend=c(\"Data\", \"Model\"), \n  col=c(\"firebrick\", \"black\"), \n  lty= c(0,1),\n  pch=c(0,NA),\n  bty = \"n\", \n  cex = 1.0, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.02,0.02))\n\n\n\n# dev.off()"
  },
  {
    "objectID": "blog/posts/vaccine_effectiveness/index.html",
    "href": "blog/posts/vaccine_effectiveness/index.html",
    "title": "Vaccine effectiveness",
    "section": "",
    "text": "Vaccine efficacy and effectiveness (VE) is generally estimated as one minus some measure of relative risk (RR) in the vaccinated group compared to the unvaccinated group Halloran et al.:\n\\[\n\\text{VE} = 1 - \\text{RR}\n\\]\nRR measures may include relative infection probability given exposure, hazard ratio, or the ratio of cumulative incidence (or attack rate). I will explain different effects of the vaccine using the cumulative incidence (\\(C\\)) measure, as outlined in the Study design for dependent happenings.\n\nIn population \\(A\\), a fraction, \\(f\\), of the population was vaccinated (denoted as \\(1\\)) and the rest, \\(1-f\\), of the Population A remains unvaccinated (denoted as \\(0\\)). In Population \\(B\\), no one was vaccinated. I will use \\(C_{A0}\\) and \\(C_{A1}\\) to denote cumulative incidence, \\(C\\), in the unvaccinated and vaccinated individuals of Population \\(A\\). Similarly, \\(C_{B0}\\) will denote the cumulative incidence in the unvaccinated population \\(B\\).\nVaccine effectiveness (VE) can be divided into four different measures: direct, indirect, total, and overall VE, which are denoted as DVE, IVE, TVE, and OVE, respectively.\n\\[\n\\text{DVE} = 1 - \\frac{C_{A1}}{C_{A0}},~\n\\text{IVE} = 1 - \\frac{C_{A0}}{C_{B0}},~\n\\text{TVE} = 1 - \\frac{C_{A1}}{C_{B0}},~\n\\text{OVE} = 1 - \\frac{f C_{A1}+(1-f) C_{A0}}{C_{B0}}.\n\\]\nGiven the above equations, the following relationships hold:\n\\[\n\\text{IVE} = 1-\\frac{1-\\text{TVE}}{1-\\text{DVE}},~\n\\tag{1}\\]\n\\[\\text{OVE} = 1-\\left(f\\left(1-\\text{TVE}\\right)+(1-f)\\left(1-\\text{IVE}\\right)\\right)\\] TVE may be replaced using IVE and DVE as below.\n\\[\\text{OVE} = 1-\\left(f\\left(1-\\text{DVE})(1-\\text{IVE}\\right)+(1-f)\\left(1-\\text{IVE}\\right)\\right)\n\\tag{2}\\]\nI will illustrate the different metrics of VE using simple \\(SEIR\\) model simulations.\nThe model, seir_2grp, includes two subgroups, which will be used to differentiate vaccinated and unvaccinated populations.\n\nseir_2grp &lt;- function(t, y, params) {\n  S0 &lt;- y[\"S0\"]; E0 &lt;- y[\"E0\"]; I0 &lt;- y[\"I0\"]; R0 &lt;- y[\"R0\"]; C0 &lt;- y[\"C0\"]\n  S1 &lt;- y[\"S1\"]; E1 &lt;- y[\"E1\"]; I1 &lt;- y[\"I1\"]; R1 &lt;- y[\"R1\"]; C1 &lt;- y[\"C1\"]\n \n  beta &lt;- params[\"beta\"]\n  epsilon &lt;- params[\"epsilon\"]\n  gamma &lt;- params[\"gamma\"]\n  \n  # vaccinated and unvaccinated population are well mixed\n  muSE &lt;- beta * (I1+I0) / (S1 + E1 + I1 + R1 + S0 + E0 + I0 + R0)\n  muEI &lt;- epsilon\n  muIR &lt;- gamma\n  \n  dS0 &lt;- - muSE*S0\n  dE0 &lt;-  muSE*S0 - muEI*E0\n  dI0 &lt;-  muEI*E0 - muIR*I0\n  dR0 &lt;-  muIR*I0\n  dC0 &lt;-  muEI*E0 ## cumulative symtom onset\n  \n  dS1 &lt;- - muSE*S1\n  dE1 &lt;-  muSE*S1 - muEI*E1\n  dI1 &lt;-  muEI*E1 - muIR*I1\n  dR1 &lt;-  muIR*I1\n  dC1 &lt;-  muEI*E1 ## cumulative symtom onset\n  \n  return(list(c(dS0,dE0,dI0,dR0,dC0,dS1,dE1,dI1,dR1,dC1)))\n}\n\n\n\nA fraction \\(f\\) of population A received the vaccine, with vaccine efficacy denoted as VE. Vaccination is implemented using an all-or-nothing approach, meaning a fraction of the vaccine recipients, \\(Nf\\text{VE}\\), are completely protected from infection. Here, \\(N\\), \\(f\\), and VE represent the population size, the fraction of the population that is vaccinated, and vaccine efficacy, respectively. In the SEIR model, we can mimic the vaccination scenario by setting the initial population size for the \\(R\\) compartment to \\(Nf\\text{VE}\\).\n\nN &lt;- 1\ntend &lt;- 200\nbeta &lt;- 2.5/4.5\nepsilon &lt;- 1/5.2\ngamma &lt;- 1/4.5\nparams &lt;- c(beta=beta, epsilon=epsilon, gamma=gamma)\n\nVE &lt;- 0.7 # vaccine efficacy on susceptibility\nf &lt;- 0.3 # vacccine coverage proportion\nN0 &lt;- N*(1-f)  # unvaccinated population\nN1 &lt;- N*f # vaccinated population\npop_A &lt;- c(N0=N0, N1=N1)\ny0_A &lt;- c(S0=N0, E0=0, I0=0.01, R0=0, C0=0, \n        S1=N1*(1-VE), E1=0, I1=0.0, R1=N1*VE, C1=0)\ntend &lt;- 200\ntimes &lt;- seq(from=0, to=tend, by=1)\n\nout &lt;- deSolve::ode(y=y0_A, times=times, func=seir_2grp, parms=params)\npop_A_out &lt;- as.data.frame(out)\n\n\n\n\nNo one in the population is vaccinated.\n\npop_B &lt;- c(N0=N)\n# no need for the vaccinated group\ny0_B &lt;- c(S0=pop_B[[\"N0\"]], E0=0, I0=0.01, R0=0, C0=0, \n          S1=0.0, E1=0, I1=0.0, R1=0.0, C1=0)\nout &lt;- deSolve::ode(y=y0_B, times=times, func=seir_2grp, parms=params)\npop_B_out &lt;- as.data.frame(out) \n\nLet’s compute the four VE measures based on the cumulative incidence.\n\n(DVE &lt;- 1 - (pop_A_out[tend,\"C1\"]/pop_A[[\"N1\"]])/(pop_A_out[tend,\"C0\"]/pop_A[[\"N0\"]]))\n\n[1] 0.7\n\n(IVE &lt;- 1 - (pop_A_out[tend,\"C0\"]/pop_A[[\"N0\"]])/(pop_B_out[tend,\"C0\"]/pop_B[[\"N0\"]]))\n\n[1] 0.1118712\n\n(OVE &lt;- 1 - ((1-f)*pop_A_out[tend,\"C0\"]/pop_A[[\"N0\"]] + f*pop_A_out[tend,\"C1\"]/pop_A[[\"N1\"]])/ (pop_B_out[tend,\"C0\"]/pop_B[[\"N0\"]]))\n\n[1] 0.2983782\n\n(TVE &lt;- 1 - (pop_A_out[tend,\"C1\"]/pop_A[[\"N1\"]])/(pop_B_out[tend,\"C0\"]/pop_B[[\"N0\"]]))\n\n[1] 0.7335613\n\n\nLet’s compare the VE metrics using the equations derived above (Equation 1 and Equation 2).\n\n(IVE == 1 - (1-TVE)/(1-DVE))\n\n[1] TRUE\n\n(OVE == 1 - (f*(1-TVE)+(1-f)*(1-IVE)))\n\n[1] TRUE\n\n(IVE == 1 - (1 - OVE - f*(1-TVE))/(1-f))\n\n[1] TRUE\n\n(TVE == 1 - (1-DVE)*(1-IVE))\n\n[1] TRUE\n\n\nLet’s examine the dynamics of the four VE measures, where each metric is computed based on the cumulative incidence at each point in time.\n\ndve &lt;- 1 - (pop_A_out[,\"C1\"]/pop_A[[\"N1\"]])/(pop_A_out[,\"C0\"]/pop_A[[\"N0\"]])\nive &lt;- 1 - (pop_A_out[,\"C0\"]/pop_A[[\"N0\"]])/(pop_B_out[,\"C0\"]/pop_B[[\"N0\"]])\nove &lt;- 1 - ((1-f)*pop_A_out[,\"C0\"]/pop_A[[\"N0\"]] + f*pop_A_out[,\"C1\"]/pop_A[[\"N1\"]])/ (pop_B_out[,\"C0\"]/pop_B[[\"N0\"]])\ntve &lt;- 1 - (pop_A_out[,\"C1\"]/pop_A[[\"N1\"]])/(pop_B_out[,\"C0\"]/pop_B[[\"N0\"]])\n\n\nplot(ive, pch=1, main=\"Indirect vaccine effectiveness\", xlab=\"day\", ylab=\"IVE\")\nlines(1 - (1-tve)/(1-dve), col=\"firebrick\")\nlines(1 - (1 - ove - f*(1-tve))/(1-f), col=\"magenta\")\n\nlegend(\"topright\", \n  legend=c(\"IVE definition\",\"Based on Eq. 1\",\"Based on Eq. 2\"), \n  col = c(\"black\", \"firebrick\",\"magenta\"),\n  pch = c(1, NA, NA),\n  lty = c(NA, 1, 1),\n  bty = \"n\", \n  cex = 1.0, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.02,0.02))\n\n\n\n\n\nplot(ove, pch=1, main=\"Overall vaccine effectiveness\", xlab=\"day\", ylab=\"OVE\")\nlines(1 - (f*(1-tve)+(1-f)*(1-ive)), col=\"firebrick\")\nlegend(\"topright\", \n  legend = c(\"OVE definition\",\"Based on Eq. 2\"), \n  col = c(\"black\", \"firebrick\"),\n  pch = c(1, NA),\n  lty = c(NA, 1),\n  bty = \"n\", \n  cex = 1.0, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.02,0.02))\n\n\n\n\n\nplot(tve, pch=1, main=\"Total vaccine effectiveness\", xlab=\"day\", ylab=\"TVE\")\nlines(1 - (1-dve)*(1-ive), col=\"firebrick\")\nlegend(\"topright\", \n  legend = c(\"TVE definition\",\"Based on Eq. 1\"), \n  col = c(\"black\", \"firebrick\"),\n  pch = c(1, NA),\n  lty = c(NA, 1),\n  bty = \"n\", \n  cex = 1.0, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.02,0.02))"
  },
  {
    "objectID": "blog/posts/vaccine_effectiveness/index.html#vaccine-efficacy-and-effectiveness",
    "href": "blog/posts/vaccine_effectiveness/index.html#vaccine-efficacy-and-effectiveness",
    "title": "Vaccine effectiveness",
    "section": "",
    "text": "Vaccine efficacy and effectiveness (VE) is generally estimated as one minus some measure of relative risk (RR) in the vaccinated group compared to the unvaccinated group Halloran et al.:\n\\[\n\\text{VE} = 1 - \\text{RR}\n\\]\nRR measures may include relative infection probability given exposure, hazard ratio, or the ratio of cumulative incidence (or attack rate). I will explain different effects of the vaccine using the cumulative incidence (\\(C\\)) measure, as outlined in the Study design for dependent happenings.\n\nIn population \\(A\\), a fraction, \\(f\\), of the population was vaccinated (denoted as \\(1\\)) and the rest, \\(1-f\\), of the Population A remains unvaccinated (denoted as \\(0\\)). In Population \\(B\\), no one was vaccinated. I will use \\(C_{A0}\\) and \\(C_{A1}\\) to denote cumulative incidence, \\(C\\), in the unvaccinated and vaccinated individuals of Population \\(A\\). Similarly, \\(C_{B0}\\) will denote the cumulative incidence in the unvaccinated population \\(B\\).\nVaccine effectiveness (VE) can be divided into four different measures: direct, indirect, total, and overall VE, which are denoted as DVE, IVE, TVE, and OVE, respectively.\n\\[\n\\text{DVE} = 1 - \\frac{C_{A1}}{C_{A0}},~\n\\text{IVE} = 1 - \\frac{C_{A0}}{C_{B0}},~\n\\text{TVE} = 1 - \\frac{C_{A1}}{C_{B0}},~\n\\text{OVE} = 1 - \\frac{f C_{A1}+(1-f) C_{A0}}{C_{B0}}.\n\\]\nGiven the above equations, the following relationships hold:\n\\[\n\\text{IVE} = 1-\\frac{1-\\text{TVE}}{1-\\text{DVE}},~\n\\tag{1}\\]\n\\[\\text{OVE} = 1-\\left(f\\left(1-\\text{TVE}\\right)+(1-f)\\left(1-\\text{IVE}\\right)\\right)\\] TVE may be replaced using IVE and DVE as below.\n\\[\\text{OVE} = 1-\\left(f\\left(1-\\text{DVE})(1-\\text{IVE}\\right)+(1-f)\\left(1-\\text{IVE}\\right)\\right)\n\\tag{2}\\]\nI will illustrate the different metrics of VE using simple \\(SEIR\\) model simulations.\nThe model, seir_2grp, includes two subgroups, which will be used to differentiate vaccinated and unvaccinated populations.\n\nseir_2grp &lt;- function(t, y, params) {\n  S0 &lt;- y[\"S0\"]; E0 &lt;- y[\"E0\"]; I0 &lt;- y[\"I0\"]; R0 &lt;- y[\"R0\"]; C0 &lt;- y[\"C0\"]\n  S1 &lt;- y[\"S1\"]; E1 &lt;- y[\"E1\"]; I1 &lt;- y[\"I1\"]; R1 &lt;- y[\"R1\"]; C1 &lt;- y[\"C1\"]\n \n  beta &lt;- params[\"beta\"]\n  epsilon &lt;- params[\"epsilon\"]\n  gamma &lt;- params[\"gamma\"]\n  \n  # vaccinated and unvaccinated population are well mixed\n  muSE &lt;- beta * (I1+I0) / (S1 + E1 + I1 + R1 + S0 + E0 + I0 + R0)\n  muEI &lt;- epsilon\n  muIR &lt;- gamma\n  \n  dS0 &lt;- - muSE*S0\n  dE0 &lt;-  muSE*S0 - muEI*E0\n  dI0 &lt;-  muEI*E0 - muIR*I0\n  dR0 &lt;-  muIR*I0\n  dC0 &lt;-  muEI*E0 ## cumulative symtom onset\n  \n  dS1 &lt;- - muSE*S1\n  dE1 &lt;-  muSE*S1 - muEI*E1\n  dI1 &lt;-  muEI*E1 - muIR*I1\n  dR1 &lt;-  muIR*I1\n  dC1 &lt;-  muEI*E1 ## cumulative symtom onset\n  \n  return(list(c(dS0,dE0,dI0,dR0,dC0,dS1,dE1,dI1,dR1,dC1)))\n}\n\n\n\nA fraction \\(f\\) of population A received the vaccine, with vaccine efficacy denoted as VE. Vaccination is implemented using an all-or-nothing approach, meaning a fraction of the vaccine recipients, \\(Nf\\text{VE}\\), are completely protected from infection. Here, \\(N\\), \\(f\\), and VE represent the population size, the fraction of the population that is vaccinated, and vaccine efficacy, respectively. In the SEIR model, we can mimic the vaccination scenario by setting the initial population size for the \\(R\\) compartment to \\(Nf\\text{VE}\\).\n\nN &lt;- 1\ntend &lt;- 200\nbeta &lt;- 2.5/4.5\nepsilon &lt;- 1/5.2\ngamma &lt;- 1/4.5\nparams &lt;- c(beta=beta, epsilon=epsilon, gamma=gamma)\n\nVE &lt;- 0.7 # vaccine efficacy on susceptibility\nf &lt;- 0.3 # vacccine coverage proportion\nN0 &lt;- N*(1-f)  # unvaccinated population\nN1 &lt;- N*f # vaccinated population\npop_A &lt;- c(N0=N0, N1=N1)\ny0_A &lt;- c(S0=N0, E0=0, I0=0.01, R0=0, C0=0, \n        S1=N1*(1-VE), E1=0, I1=0.0, R1=N1*VE, C1=0)\ntend &lt;- 200\ntimes &lt;- seq(from=0, to=tend, by=1)\n\nout &lt;- deSolve::ode(y=y0_A, times=times, func=seir_2grp, parms=params)\npop_A_out &lt;- as.data.frame(out)\n\n\n\n\nNo one in the population is vaccinated.\n\npop_B &lt;- c(N0=N)\n# no need for the vaccinated group\ny0_B &lt;- c(S0=pop_B[[\"N0\"]], E0=0, I0=0.01, R0=0, C0=0, \n          S1=0.0, E1=0, I1=0.0, R1=0.0, C1=0)\nout &lt;- deSolve::ode(y=y0_B, times=times, func=seir_2grp, parms=params)\npop_B_out &lt;- as.data.frame(out) \n\nLet’s compute the four VE measures based on the cumulative incidence.\n\n(DVE &lt;- 1 - (pop_A_out[tend,\"C1\"]/pop_A[[\"N1\"]])/(pop_A_out[tend,\"C0\"]/pop_A[[\"N0\"]]))\n\n[1] 0.7\n\n(IVE &lt;- 1 - (pop_A_out[tend,\"C0\"]/pop_A[[\"N0\"]])/(pop_B_out[tend,\"C0\"]/pop_B[[\"N0\"]]))\n\n[1] 0.1118712\n\n(OVE &lt;- 1 - ((1-f)*pop_A_out[tend,\"C0\"]/pop_A[[\"N0\"]] + f*pop_A_out[tend,\"C1\"]/pop_A[[\"N1\"]])/ (pop_B_out[tend,\"C0\"]/pop_B[[\"N0\"]]))\n\n[1] 0.2983782\n\n(TVE &lt;- 1 - (pop_A_out[tend,\"C1\"]/pop_A[[\"N1\"]])/(pop_B_out[tend,\"C0\"]/pop_B[[\"N0\"]]))\n\n[1] 0.7335613\n\n\nLet’s compare the VE metrics using the equations derived above (Equation 1 and Equation 2).\n\n(IVE == 1 - (1-TVE)/(1-DVE))\n\n[1] TRUE\n\n(OVE == 1 - (f*(1-TVE)+(1-f)*(1-IVE)))\n\n[1] TRUE\n\n(IVE == 1 - (1 - OVE - f*(1-TVE))/(1-f))\n\n[1] TRUE\n\n(TVE == 1 - (1-DVE)*(1-IVE))\n\n[1] TRUE\n\n\nLet’s examine the dynamics of the four VE measures, where each metric is computed based on the cumulative incidence at each point in time.\n\ndve &lt;- 1 - (pop_A_out[,\"C1\"]/pop_A[[\"N1\"]])/(pop_A_out[,\"C0\"]/pop_A[[\"N0\"]])\nive &lt;- 1 - (pop_A_out[,\"C0\"]/pop_A[[\"N0\"]])/(pop_B_out[,\"C0\"]/pop_B[[\"N0\"]])\nove &lt;- 1 - ((1-f)*pop_A_out[,\"C0\"]/pop_A[[\"N0\"]] + f*pop_A_out[,\"C1\"]/pop_A[[\"N1\"]])/ (pop_B_out[,\"C0\"]/pop_B[[\"N0\"]])\ntve &lt;- 1 - (pop_A_out[,\"C1\"]/pop_A[[\"N1\"]])/(pop_B_out[,\"C0\"]/pop_B[[\"N0\"]])\n\n\nplot(ive, pch=1, main=\"Indirect vaccine effectiveness\", xlab=\"day\", ylab=\"IVE\")\nlines(1 - (1-tve)/(1-dve), col=\"firebrick\")\nlines(1 - (1 - ove - f*(1-tve))/(1-f), col=\"magenta\")\n\nlegend(\"topright\", \n  legend=c(\"IVE definition\",\"Based on Eq. 1\",\"Based on Eq. 2\"), \n  col = c(\"black\", \"firebrick\",\"magenta\"),\n  pch = c(1, NA, NA),\n  lty = c(NA, 1, 1),\n  bty = \"n\", \n  cex = 1.0, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.02,0.02))\n\n\n\n\n\nplot(ove, pch=1, main=\"Overall vaccine effectiveness\", xlab=\"day\", ylab=\"OVE\")\nlines(1 - (f*(1-tve)+(1-f)*(1-ive)), col=\"firebrick\")\nlegend(\"topright\", \n  legend = c(\"OVE definition\",\"Based on Eq. 2\"), \n  col = c(\"black\", \"firebrick\"),\n  pch = c(1, NA),\n  lty = c(NA, 1),\n  bty = \"n\", \n  cex = 1.0, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.02,0.02))\n\n\n\n\n\nplot(tve, pch=1, main=\"Total vaccine effectiveness\", xlab=\"day\", ylab=\"TVE\")\nlines(1 - (1-dve)*(1-ive), col=\"firebrick\")\nlegend(\"topright\", \n  legend = c(\"TVE definition\",\"Based on Eq. 1\"), \n  col = c(\"black\", \"firebrick\"),\n  pch = c(1, NA),\n  lty = c(NA, 1),\n  bty = \"n\", \n  cex = 1.0, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.02,0.02))"
  },
  {
    "objectID": "blog/posts/truncation-in-stan/index.html",
    "href": "blog/posts/truncation-in-stan/index.html",
    "title": "Estimating a time-to-event distribution in Stan",
    "section": "",
    "text": "Stan instead of optim\nAs in the previous post, let’s create a sample through a non-homogeneous process for the infection events and a Gamma distribution for the serial (or generation) interval.\n\nset.seed(42)\nn &lt;- 1000\ntmax &lt;- 30 # maximum time of first event\nr &lt;- 0.14 # growth rate\nX &lt;- vector(\"double\", n)\ni &lt;- 1\nct &lt;- 0\n# generate sample through a nonhomogeneous Poisson process\nwhile (ct &lt; tmax) {\n  t &lt;- rexp(1, rate=exp(r*ct))\n  ct &lt;- ct + t\n  X[i] &lt;- ct\n  i &lt;- i+1\n}\nX &lt;- X[X &gt; 0]\n\n# parameters for the serial interval\nshape_true &lt;- 2.2\nscale_true &lt;- 3.3\n\ndf &lt;- data.frame(X=X)\nsi &lt;- rgamma(length(X), shape=shape_true, scale=scale_true)\ndf$Y &lt;- df$X + si\n\ntau &lt;- 33 # truncation time\nunder_tau &lt;- df$Y &lt; tau \nnewdf &lt;- df[under_tau,]\n\nnumerator_func &lt;- function(x, y, parms){\n  exp(r*x)*dgamma(y-x, shape=parms[[1]], scale=parms[[2]])\n}\n\ndenominator_func &lt;- function(t, parms, tmax) {\n  exp(r*t)*pgamma(tmax-t, shape=parms[[1]], scale=parms[[2]])\n}\n\n# single likelihood\nll_right_trunc_exp_growth &lt;- function(parms,x,y,tmax){\n  log(numerator_func(x=x, y=y, parms=parms)) - log(integrate(denominator_func,lower=0, upper=tmax, parms=parms, tmax=tmax)$value)\n}\n\n# sum of negative log likelihoods\nnll_right_trunc_exp_growth &lt;- function(parms, X, Y, tmax){\n  sll &lt;- 0\n  for(i in seq_along(X)) {\n    sll &lt;- sll + ll_right_trunc_exp_growth(parms=parms,x=X[i],y=Y[i],tmax=tmax)\n  }\n  return(-sll)\n}\n\nres_optim = optim(par=c(1,2), \n             fn=nll_right_trunc_exp_growth, \n             X=newdf$X, \n             Y=newdf$Y,\n             tmax=tmax,\n             method = \"Nelder-Mead\",\n             control = list(maxit=2e4, reltol=1e-15))\n\n\nStan program\nGamma distribution accounting for truncation\n\nstan_code &lt;- \"\ndata {\n    int&lt;lower = 0&gt; N; // number of records\n    vector&lt;lower = 0&gt;[N] X;\n    vector&lt;lower = 0&gt;[N] Y;\n    real tau;\n}\n\nparameters {\n    real shape;\n    real scale;\n}\n\nmodel {\n    shape ~ exponential(0.1);\n    scale ~ exponential(0.1);\n    target += gamma_lpdf(Y - X | shape, 1/scale) - gamma_lcdf(tau-X | shape, 1/scale);\n}\"    \n\nGamma distribution accounting for truncation and exponential growth of infections\n\nstan_code &lt;- \"\nfunctions {\n  real denominator_density(real x,\n                           real xc,                \n                           array[] real theta,     \n                           array[] real x_r,                        \n                           array[] int x_i){\n    real shape = theta[1];\n    real scale = theta[2];\n  \n    return exp(0.14 * x) * gamma_cdf(33 - x, shape, 1/scale);\n  }\n}\ndata {\n    int&lt;lower = 0&gt; N; // number of records\n    vector&lt;lower = 0&gt;[N] X;\n    vector&lt;lower = 0&gt;[N] Y;\n    real tau;\n    real r;\n}\n\ntransformed data{     \n  array[0] real x_r;\n  array[0] int x_i;  \n} \n\nparameters {\n    real shape;\n    real scale;\n}\n\ntransformed parameters {\n  vector[N] log_exp_r;\n  for (n in 1:N)\n    log_exp_r[n] = log(exp(r*X[n]));\n}\n\nmodel {\n    shape ~ exponential(0.1);\n    scale ~ exponential(0.1);\n    \n    for (i in 1:N)\n      target += log(exp(r*X[i])) + gamma_lpdf(Y[i] - X[i] | shape, 1/scale) -                          log(integrate_1d(denominator_density, 0, tau,\n                           {shape, scale}, x_r, x_i, 1e-2));\n     \n}\"\n\n\n\nCompile and sample\nintegrate_1d(denominator_density, 0, tau, {shape, scale}, x_r, x_i, 1e-3) cause errors. Four of the two samplers generated samples if the rel_tol is increased to 1e-2 for seed=42.\n\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\nmod &lt;- stan_model(model_code=stan_code, verbose=TRUE)\ndata &lt;- list(N=nrow(newdf), X=newdf$X, Y=newdf$Y, tau=tau, r=r)\n# smp &lt;- sampling(object=mod, data=data, seed=33, chains=4, iter=2000)\n# saveRDS(smp, \"stan_trunc_smp_20231124.rds\")\n\nLet’s explore the posterior distribution.\n\nsmp &lt;- readRDS(\"stan_trunc_smp_20231124.rds\")\ndf &lt;- as.data.frame(smp)\npr &lt;- c(0.5,0.025,0.975)\nd &lt;- as.data.frame(t(apply(df[,c(\"shape\", \"scale\")],\n                           2, quantile, probs=pr)))\n\nd$name &lt;- c(\"shape\", \"scale\")\nd$true &lt;- c(shape_true, scale_true)\nd$optim &lt;- res_optim$par\nd\n\n           50%     2.5%    97.5%  name true    optim\nshape 2.085903 1.760453 2.448298 shape  2.2 2.090817\nscale 3.484215 2.680270 4.770225 scale  3.3 3.482427\n\n\nLet’s plot the results.\n\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nextrafont::loadfonts()\nggplot(d)+ \n  geom_errorbar(aes(x=name, ymin=`2.5%`, ymax=`97.5%`), width=0.0)+\n  geom_point(aes(x=name, y=`50%`, color=\"Stan\"), size=3)+\n  geom_point(aes(x=name, y=true, col=\"True value\"), size=3)+\n  geom_point(aes(x=name, y=optim, col=\"Optim\"), size=3)+\n  scale_color_manual(values=c(\"Stan\"=\"black\",\n                              \"True value\"=\"firebrick\", \"Optim\"=\"steelblue\"))+\n  labs(x=\"\", y=\"\", title=\"Median estimates with 95% CrI\")+\n  theme(legend.position=\"bottom\", legend.title=element_blank())+\n  scale_x_discrete(breaks=c(\"shape\",\"scale\"),\n                   labels=c(expression(theta[1]),expression(theta[2])))+\n  coord_flip()\n\n\n\n# ggsave(\"right_trunc_stan.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)\n\n\nd &lt;- df[, c(\"shape\",\"scale\")]\ndlong &lt;- tidyr::pivot_longer(d, cols=c(\"shape\",\"scale\"),\n                             names_to=\"param\")        \ndlong$param &lt;- as.factor(dlong$param)\nlibrary(dplyr)\nggplot(dlong)+ \n  geom_histogram(aes(x=value))+\n  facet_wrap(~param, nrow=1, scales = \"free_x\")+\n  geom_vline(data=filter(dlong, param ==\"shape\"), aes(xintercept=shape_true), color=\"firebrick\", linewidth=1.2) +\n  geom_vline(data=filter(dlong, param ==\"scale\"), aes(xintercept=scale_true), color=\"firebrick\", linewidth=1.2)"
  },
  {
    "objectID": "blog/posts/sub-exponential-growth/index.html",
    "href": "blog/posts/sub-exponential-growth/index.html",
    "title": "Sub-exponential growth",
    "section": "",
    "text": "대부분의 SIR 모형은 감염병 확산의 메커니즘을 아래와 같은 식으로 표현한다.\n\\[\\frac{\\mathrm{d}I}{\\mathrm{d}t} = \\beta S\\frac{I}{N} - \\gamma I.\\]\n말로 설명하자면 다음과 같다. 감수성자가 단위 시간 당 평균적으로 \\(\\beta\\) 의 유효 접촉, 다시 말해 감염자와 만나면 감염이 일어나게 되는 접촉을 하게 된다. 무작위로 접촉을 하는 경우 감염자를 만날 확률은 \\(\\frac{I}{N}\\) 와 같다. \\(N\\)은 총 인구 수를 의미한다. 이러한 감염병 확산 메커니즘을 frequency-dependent 하다고 말한다.\n이와 같은 메커니즘의 결과 중 하나는 감염병의 확산 초기에 감염자의 수가 exponential growth (EG)를 보인다는 것이다. 하지만 Chowell et al. 이 지적한 것처럼 실제 감염병 유행 자료를 살펴 보면 감염자수의 증가 속도는 SIR 모형이 예측하는 것 보다는 느린 속도 sub-exponential growth (SEG) 가 흔히 나타난다. 기존의 연구들에서는 크게 두 가지의 메커니즘을 들어 SEG 를 설명한다. 첫째는 어떤 이유로든 감염병이 확산되고 있는 인구집단에서 inhomogeneous mixing이 일어나는 경우이다. 왜 inhomogeneous mixing이 SEG를 나타내는지 그리고 정량적으로 어떤 관계가 있는지는 다음에 살펴보자. 이번 포스트에서는 사람들이 골고루 섞이지 않음으로 인해 감염병의 발생이 특정 지역 및 집단에 국한 되어 전체적으로는 확산 속도가 느려진다는 정도로 이해해도 되겠다. 이렇게 inhomogeneous mixing 이 나타나는 경우는 집단 내에서 특히 접촉이 많은 소규모 집단이 있다거나 (네트워크 개념을 사용하자면 clustering) 혹은 공간적으로 더 위험한 지역과 덜 위험한 지역이 있다고 가정할 수도 있겠다. Inhomogeneous mixing이외 에도 감염병 확산에 대응하여 사람들이 위험 행동을 줄여나가면 확산 속도가 점차 감소하여 SEG가 나타날 수 있다.\nSEG를 표현하는 간단한 방법 중 하나는 아래 식에서 처럼 \\(\\alpha\\)와 같은 지수를 사용하는 것이다 .\n\\[\\frac{\\mathrm{d}I}{\\mathrm{d}t} = \\beta S\\frac{I^\\alpha}{N} - \\gamma I.\\]\n이번 포스팅에서는 SEG을 SEIR 모형을 이용하여 구현하고 모수 추정 후 EG 모형과 비교해보고자 한다.\n우선 아래와 같이 SEIR 모형을 구현한다.\n\nseir_ode &lt;- function(t, y, params) {\n  # state variables \n  S &lt;- y[\"S\"]; \n  E &lt;- y[\"E\"]; \n  I &lt;- y[\"I\"]; \n  R &lt;- y[\"R\"];\n  CI &lt;- y[\"CI\"]\n  \n  epsilon &lt;- params[[\"epsilon\"]] # 1/epsilon = latent period\n  gamma &lt;- params[[\"gamma\"]] # 1/gamma = duration of infectiousness\n  beta &lt;- params[[\"beta\"]] # R0 = beta/gamma\n  alpha &lt;- params[[\"alpha\"]]\n  N &lt;- S + E + I + R # total population size\n  muSE &lt;- beta * S * (I^(alpha)) / N # rate from S to E\n  muEI &lt;- epsilon * E # rate from E to I, i.e., 1/epsilon = latent period\n  muIR &lt;- gamma * I # rate from I to R\n  \n  dS &lt;- - muSE # rate of change for S\n  dE &lt;- muSE - muEI # rate of change for E\n  dI &lt;- muEI - muIR # rate of change for I\n  dR &lt;- muIR # rate of change for R\n  dCI &lt;- muSE # rate of change for R\n  \n  return(list(c(dS, dE, dI, dR, dCI))) # return as a list to use deSolve package\n}\n\n초기 조건을 정의하자. 이 값들은 스크립트에서 global 변수로 사용할 것이다.\n\n# initial conditions as global variables\nI0 &lt;- 10 # initially infected people\ny0 &lt;- c(S=10000 - I0, E=0, I=I0, R=0, CI=0) # initial values for state variables\ntend &lt;- 100 # simulation end time 50 days\ntimes &lt;- seq(0, tend, by=1) # daily output for 150 days\n\n기본 모수를 정의하는 함수와 단위 시간 당 발생자 수를 조사하는 함수를 정의하자. 이 것들은 나중에 모수 추정과정에서 사용될 것이다.\n\n# baseline parameters\npinit &lt;- function(beta=0.3, alpha=1) {\n  params &lt;- list() # parameter input for the SIR model\n  params$epsilon &lt;- 0.5\n  params$gamma &lt;- 0.2\n  params$beta &lt;- beta\n  params$alpha &lt;- alpha\n  \n  return(params)\n}\n\nlibrary(dplyr)\nlibrary(deSolve)\n# incidence over a given time interval, delta_t\nincidence &lt;- function(p, delta_t=7) {\n  parm = pinit()\n  parm$beta &lt;- p[[\"beta\"]]\n  if(length(p) &gt; 1) {\n     parm$alpha &lt;- p[[\"alpha\"]]\n  }\n  ode(y=y0, times=times, func=seir_ode, parms=parm) %&gt;%\n  as.data.frame() -&gt; out\n  di = c(0, diff(out$CI))\n\n  return(di[seq(1, length(di), by=delta_t)])\n}\n\n아래와 같이 모수 추정에 사용할 거짓 자료를 만들어 보자. 전에 사용하 듯이 관찰값은 모형 예측값을 모수로 가지는 푸아송 변수로 가정하자. 기본 값으로 한 주간에 감염자수를 자료로 사용한다. 그림에서 원들은 거짓 관찰값을 점선은 모형 예측값을 나타낸다. 검정색과 빨강색은 각각 EG와 SEG를 나타낸다.\n\n# create fake data\nset.seed(42) # for reproducibility\n\ninc1 = incidence(p=pinit()) # baseline parameters, i.e., alpha=1\ndat1 &lt;- rpois(length(inc1), lambda=inc1)\nplot(dat1, xlab=\"Week\", ylab=\"Number of cases\", main=\"Exponential growth\")\nlines(inc1, col=2)\n\n\n\ninc2 = incidence(p=pinit(beta=0.5, alpha=0.8)) \ndat2 &lt;- rpois(length(inc2), lambda=inc2)\nplot(dat2, xlab=\"Week\", ylab=\"Number of cases\", main=\"Sub-exponential growth\")\nlines(inc2, col=2)\n\n\n\n\n거짓 자료를 이용하여 모수 추정을 하여 보자. 모수 추정 함수를 정의 하기 전에 아래와 같이 간단한 함수들을 정의하자. \\(\\mathrm{expit}\\)은 0과 1사이의 값만 정의되는 모수 (i.e., \\(\\alpha\\)) 를 사용하기 위해서 정의 하였고 이를 다시 원래값으로 되 돌리는 데 사용할 \\(\\mathrm{logit}\\)을 정의하였다. \\(\\mathrm{AIC}\\) (Akaike information criterion)는 모형의 우수함 (quality)의 상대 비교를 위해 사용한다. AIC 는 사용된 모형이 실제 자료를 만들어낸 모형과 다름으로 인해 잃어버리게 되는 정보의 양을 나타내는 상대적인 값이다. \\(\\mathrm{AIC}\\) 값이 작을 수록 정보를 덜 잃어버렸다는 뜻으로 더 우수한 모형을 나타낸다고 할 수 있다. \\(\\mathrm{AIC_c}\\) 는 모수 추정 시 사용된 자료의 수가 적은 경우에 더 적합한 방법이다.\n\\[ \\mathrm{logit}(p) := \\mathrm{ln} (\\frac{p}{1-p})\\] \\[\\mathrm{expit}(x) :=  \\frac{1}{1+\\mathrm{exp}(-x)}\\] \\[\\mathrm{AIC} := 2k - 2\\mathrm{ln}(\\hat{L})\\] \\[ \\mathrm{AIC_c} := \\mathrm{AIC} + \\frac{2 k^2+2 k}{n-k-1}\\]\n\nexpit &lt;- function(x) {\n  1/(1+exp(-x))  \n}\nlogit &lt;- function(x) {\n  log(x/(1-x))  \n}\naic = function(k, L){\n  2*k - 2*log(L)\n}\naicc = function(k, L, n){\n  2*k - 2*log(L) + (2*k^2+2*k)/(n-k-1)\n}\n\n모형 에측값과 관찰값과의 유사성을 측정하기 위해 likelihood 함수를 사용하자. 함수를 최소화 하는 optim의 기본 기능을 사용할 것이기 때문에 negative log likelihood를 정의한다. SEG 모형의 경우 \\(\\beta, \\alpha\\) 두 개의 함수를 추정하자.\n\nnegloglik &lt;- function (p, y) {\n  if (length(p) == 1) x &lt;- incidence(p=pinit(beta=exp(p[1])))\n  if (length(p) == 2) x &lt;- incidence(p=pinit(beta=exp(p[1]), alpha=expit(p[2])))\n  nll &lt;-  - sum(dpois(y, lambda=x, log=T), na.rm=T)\n  return(nll)\n}\n\n이번에는 이번 포스팅의 주제인 SEG 모형으로 만든 거짓자료를 EG 그리고 SEG 모형 두 가지로 최적화하여 보자.\n\nfit2 = optim(par=log(0.3), fn=negloglik, y=dat2, method=\"Brent\", lower=log(1e-6), upper=log(0.9))\nfit3 = optim(par=c(log(0.3), logit(0.2)), fn=negloglik, y=dat2, method=\"Nelder-Mead\")\n\n# check log likelihood\n-fit2$value\n\n[1] -54.8522\n\n-fit3$value\n\n[1] -33.3477\n\nplot(dat2, xlab=\"Week\", ylab=\"Number of cases\")\nlines(incidence(p=pinit(beta=exp(fit2$par[1]))), col=2, lty=2)\nlines(incidence(p=pinit(beta=exp(fit3$par[1]), alpha=expit(fit3$par[2]))), col=3, lty=2)\nlegend(1, 17, legend=c(\"Data\", \"Exponental fit\", \"Sub-exponential fit\"),\n       col=c(\"black\", \"red\", \"green\"), lty=c(NA,2.2), pch=c(1,NA,NA), cex=0.8)\n\n\n\n\n플롯을 살펴보았을때 SEG 모형을 이용한 피팅이 더 잘 맞아들어가는 것 같은데 \\(\\mathrm{AIC}\\)를 이용해서 모형을 비교해보자. \\(\\mathrm{AIC}\\) 와 \\(\\mathrm{AIC_c}\\) 둘 다 SEG 모형이 자료를 더 잘 설명하는 모형임을 보여준다.\n\n# Akaike information criterion to compare models\naic(k=1, L=exp(-fit2$value)) \n\n[1] 111.7044\n\naic(k=2, L=exp(-fit3$value)) \n\n[1] 70.6954\n\naicc(k=1, L=exp(-fit2$value), n=length(dat2))\n\n[1] 112.0121\n\naicc(k=2, L=exp(-fit3$value), n=length(dat2))\n\n[1] 71.6954"
  },
  {
    "objectID": "blog/posts/sir_deSolve_odin_diffeqr/index.html",
    "href": "blog/posts/sir_deSolve_odin_diffeqr/index.html",
    "title": "SIR model benchmarks: deSolve, odin, and diffeqr",
    "section": "",
    "text": "deSolve package\n\nsir_deSolve &lt;- function(t, u, p){\n  N &lt;- sum(u)\n  du1 &lt;- - p[1]*u[1]*u[2]/N\n  du2 &lt;- + p[1]*u[1]*u[2]/N - p[2]*u[2]\n  du3 &lt;- + p[2]*u[2]\n    \n  return(list(c(du1,du2,du3))) \n}\n\nlibrary(deSolve)\nu0 &lt;- c(0.99, 0.01, 0)\ntspan &lt;- seq(from=0, to=50)\np &lt;- c(0.4, 0.2)\n\noutdf &lt;- as.data.frame(ode(y=u0, times=tspan, func=sir_deSolve, parms=p))\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(outdf,aes(x=time))+\n  geom_line(aes(y=`1`, color=\"S\")) +\n  geom_line(aes(y=`2`, color=\"I\")) +\n  geom_line(aes(y=`3`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\nManual C++\nEuler method was implemented\n\nsir_cpp &lt;- '\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nList sir_cpp(List params) {\n  double tau = params[\"tau\"]; // time step size\n  double ndays = params[\"ndays\"]; // number of days for output\n  int nsubsteps = ceil(1/tau);\n  \n  NumericVector S(ndays+1);\n  NumericVector I(ndays+1);\n  NumericVector R(ndays+1); \n  NumericVector time(ndays+1);\n  \n  S(0) = params[\"susceptible\"];\n  I(0) = params[\"infectious\"];\n  R(0) = params[\"recovered\"];\n  \n  double b = params[\"b\"]; // transmission rate per unit of time\n  double g = params[\"g\"]; // recovery rate\n  \n  for (int day = 0; day &lt; ndays; day++) {\n    double St = S[day];\n    double It = I[day];\n    double Rt = R[day];\n    \n    for (int substep = 0; substep &lt; nsubsteps; substep++){\n      double N = St + It + Rt;\n      double foi = b * It / N;\n\n      double StoI = St * foi * tau;\n      double ItoR = It * g * tau;\n\n      double dS = - StoI;\n      double dI = + StoI - ItoR;\n      double dR = + ItoR;\n\n      St = St + dS;\n      It = It + dI;\n      Rt = Rt + dR;\n    }\n    // Update next timestep\n    S[day + 1] = St;\n    I[day + 1] = It;\n    R[day + 1] = Rt;\n    time[day + 1] = day+1;\n  }\n\n  DataFrame result = DataFrame::create(\n    Named(\"time\") = time,\n    Named(\"S\") = S,\n    Named(\"I\") = I,\n    Named(\"R\") = R);\n\n  return result;\n}'\n\n\nRcpp::cppFunction(code=sir_cpp)\n\nparams &lt;- list()\nparams &lt;- within(params, {\n  tau &lt;- 0.1 # in days\n  ndays &lt;- 50\n\n  b &lt;- 0.4\n  g &lt;- 0.2\n  \n  susceptible &lt;- 0.99\n  infectious &lt;- 0.01\n  recovered &lt;- 0.0\n})\n\nout_cpp &lt;- sir_cpp(params)\n\nggplot(out_cpp, aes(x=time))+\n  geom_line(aes(y=`S`, color=\"S\")) +\n  geom_line(aes(y=`I`, color=\"I\")) +\n  geom_line(aes(y=`R`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\nodin package\n\nlibrary(odin)\nsir_odin &lt;- odin::odin({\n  ## Derivatives\n  deriv(S) &lt;- -b*S*I/(S+I+R)\n  deriv(I) &lt;- b*S*I/(S+I+R)-g*I\n  deriv(R) &lt;- g*I\n\n  ## Initial conditions\n  initial(S) &lt;- 0.99\n  initial(I) &lt;- 0.01\n  initial(R) &lt;- 0.00\n\n  ## parameters\n  b &lt;- user(0.4)\n  g &lt;- user(0.2)\n})\n\nsir_mod &lt;- sir_odin$new(b=0.4, g=0.2)\ntspan &lt;- seq(from=0, to=50)\nout_odin &lt;- as.data.frame(sir_mod$run(tspan))\n\nggplot(out_odin, aes(x=t))+\n  geom_line(aes(y=`S`, color=\"S\")) +\n  geom_line(aes(y=`I`, color=\"I\")) +\n  geom_line(aes(y=`R`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\ndiffeqr package\n\nlibrary(diffeqr)\nde &lt;- diffeqr::diffeq_setup()\n\nsir_julia &lt;- function(u, p, t){\n  N = sum(u)\n  du1 = - p[1]*u[1]*u[2]/N\n  du2 = + p[1]*u[1]*u[2]/N - p[2]*u[2]\n  du3 = + p[2]*u[2]\n    \n  return(c(du1,du2,du3))\n}\n\nu0 &lt;- c(0.99, 0.01, 0.0)\ntspan &lt;- c(0.0, 50.0)\np &lt;- c(0.4, 0.2)\nprob &lt;- de$ODEProblem(sir_julia, u0, tspan, p)\nprob_jit &lt;- diffeqr::jitoptimize_ode(de, prob)\n\nsol &lt;- de$solve(prob_jit, de$Tsit5(), saveat=1)\n\nmat &lt;- sapply(sol$u, identity)\nudf &lt;- as.data.frame(t(mat))\ntudf &lt;- cbind(data.frame(t=sol$t), udf)\n\nggplot(tudf, aes(x=t))+\n  geom_line(aes(y=V1, color=\"S\")) +\n  geom_line(aes(y=V2, color=\"I\")) +\n  geom_line(aes(y=V3, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n# ggsave(\"diffeqr_sir.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)  \n\n\n\nBenchmarks\nThe diffeqr is the most efficient tool for running the SIR model in its ODE form.\nAs a side note, the test gave an unfair advantage to the sir_cpp due to the use of the Euler method with 0.1 day step size. In fact, deSolve::ode outperformed sir_cpp when it was used with method=\"euler\". Therefore there is no need to write manually CPP models if an ODE solver is what you need. The situation might differ when implementing a stochastic model, which will be discussed in a later post.\n\nlibrary(microbenchmark)\n\nbenchmark = microbenchmark(\n  deSolve = ode(y=u0, times=tspan, func=sir_deSolve, parms=p),\n  cpp = sir_cpp(params),\n  odin = sir_mod$run(tspan),\n  julia = de$solve(prob_jit, de$Tsit5(), saveat=1),\n  times = 1000\n)\n\n\nbenchmark &lt;- readRDS(\"benchmark.rds\")\nlibrary(dplyr)\nbenchmark |&gt; \n  group_by(expr) |&gt;\n  mutate(sec=time/1000) |&gt; \n  summarize(\n    lower_sec = quantile(sec, probs=0.025),\n    median_sec = quantile(sec, probs=0.5),\n    upper_sec = quantile(sec, probs=0.975))\n\n# A tibble: 4 × 4\n  expr    lower_sec median_sec upper_sec\n  &lt;fct&gt;       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 deSolve      520.       575.     1020.\n2 cpp          228.       309.      639.\n3 odin         199.       285.     1054.\n4 julia        102.       179.      344."
  },
  {
    "objectID": "blog/posts/simple-model-complicated-dynamics/index.html",
    "href": "blog/posts/simple-model-complicated-dynamics/index.html",
    "title": "Simple mathematical models with very complicated dynamics",
    "section": "",
    "text": "Simple mathematical models with very complicated dynamics\nRobert M. May Nature Vol. 261 June 10, 1976\nThis article discusses a simple first order difference equations that can display very complicated dynamics.\n\\[X_{t+1} = F(X_t)\\]\nIn biological population, the nonlinear function \\(F(x)\\) often has the following properties. \\(F(0)=0\\); \\(F(x)\\) increases monotonically as \\(X\\) increases through the range of \\(0&lt;X&lt;A\\) (with \\(F(x)\\) attaining its maximum value at \\(X=A\\)); \\(F(X)\\) decreases monotonically as \\(X\\) increases beyond \\(X=A\\) \\[N_{t+1} = N_t(a-bN_t)\\]\n\\[X_{t+1} = a X_t (1-X_t)\\]\nX must remain on the interval \\(0&lt;X&lt;1\\); if \\(X\\) ever exceeds unity, subsequent iterations diverge towards \\(-\\infty\\). Furthermore, \\(F(X)\\) attains a maximum value of \\(a/4\\) at \\(X=1/2\\); the equation therefore possesses non-trivial dynamical behaviour only if \\(a&lt;4\\). On the other hand, all trajectories are attracted to \\(X=0\\) if \\(a&lt;1\\).\n\n# function to compute the value at the next time step\n# 0 &lt; x &lt; 1\n# a &lt; 1 for x to go to zero\n# a &gt; 4 leads to x &gt; 1 at one point, which then leads to - infinity\n# 1 &lt; a &lt; 4 for x to exhibit non-trivial dynamics\nx_next &lt;- function(a, x){\n  a*x*(1-x)\n}\n\nx0 = seq(0.01, 0.99, 0.01)\na = c(2.707, 3.414) # values were adopted from the paper by May Nature Vol. 261 June 10, 1976\nxnext = sapply(x0, function(x) x_next(a, x))\n\nplot(x0, xnext[1,], type='l', ylim=c(0,1), xlim=c(0,1),\n     xlab=expression(X[t]), ylab=expression(X[t+1]))\nlines(x0, xnext[2,])\nlines(0:1, 0:1) # line y = x\n\nxstar = 1 - 1/a # points where X(t+1) = X(t)\npoints(xstar[1], xstar[1])\npoints(xstar[2], xstar[2], col=2)\n# slope at the point x given a\ndx &lt;- function(a,x){\n  -2*a*x+a\n}\n\n# function to compute intercept at the given slope b and point x\nintcpt = function(b,x){\n  x - b*x\n}\n\nabline(a=intcpt(b=dx(a=a,x=xstar[1]),x=xstar[1]), b=dx(a=a,x=xstar[1]), lty=2)\nabline(a=intcpt(b=dx(a=a,x=xstar[2]),x=xstar[2]), b=dx(a=a,x=xstar[2]), lty=2, col=2)\n\n\n\n\n\nx_iter &lt;- function(a, x, iter, func){\n  xvec = rep(NA, iter)\n  xvec[1] = x\n  for(i in 2:iter){\n    xvec[i] = func(a, xvec[i-1])  \n  }\n  return(xvec)\n}\nplot(x_iter(2.9, 0.8, 100, x_next), type=\"l\")\n\n\n\n\n\\(X_{t+1} = X_t \\textrm{exp}[r(1-X_t)]\\)\n\nx_next_exp &lt;- function(r, x){\n  x*exp(r*(1-x))\n}\n\nplot(x_iter(2, 0.8, 100, x_next_exp), type=\"l\")\n\n\n\n\n\nx_next2 &lt;- function(r, x){\n x1 &lt;- a*x*(1-x)\n x2 &lt;- a*x1*(1-x1)\n return(x2)\n}\n\nxnext = sapply(x0, function(x) x_next2(a, x))\n\nplot(x0, xnext[1,], type='l', ylim=c(0,1), xlim=c(0,1),\n     xlab=expression(X[t]), ylab=expression(X[t+2]))\nlines(x0, xnext[2,])\nlines(0:1, 0:1) # line y = x"
  },
  {
    "objectID": "blog/posts/seir-models/index.html",
    "href": "blog/posts/seir-models/index.html",
    "title": "SEIR model",
    "section": "",
    "text": "Susceptible-Exposed-Infective-Recovered (SEIR) 모형\nSEIR 모형은 잠복기가 어느 정도 긴 감염병 (예를 들어 코로나19)의 전파를 모형하는 데 사용한다. 이번 포스트에서는 SEIR 모형을 만드는 방법을 알아본다. 결정론적 (deterministic) 그리고 확률론적 (stochastic) 방법으로 SEIR 모형을 R언어로 만들어 본다.\n\nDeterministic model\n결정론적 모형은 주로 미분식 (differential equation)을 이용하여 구현한다. \\[\\begin{equation} \\begin{split}  \\frac{dS}{dt} &= - \\beta S\\frac{I}{N}\\\\ \\frac{dE}{dt} &= \\beta S\\frac{I}{N} - \\epsilon E\\\\ \\frac{dI}{dt} &= \\epsilon E - \\gamma I\\\\ \\frac{dR}{dt} &= \\gamma I \\end{split} \\end{equation}\\]\n\nseir_ode &lt;- function(t, y, params) {\n  # state variables \n  S &lt;- y[1]; E &lt;- y[2]; I &lt;- y[3]; R &lt;- y[4];\n  beta &lt;- params[[\"beta\"]] # beta = transmission rate\n  epsilon &lt;- params[[\"epsilon\"]] # 1/epsilon = latent period\n  gamma &lt;- params[[\"gamma\"]] # 1/gamma = duration of infectiousness\n  \n  N &lt;- S + E + I + R # total population size\n  muSE &lt;- beta * S * I / N # rate from S to E\n  muEI &lt;- epsilon * E # rate from E to I, i.e., 1/epsilon = latent period\n  muIR &lt;- gamma * I # rate from I to R\n  \n  dS &lt;- - muSE # rate of change for S\n  dE &lt;- muSE - muEI # rate of change for E\n  dI &lt;- muEI - muIR # rate of change for I\n  dR &lt;- muIR # rate of change for R\n  \n  return(list(c(dS, dE, dI, dR))) # return as a list to use deSolve package\n}\n\n미분식을 적분하여 SEIR 변수들의 시간에 따른 추이를 살펴보자. 적분은 deSolve 패키지의 ode 함수를 이용한다.\n\nI0 &lt;- 0.01 # initially infected people\ny0 &lt;- c(S = 1 - I0, E = 0, I = I0, R = 0) # initial values for state variables\nparams &lt;- list() # parameter input for the SIR model\nparams$epsilon &lt;- 0.5\nparams$gamma &lt;- 0.2\nparams$beta &lt;- 0.4  \ntend &lt;- 100 # simulation end time 50 days\ntimes &lt;- seq(0, tend, by = 1) # daily output for 150 days\n\n# ODE integration using the deSolve package\nlibrary(deSolve)\nlibrary(dplyr) # to use %&gt;%\node(y=y0, times=times, func=seir_ode, parms=params) %&gt;%\n  as.data.frame() -&gt; out\nlibrary(tidyr) # turn the data into a long format for easier plot\noutlong &lt;- out %&gt;% pivot_longer(cols=2:5, names_to = \"State\")\n\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nextrafont::loadfonts()\n\nggplot(outlong, aes(x=time, y=value, color=State)) +\n  geom_line(linewidth = 1.2) +\n  labs(x = 'Time (day)', y = 'Proportion')+\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") \n\n\n\n\n\n\n확률론적 모형\n두 가지 방식으로 확률론적 모형을 구현하여 본다. 첫번째는 \\(\\tau\\)-leaping 방법과 유사하나 푸아송 분포 대신 binomial 분포를 사용한다. 푸아송 분포와 달리 상한선이 정해지므로 각 상태 변수가 음수로 가는 것을 막을 수 있는 잇점이 있다. S에서 E로 단위 시간 \\(\\delta\\) 동안 이동하는 수는 아래와 같이 정해진다. \\[\\begin{equation} \\begin{split}  \\Delta N_{SE} &= \\textrm{Binomial}\\left( S(t), 1-\\textrm{exp}[{-r_{SE}\\delta }]\\right) \\\\ S(t+\\delta) &= S(t) - \\Delta N_{SE}\\ \\end{split} \\end{equation}\\] 비슷한 방법으로 \\(E\\)에서 \\(I\\) 그리고 \\(I\\)에서 \\(R\\)로 변하는 수를 계산하여 아래와 같이 구현한다.\n\nseir_stoch_step &lt;- function (y, params, delta) {\n  \n  beta &lt;- params[[\"beta\"]]\n  epsilon &lt;- params[[\"epsilon\"]]\n  gamma &lt;- params[[\"gamma\"]] \n\n  S &lt;- y[\"S\"]; E &lt;- y[\"E\"]; I &lt;- y[\"I\"]; R &lt;- y[\"R\"];\n\n  N &lt;- S + E + I + R\n  rSE &lt;- beta * I / N\n  rEI &lt;- epsilon\n  rIR &lt;- gamma\n  # number of events over the time step, delta, modeled as binomial random variable     \n  nSE &lt;- rbinom(1, S, 1 - exp(- rSE * delta))\n  nEI &lt;- rbinom(1, E, 1 - exp(- rEI * delta))\n  nIR &lt;- rbinom(1, I, 1 - exp(- rIR * delta))\n\n  dSdt &lt;- - nSE\n  dEdt &lt;- nSE - nEI\n  dIdt &lt;- nEI - nIR\n  dRdt &lt;- nIR\n  dCEdt &lt;- nSE\n  dCIdt &lt;- nEI\n\n return (list(c(dSdt, dEdt, dIdt, dRdt)))\n}\n\n위 함수는 한 번의\\(\\delta\\)동안 변화를 출력하기 때문에 원하는 기간 동안 연속해서 계산하기 위해 아래와 같은 함수를 추가적으로 만든다.\n\nstoch_solve &lt;- function(func, y, times, params, delta) {\n  # times indicate the times for which we want to see outputs\n  out &lt;- data.frame(matrix(NA, nrow = length(times), ncol = (length(y)+1)))\n  out[1, ] &lt;- c(times[1], y)\n  row &lt;- 2\n  \n  substeps &lt;- round((times[2]-times[1])/delta)\n  for (t in 1:(length(times)-1)) {\n    for (t2 in 1:substeps) {\n      y &lt;- y + unlist(func(y, params, delta))\n    }\n    out[row, ] &lt;- c(t, y)\n    row &lt;- row + 1\n  }\n  names(out) &lt;- c(\"time\", names(y))\n  return (out)\n}\n\n위 stoch_solve 함수를 이용하여 계산하고 플롯팅을 해본다. ODE 모형의 결과는 proportion으로 주어져 있으니 1,000을 곱한 후 비교하면 결과가 크게 다르지 않음을 알 수 있다. stoch_solve를 여려 번 실행하여 평균을 비교하면 그리고\\(\\delta\\)을 작게 할 수록 ODE 모형의 결과와 가까워진다.\n\nres &lt;- stoch_solve(func = seir_stoch_step, y=1000*y0, times=0:100, params = params, delta=0.2)\nreslong &lt;- pivot_longer(res, cols=2:5, names_to = \"State\")\n\nggplot(reslong, aes(x = time, y = value, color = State)) +\n  geom_line(linewidth = 1.2) +\n  labs(x = 'Time (day)', y = 'Number')+\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") \n\n\n\n\n\n\nGillespie algorithm\n위에서 기술한 확률론적 방법은 우리가 이미 정한 time interval \\(\\delta\\)에 따라 오차가 발생하는 반면 Gillespie algorithm 을 이용해서 통계적으로 정확한 stochastic simulation 을 할 수 있다.\n\nseir_gillespie &lt;- function(y, params) {\n  S &lt;- y[\"S\"]\n  E &lt;- y[\"E\"]\n  I &lt;- y[\"I\"]\n  R &lt;- y[\"R\"]\n  \n  beta &lt;- params[[\"beta\"]]\n  epsilon &lt;- params[[\"epsilon\"]]\n  gamma &lt;- params[[\"gamma\"]]\n  \n  N &lt;- S + E + I + R\n  event_occurred &lt;- FALSE\n  tau &lt;- 0\n  if (I &gt; 0 & S &gt; 0) {## no need to proceed if no one is infectious or no one is susceptible\n    rate_StoE &lt;- beta * S * I / N\n    rate_EtoI &lt;- epsilon * E\n    rate_ItoR &lt;- gamma * I\n    \n    rate_all &lt;- c(rate_StoE, rate_EtoI, rate_ItoR) # event rates\n    tau &lt;- rexp(1, rate = sum(rate_all)) # time to the next event\n    event &lt;- sample(length(rate_all), 1, prob = rate_all) # next event\n    if (event == 1) {\n      S &lt;- S - 1\n      E &lt;- E + 1\n    }\n    else if (event == 2) {\n      E &lt;- E - 1\n      I &lt;- I + 1\n    }\n    else if (event == 3) {\n      I &lt;- I - 1\n      R &lt;- R + 1\n    }\n    event_occurred &lt;- TRUE;\n  }\n  return (list(y = c(S, E, I, R),\n               tau = tau,\n               event_occurred = event_occurred))\n}\n\nseir_gillespie는 한 번의 event 후 결과를 출력하므로 아래와 같이 추가적인 함수를 구성하여 시물레이션을 한다.\n\nrun_seir_gillespie &lt;- function(func, tend, y, params, report_dt = 1) {\n  res &lt;- data.frame(time = 0, t(y)) # store the simulation results\n  t &lt;- 0\n  yt &lt;- y\n  while (t &lt; tend) {\n    sim &lt;- func(y = yt, params = params) # one event according to the Gillespie algorithm\n    t &lt;- t + sim$tau\n    yt &lt;- sim$y\n    if (t &gt;= report_dt) { # add to the result only when the t is reaches report dt\n      res &lt;- rbind(res, c(t, t(yt)))\n      report_dt &lt;- report_dt  + 1\n    }\n    if (!sim$event_occurred)\n      break\n  }\n  return (res)\n}\n\n시물레이션 결과를 플롯팅 한다.\n\nres &lt;- run_seir_gillespie(func = seir_gillespie, \n                     tend = tend, \n                     y = y0 * 1000, \n                     params = params, \n                     report_dt = 1)\n\nreslong &lt;- pivot_longer(res, cols=2:5, names_to = \"State\")\n\nggplot(reslong, aes(x = time, y = value, color = State)) +\n  geom_line(linewidth = 1.2) +\n  labs(x = 'Time (day)', y = 'Number')+\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") + \n  ggtitle(\"Gillespie algorithm\")\n\n\n\n# ggsave(\"gillespie.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "blog/posts/right-truncation2/index.html",
    "href": "blog/posts/right-truncation2/index.html",
    "title": "Estimating a time-to-event distribution from right-truncated data",
    "section": "",
    "text": "Seamen writes: Data on time to an event are said to be right truncated if they come from a set of individuals who have been randomly sampled from a population using a sampling mechanism that selects only individuals who have experienced the event by a given time, called the truncation time.\nThe analysis of right-truncated data requires statistical methods that account for the fact that each of the sampled individuals must have experienced the final event by their truncation time.\nLet’s assume that we are interested in estimating the serial interval and let \\(X\\) and \\(Y\\) denote the times of symptom onset of the infector and the infectee, with \\(0\\leq X \\leq Y\\). Let \\(T=Y-X\\) denote the serial interval. Let \\(f_T^∗(t)\\) and \\(F_T^∗(t)\\) denote, respectively, the probability density (or mass) function of \\(T\\) and the distribution function of \\(T\\). We obtain an i.i.d. sample, \\((x_1, t_1), . . . , (x_n, t_n)\\), from the probability distribution of \\((X, T)\\) given \\(X + T ≤ \\tau\\) for some \\(\\tau &gt; 0\\).\n\\[f_{X, T}(x,t|X+T \\leq \\tau) = \\frac{f_X(x) f^*_T(t)I(x+t \\leq \\tau)}{\\int_0^{\\tau} f_X(x') F^*_T(\\tau-x')\\text{d}x'}\\] , where \\(f_X (x)\\) denotes the conditional probability density (or mass) function of \\(X\\) given \\(X \\leq \\tau\\), and \\(I(\\cdot)\\) denotes the indicator function. If \\(X\\) and \\(T\\) are discrete, we shall assume, without loss of generality, that \\(\\tau\\) is an integer. We should like to estimate \\(F^∗_T(t)\\)\nIf we assume no truncation (i.e., \\(F^*_T(\\tau-x)=1~ \\forall x\\))\n\nDuring an initial period of an epidemic\nSeamen provides two approaches to formulating a likelihood that can be applied during the initial period of an epidemic. The first approach is: \\[f_X (x;r) =  \\frac{\\text{exp}(rx)}{\\int_0^{\\tau}\\text{exp}(rs)ds }=\\frac{r\\text{exp}(rx)}{\\text{exp}(r\\tau) - 1 } \\propto \\text{exp}(rx)\\], and \\(T \\sim \\text{Gamma}(\\theta_1, \\theta_2)\\).\n\n\nExperiment\nLet’s first create samples where infectees (\\(X\\)) are created through a non-homogeneous Poisson process where rate, \\(h\\), is modeled as an exponential growth with a rate, \\(r\\), (i.e., \\(h=\\text{exp}(rt)\\).\n\nset.seed(42)\nn &lt;- 1000\ntmax_i &lt;- 30 # first events that happened only before time 30\nr &lt;- 0.14 # growth rate\nX &lt;- vector(\"double\", n)\ni &lt;- 1\nct &lt;- 0\n# generate sample through a nonhomogeneous Poisson process\nwhile (ct &lt; tmax_i) {\n  t &lt;- rexp(1, rate=exp(r*ct))\n  ct &lt;- ct + t\n  X[i] &lt;- ct\n  i &lt;- i+1\n}\nX &lt;- X[X &gt; 0]\n\n# parameters for the serial interval\nshape_true &lt;- 2.2\nscale_true &lt;- 3.3\n\ndf &lt;- data.frame(X=X)\nsi &lt;- rgamma(length(X), shape=shape_true, scale=scale_true)\ndf$Y &lt;- df$X + si\n\ntmax &lt;- 33 # truncation time\nunder_tmax &lt;- df$Y &lt; tmax \nnewdf &lt;- df[under_tmax,]\n\nnll &lt;- function(parms, X, Y) -sum(dgamma(Y-X, shape=parms[[1]], scale=parms[[2]], log=TRUE))\nres1 = optim(par=c(1,2), fn=nll, X=newdf$X, Y=newdf$Y,\n             method = \"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\n\n# \nnll_right_trunc &lt;- function(parms, X, Y, tmax) -sum(log(dgamma(Y-X, shape=parms[[1]], scale=parms[[2]])/pgamma(tmax-X, shape=parms[[1]], scale=parms[[2]])))\n\n# the following would not work. why?\n# nll_right_trunc &lt;- function(parms, X, Y, tmax) -sum(log(dgamma(Y-X, shape=parms[[1]], scale=parms[[2]])/pgamma(tmax, shape=parms[[1]], scale=parms[[2]])))\n\nres2 = optim(par=c(1,2), \n             fn=nll_right_trunc, \n             X=newdf$X, \n             Y=newdf$Y,\n             tmax=tmax,\n             method = \"Nelder-Mead\",\n             control = list(maxit=2e4, reltol=1e-15))\n\n\n\nExponential growth for \\(X\\)\n\nnumerator_func &lt;- function(x, y, parms){\n  exp(r*x)*dgamma(y-x, shape=parms[[1]], scale=parms[[2]])\n}\n\n# using the full probability density function\n# numerator_func &lt;- function(x, y, parms){\n#   exp(r*x)/(exp(r*tmax)-1)*dgamma(y-x, shape=parms[[1]], scale=parms[[2]])\n# }\n\n# uniform distribution - same as the vanilla truncation assumption\n# numerator_func &lt;- function(x, y, parms){\n#   dgamma(y-x, shape=parms[[1]], scale=parms[[2]])\n# }\n\n# NB: x is not used in this formuation but is included as params to make consistent w/ other alternative formulations\ndenominator_func &lt;- function(t, x, parms, tmax) {\n  exp(r*t)*pgamma(tmax-t, shape=parms[[1]], scale=parms[[2]])\n}\n\n# the following would not give the correct answer. why?\n# denominator_func &lt;- function(t, x, parms, tmax) {\n#   exp(r*t)*pgamma(tmax-x-t, shape=parms[[1]], scale=parms[[2]])\n# }\n\n# same as the vanialla truncation model\n# denominator_func &lt;- function(t, x, parms, tmax) {\n#   dgamma(tmax-x-t, shape=parms[[1]], scale=parms[[2]])\n# }\n\n# single likelihood\nll_right_trunc_exp_growth &lt;- function(parms,x,y,tmax){\n  log(numerator_func(x=x, y=y, parms=parms)) - log(integrate(denominator_func,lower=0, upper=tmax, x=x, parms=parms, tmax=tmax)$value)\n}\n\n# sum of negative log likelihoods\nnll_right_trunc_exp_growth &lt;- function(parms, X, Y, tmax){\n  sll &lt;- 0\n  for(i in seq_along(X)) {\n    sll &lt;- sll + ll_right_trunc_exp_growth(parms=parms,x=X[i],y=Y[i],tmax=tmax)\n  }\n  return(-sll)\n}\n\nres3 = optim(par=c(1,2), \n             fn=nll_right_trunc_exp_growth, \n             X=newdf$X, \n             Y=newdf$Y,\n             tmax=tmax,\n             method = \"Nelder-Mead\",\n             control = list(maxit=2e4, reltol=1e-15))\n\nparmdf &lt;- data.frame(true = c(shape_true, scale_true, shape_true*scale_true))\nparmdf$no_trunc &lt;- c(res1$par, prod(res1$par))\nparmdf$trunc &lt;- c(res2$par, prod(res2$par))\nparmdf$trunc_exp_growth &lt;- c(res3$par, prod(res3$par))\n\nparmdf\n\n  true no_trunc    trunc trunc_exp_growth\n1 2.20 2.115872 1.932797         2.100084\n2 3.30 2.273549 3.684688         3.427339\n3 7.26 4.810538 7.121754         7.197701\n\n\nParameter estimates based on the methods that account for right truncation and exponential growth appear to match better with the true values than the those based on the method that accounts only for right truncation.\n\n\nPlot\nLet’s plot the distribution\n\nn &lt;- 1e5\nx0 &lt;- rgamma(n, shape=shape_true, scale=scale_true)\nx1 &lt;- rgamma(n, shape=res1$par[[1]], scale=res1$par[[2]])\nx2 &lt;- rgamma(n, shape=res2$par[[1]], scale=res2$par[[2]])\nx3 &lt;- rgamma(n, shape=res3$par[[1]], scale=res3$par[[2]])\n\ndf = data.frame(model=rep(c(\"True\",\"No truncation\", \"Right truncated\", \"Right truncated, exp growth\"), each=n), val=c(x0,x1,x2,x3))\n\nlibrary(ggplot2)\nextrafont::loadfonts()\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\ndf |&gt; ggplot(aes(x=val, fill=model))+\n  geom_density(alpha=0.2) +\n  labs(x=\"value\", y=\"density\")+\n  theme(legend.position = \"bottom\", \n        legend.title = element_blank())\n\n\n\n# ggsave(\"right_trunc2.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "blog/posts/reproduction-number/index.html",
    "href": "blog/posts/reproduction-number/index.html",
    "title": "감염재생산지수 계산하기",
    "section": "",
    "text": "코로나19에 효과적으로 대응하고자 방역 당국과 연구자들이 코로나19의 전파 양상을 분석한 결과가 뉴스에 종종 보도 되었는데 그 내용 중에 빠지지 않는 것이 감염재생산지수이다. 영어로는 reproduction number (\\(\\mathcal{R}\\)) 로 불리는 데 한 명의 감염자로부터 야기된 후속 감염자의 수를 말한다. \\(\\mathcal{R}\\)이 1을 넘으면 감염자의 규모가 시간이 지남에 따라 커질 것이고 1보다 작으면 규모가 감소할 것이다. 누가 누구를 감염시켰는지 모두 알고 있다면 감염자들의 수를 세서 \\(\\mathcal{R}\\) 구할 수 있을 것이다. 하지만 한국 코로나 19 상황처럼 확진자가 많아서 모든 환자의 감염 경로를 알지 못하고 일별 확진자 자료를 가지고 있다면 어떻게 \\(\\mathcal{R}\\)을 계산할까? 이 글에서는 이에 관해 살펴보고자 한다.\n\n\n\\(\\mathcal{R}\\)의 정의\n위에서 언급한 것처럼 \\(\\mathcal{R}\\)은 한 명의 감염자에서 야기되는 후속 감염자의 수를 의미한다. 감염을 야기한 사람을 먼저 왔다는 의미로 ‘선행 감염자’ (infector) 그리고 새로이 감염된 사람들을 후에 감염되었다는 의미로 ‘후속 감염자’ (infectee) 라 칭하겠다. 그렇다면 아래와 같은 식을 쓸 수 있을 것 같다. \\[\\mathcal{R} = \\frac{새끼의 수}{어미의 수} =\\frac{후속 감염자 수}{선행 감염자 수} = \\frac{\\textrm{number of infectee}}{\\textrm{number of infector}}\\]\n\n\n\\(\\mathcal{R}\\) 계산 방법\n일별 확진자 자료를 이용하여 (\\(\\mathcal{R}\\))을 구하는 방법을 알아보기 전에 감염 경로를 모두 아는 경우를 살펴보자. 예를 들어 아래 그림과 같이 감염병이 전파되고 있다고 생각해보자. 그림에서 점들은 사람을 나타내고 화살표는 감염이 일어난 방향을 나타낸다. 그리고 0, 1, 2는 세대를 나타내는데 0세대는 외부에서 유입된 최초 감염자를 나타낸다. 측 최초 감염자가 3명을 감염시켰고 후속 감염자들도 각각 3명을 감염시켰다.\n\n2세대 이후의 상황은 모른다 가정하고 2세대까지만 계산에 넣으면 다음과 같이 계산할 수 있을 것이다. \\(\\mathcal{R}=12/4=3\\). 감염이 계속 일어나 총 \\(n\\)명의 인구 집단이 모두 감염되었다면 \\(\\mathcal{R}\\)은 얼마일까? 선행 감염자의 수는 최초의 유입된 감염자를 포함해서 \\(n+1\\) 그리고 후속 감염자의 수는 \\(n\\)이 될 것이다. 즉 \\(\\mathcal{R} = \\frac{n}{n+1}\\). 그리고 \\(n\\)이 큰 경우라면 \\(\\mathcal{R}\\)은 1로 수렴할 것이다.\n본론으로 들어가서 감염 경로는 모른채 일별 확진자수만을 가지고 \\(\\mathcal{R}\\)을 어떻게 계산할까? 아래 그림을 살펴보자. 이 그림은 중국에서 처음 발견된 확진자 수를 나타내는 유행 곡선 (epidemic curve) 이다. 붉은막대는 발열자를 나타내는데 논의의 편의를 위해서 감염자 수라 가정해보자. 녹색 네모로 표시한 2월 17일에 감염된 사람들은 녹색 화살표로 나타낸 것처럼 2월 17 일 이전에 감염된 사람들에 의하여 감염되었을 것이다. 정확히 누구에게 혹은 몇 일에 감염된 사람으로부터 감염되었는지는 알 수 없지만 말이다. 그리고 한 가지 더 알 수 있는 것은 화살표의 두께로 표현한 것처럼 선행 감염자가 언제 감염되었는지에 따라 2월 17일에 후속 감염을 일으킬 수 있는 확률이 다를 수 있다는 사실이다. 달리 표현하면 감염 후 시간이 지남에 따라 후속 감염을 일으킬 수 있는 확률이 변하게 된다는 것을 의미한다.\n\n감염 후 시간에 따라 후속 감염을 일으킬 수 있는 확률이 변할 수 있다는 것은 코로나19에 걸리게 되면 나타나는 일련의 인체 내에서의 변화 및 사람의 생활 습성등을 고려하면 어느 정도 이해할 수 있다. 바이러스에 감염되어 후속 감염자를 만들어 내기 위해서는 바이러스가 인체 내에서 증식해야 하므로 시간이 필요하다. 소위 잠재기 (latent period)가 필요하다. 이후 바이러스가 계속 증식하고 증가하고 감염 확률이 증가할 것이다. 이후 잠복기 (incubation period)를 거쳐 증상이 나타나고 회복기에 접어들면 감염 확률이 줄어들 것이다. 이런한 일련의 인체 반응에 더해 사람의 행동도 감염 확률에 영향을 미칠 것이다. 즉 몸에 바이러스가 아무리 많아도 아파서 타인을 만나지 않는다면 전파는 일어나지 않을 것이다.\n감염 후 시간에 따라 후속 감염을 일으킬 확률은 세대기 (generation interval, generation time, or transmission interval)의 분포를 이용하면 표현이 가능하다. 세대기는 한 감염자가 후속 감염을 일으킬 때 까지 걸리는 시간이다. 코로나19의 세대기는 대체로 아래와 같은 분포를 가진다고 가정해 보자. 즉 감염됨 사람이 후속 감염을 일으키려면 감염 후 하루가 지나야 하고 6일 째가 되면 후속 감염을 일으키지 않는다고 가정해보자.\n\n이걸 역으로 생각해보면 오늘 감염된 사람이 발견된 경우 이 사람을 감염시킨 선행 감염자는 2일-5일 전에 감염되었을 것이다. 이러한 세대기의 분포를 이용하면 \\(\\mathcal{R}\\) 계산식에서 문제가 되었던 부분 즉 분모에 해당하는 선행 감염자 수를 계산해 볼 수 있다. 일별 감염자가 100명씩 열흘간 발생했다고 가정해보자. 감염자 수가 일정하게 유지되고 있으니 계산할 것도 없이 \\(\\mathcal{R}\\)은 1일 것이다. 그래도 위의 논리를 이용하여 계산 하여 보자. 오늘 감염된 사람 100명이 후속 감염자가 되고 2일-5일 전에 감염된 사람이 선행 감염자가 된다. 주의할 점은 2일-5일 사이에 감염된 사람 중 위의 확률에 따라 일부만이 선행 감염자가 된다. \\[\\mathcal{R} = \\frac{후속 감염자 수}{선행 감염자 수} = \\frac{100}{100 \\times 0.25 + 100 \\times 0.35 + 100 \\times 0.25 + 100 \\times 0.15 } = 1\\]"
  },
  {
    "objectID": "blog/posts/R0_mathematica/index.html",
    "href": "blog/posts/R0_mathematica/index.html",
    "title": "Basic reproduction number: An algorithmic approach",
    "section": "",
    "text": "A recent article published in Mathematics discusses an approach to calculating \\(\\mathcal{R}_0\\). Since I have previously written a post about calculating \\(\\mathcal{R}_0\\) using Sympy, I wanted to explore a new approach proposed by the article.\nThe article claims that \\(\\mathcal{R}_0\\) is a not function of the original set of ordinary differential equations (ODEs) because \\((F, V)\\) gradient decomposition may not be unique for a given set of ODEs. As a reminder, Diekmann et al. showed that \\(\\mathcal{R}_0\\) is the spectral radius, or Perron–Frobenius eigenvalue, of the next generation operator.\n\\[\\begin{align}\n\\mathrm{d}S/\\mathrm{d}t &= -\\beta I S/N \\\\\n\\mathrm{d}I/\\mathrm{d}t &= \\beta I S/N - \\gamma I\\\\\n\\mathrm{d}R/\\mathrm{d}t &= \\gamma I\n\\end{align}\\]\nFollowing Python and Mathematica codes generate that \\[\\mathcal{R}_0 = \\frac{\\beta}{\\gamma}.\\] if \\(\\beta\\) and \\(\\gamma\\) are replaced with \\(b\\) and \\(g\\), respectively.\n\nfrom sympy import *\nb, k, g, = symbols('b k g')\nF = Matrix([[0, b],[0, 0]])\nV = Matrix([[k, 0], [-k, g]])\nM = F*(V**-1)\neigval = M.eigenvals(simplify=true)\ninit_printing(use_unicode=True)\nlst = list(eigval.keys())\nlst[1]\n\nb\n─\ng\n\n\nMathematica uses the following next generation method (NGM) function provided in the article.\n\nNGM[mod_, inf_] := \n Module[{dyn, X, infc, M, V, F, F1, V1, K}, dyn = mod[[1]]; \n  X = mod[[2]]; infc = Complement[Range[Length[X]], inf]; \n  M = Grad[dyn[[inf]], X[[inf]]]\n  (*The jacobian of the infectious equations*); \n  V1 = -M /. Thread[X[[infc]] -&gt; 0]\n  (*V1 is a first guess for V,\n  retains all gradient terms which disappear when the non infectious \\\ncomponents are null*); F1 = M + V1\n  (*F1 is a first guess for F,containing all other gradient terms*); \n  F = Replace[F1, _. _?Negative -&gt; 0, {2}];\n  (*all terms in F1 containing minuses are set to 0*); V = F - M;\n  K = (F . Inverse[V]) /. Thread[X[[inf]] -&gt; 0] // FullSimplify;\n  {M, V1, F1, F, V, K}]\n\neqnsSEIR = {\n  -b s i ,\n  b s i - k e,\n  k e - g i, \n  g i }\n\nvarsSEIR = {s, e, i, r}\nmodSEIR = {eqnsSEIR, varsSEIR}\nNGM[modSEIR, {2, 3}] /. s -&gt; 1\n\nAs in the previous post, I applied the method to the model used in Pitzer et al., of which the model may be expressed in the following set of equations.\n\\[\\begin{align}\n\\mathrm{d}S_1/\\mathrm{d}t &= B + \\epsilon S_2 - (\\lambda_p+\\lambda_w+\\mu)S_1\\\\\n\\mathrm{d}I_1/\\mathrm{d}t &= (\\lambda_p+\\lambda_w)S_1 - (\\delta+\\mu) I_1 \\\\\n\\mathrm{d}R/\\mathrm{d}t &= \\delta(1-\\theta-\\alpha)(I_1+I_2) - (\\omega +\\mu)R \\\\\n\\mathrm{d}C/\\mathrm{d}t &= \\delta\\theta(I_1+I_2) - \\mu C \\\\\n\\mathrm{d}S_2/\\mathrm{d}t &= \\omega R -\\epsilon S_2 - (\\lambda_p+\\lambda_w+\\mu) S_2\\\\\n\\mathrm{d}I_2/\\mathrm{d}t &= (\\lambda_p+\\lambda_w) S_2 - (\\delta+\\mu) I_2 \\\\\n\\mathrm{d}W/\\mathrm{d}t &= \\gamma(I_1+rI_2+rC) - \\xi W\n\\end{align}\\]\nFollowing Python and Mathematica codes generates that\n\\[\\begin{align}\n\\mathcal{R}_0 = \\frac{1}{\\mu+\\delta} \\left(\\beta_p +\\frac{\\gamma \\beta_w}{\\xi}\\right) \\left(1 +\\frac{\\delta\\theta r}{\\mu}\\right)\n\\end{align}\\]\nIn the following Python codes \\(p, r, w, d, m, t, g, x\\) represent \\(\\beta_p, r, \\beta_w, \\delta, \\mu, \\theta, \\gamma, \\xi\\) in the equation.\n\np, r, w, d, m, t, g, x = symbols('p r w d m t g x')\nF = Matrix([[p, r*p, r*p, w], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]])\nV = Matrix([[d+m, 0, 0, 0], [0, d+m, 0, 0], [-d*t, -d*t, m, 0], [-g, -r*g, -r*g, x]])\nM = F*(V**-1)\neigval = M.eigenvals(simplify=true)\ninit_printing(use_unicode=True)\neigval \nlst = list(eigval.keys())\nR0_eig = lst[1]\nR0 = (1/(d+m))*(p+g*w/x)*(1+(d*t*r)/m) # R0 from the Pitzer (2014)\nsimplify(R0-R0_eig) # 0 for the same expression (symbolic assessment)\nR0.equals(R0_eig) # True for the same expression (numerical assessment)\n\n\neqnsSIRW = {\n  B + \\[Epsilon] Subscript[s, \n    2] - (Subscript[\\[Beta], \n       P] (Subscript[i, 1] + r Subscript[i, 2] + r c) + \n      Subscript[\\[Beta], W] w + \\[Mu]) Subscript[s, 1],\n  \\[Omega] R - \\[Epsilon] Subscript[s, \n    2] - (Subscript[\\[Beta], \n       P] (Subscript[i, 1] + r Subscript[i, 2] + r c) + \n      Subscript[\\[Beta], W]  w + \\[Mu]) Subscript[s, 2],\n  (Subscript[\\[Beta], P] (Subscript[i, 1] + r Subscript[i, 2] + r c) +\n       Subscript[\\[Beta], W]  w) Subscript[s, \n    1] \\[Minus] (\\[Delta] + \\[Mu]) Subscript[i, 1],\n  (Subscript[\\[Beta], P] (Subscript[i, 1] + r Subscript[i, 2] + r c) +\n       Subscript[\\[Beta], W]  w) Subscript[s, \n    2] - (\\[Delta] + \\[Mu]) Subscript[i, 2],\n  \\[Delta] \\[Theta] (Subscript[i, 1] + Subscript[i, 2]) - \\[Mu] c,\n  \\[Gamma] (Subscript[i, 1] + r Subscript[i, 2] + r c) - \\[Xi] w,\n  \\[Delta] (1 - \\[Theta]) (Subscript[i, 1] + Subscript[i, \n      2]) - (\\[Omega] + \\[Mu]) R}\n\nvarsSIRW = {Subscript[s, 1], Subscript[s, 2], Subscript[i, 1], \n  Subscript[i, 2], c, w, R }\n  \nmodSIRW = {eqnsSIRW, varsSIRW}\nNGM[modSIRW, Range[3, 6]]\n\nAgain, manually defining \\(F,V\\) as follows leads to the same answer\n\nF = {{Subscript[\\[Beta], P], r Subscript[\\[Beta], P], \n   r Subscript[\\[Beta], P], Subscript[\\[Beta], W] }, {0, 0, 0, 0}, {0,\n    0, 0, 0}, {0, 0, 0, 0}}\nV = {{\\[Delta] + \\[Mu], 0, 0, 0}, {0, \\[Delta] + \\[Mu], 0, \n   0}, {-\\[Delta] \\[Theta], -\\[Delta] \\[Theta], \\[Mu], \n   0}, {-\\[Gamma], -r \\[Gamma], -r \\[Gamma], \\[Xi]}}\nEigenvalues[Dot[F, Inverse[V]]] // FullSimplify\n\n\\[\\mathcal{R}_0 = \\frac{(\\mu +\\delta  \\theta  r) \\left(\\xi  \\beta_P+\\gamma  \\beta_W \\right)}{\\mu  \\xi  (\\delta +\\mu )}.\\]"
  },
  {
    "objectID": "blog/posts/pubmed-chatgpt_summary/index.html",
    "href": "blog/posts/pubmed-chatgpt_summary/index.html",
    "title": "PubMed search, ChatGPT summary, and sending an email in R",
    "section": "",
    "text": "Search the PubMed database Use the entrez_search function from the rentrez package to search the PubMed database\n\n\nlibrary(rentrez)\n\ndate_start &lt;- gsub(\"-\", \"/\", Sys.Date()-2)\ndate_end &lt;- gsub(\"-\", \"/\", Sys.Date()-1)\nsearch_query &lt;- paste0(\"(typhoid OR cholera) AND \", date_start,\":\", date_end, \"[dp]\") # the search query\nsearch_results &lt;- entrez_search(db=\"pubmed\", term=search_query) # any other useful parameters?\n\n\nFetch the details of the article in xml format\n\n\n# Retrieve the details of the data in xml format based on pubmed ids\narticle_details &lt;- entrez_fetch(db=\"pubmed\", id=search_results$ids, rettype=\"xml\")\n\n\nParse the XML using the xml2 package\n\n\nlibrary(xml2)\n# Parse the XML data\ndoc &lt;- read_xml(article_details)\n# Extract the titles and abstracts\ntitles &lt;- xml_text(xml_find_all(doc, \"//ArticleTitle\"))\n# abstracts &lt;- xml_text(xml_find_all(doc, \"//AbstractText\"))\nabstracts &lt;- xml_text(xml_find_all(doc, \"//Abstract\"))\ndois &lt;- xml_text(xml_find_all(doc, \".//PubmedData/ArticleIdList/ArticleId[@IdType='doi']\")) # to get the doi's\n\n\nCall ChatGPT to summarize the abstract in 1-2 sentences This is a subscription-based service. You must have a ChatGPT API key and must have signed up for their paid service.\n\n\n# gpt-4o\nprompt_chatgpt &lt;- function(prompt, api_key=NULL, model=\"gpt-4o\", temperature=0.8){\n  model &lt;- grep(model, c(\"gpt-3.5-turbo\", \"gpt-4o\"), value=TRUE)\n  response &lt;- httr::POST(\n    url = \"https://api.openai.com/v1/chat/completions\", \n    httr::add_headers(Authorization = paste(\"Bearer\", api_key)),\n    httr::content_type_json(),\n    encode = \"json\",\n    body = list(\n      model = model,\n      temperature = temperature, # this is the degree of randomness of the model's output\n      messages = list(list(\n        role = \"user\", \n        content = prompt\n     ))\n   )\n  )\n  return(httr::content(response)$choices[[1]]$message$content)\n}\n\n\nMake R send you an email everyday Use simple HTML syntaxes (&lt;p&gt;&lt;/p&gt; or &lt;b&gt;&lt;/b&gt;) to compose an email message using blastula package\n\n\nlibrary(blastula)\n\ncreate_summary &lt;- function(titles, abstract_summary, ids, dois) {\n  summary &lt;- sapply(1:length(abstract_summary), function(i) paste0(\"&lt;p&gt;\", \" &lt;b&gt;\", \" &lt;a href=https://pubmed.ncbi.nlm.nih.gov/\", ids[i], \"/&gt; \", titles[i], \"&lt;/a&gt;\", \" &lt;/b&gt; \", abstract_summary[i], \" PMID=\", ids[i], \" DOI=\", dois[i], \"&lt;/p&gt;\"))\n  return(summary)\n}\n\nemail &lt;- compose_email(\n  title = \"Test Email\",\n  body = md(create_summary(titles, abstract_summary, ids, dois)))\n\nemail %&gt;%\n  smtp_send( \n    from = \"kimfinale@gmail.com\",\n    to = \"jonghoon.kim@ivi.int\",\n    subject = \"Daily summary of PubMed search\",\n    # credentials = creds_key(id = \"gmail\"),\n    credentials = creds_file(\"gmail_cred\")\n  )\n\n\nSave all as a single R script\n\n\ndate_start &lt;- gsub(\"-\", \"/\", Sys.Date()-2)\ndate_end &lt;- gsub(\"-\", \"/\", Sys.Date()-1)\nsearch_query &lt;- paste0(\"(typhoid OR cholera OR transmission OR modeling) AND \", date_start,\":\", date_end, \"[dp]\") \n \nchatgpt_api_key &lt;- Sys.getenv(\"CHATGPT_API_KEY\")\n\nsearch_res &lt;- rentrez::entrez_search(db=\"pubmed\", term=search_query)\nmodel &lt;- \"gpt-3.5\"  \nif (length(search_res$ids) &gt; 0) { # one or more hits\n  ids &lt;- search_res$ids\n  details &lt;- rentrez::entrez_fetch(db=\"pubmed\", id=ids, rettype=\"xml\")\n  doc &lt;- xml2::read_xml(details)\n  titles &lt;- xml2::xml_text(xml2::xml_find_all(doc, \"//ArticleTitle\"))\n  abstracts &lt;- xml2::xml_text(xml2::xml_find_all(doc, \"//Abstract\"))\n  dois &lt;- xml2::xml_text(xml2::xml_find_all(doc, \"//PubmedData/ArticleIdList/ArticleId[@IdType='doi']\"))\n  \n  abstract_summary &lt;- rep(NA, length(abstracts))\n\n  if (length(abstracts) &gt; 1) {\n    for (i in 1:length(abstracts)) {\n      prompt &lt;- paste0(\"Your task is to generate a short summary of a scientific article based on its title and abstract. Summarize the text delimited by triple backticks into a single sentence. Please do not repeat the title. ``` Title: \", titles[i], \". Abstract: \", abstracts[i], \"```\")\n      abstract_summary[i] &lt;- \n        prompt_chatgpt(prompt=prompt, api_key=chatgpt_api_key, model=model)\n    }\n  }\n  # create a summary for the email  \n  summary &lt;- sapply(1:length(abstract_summary), \n                    function(i) paste0(\"&lt;p&gt;\", \" &lt;b&gt;\", \" &lt;a href=https://pubmed.ncbi.nlm.nih.gov/\", ids[i], \"/&gt; \", titles[i], \"&lt;/a&gt;\", \" &lt;/b&gt; \", abstract_summary[i], \" PMID=\", ids[i], \". DOI=\", dois[i], \". &lt;/p&gt;\"))\n\n  intro &lt;- paste0(\"&lt;p&gt;Please enjoy the articles retrieved from PubMed based on your search query, \", search_query, \", published between \", date_start, \", and \", date_end, \". Each article is accompanied by a one-sentence summary provided by the ChatGPT, \", model, \". For feedback, please contact Jong-Hoon Kim at jonghoon.kim@ivi.int.&lt;/p&gt;\")\n  \n  summary &lt;- c(intro, summary)\n  email &lt;- blastula::compose_email(\n    title = \"Weekly summary of PubMed search\",\n    body =  blastula::md(summary))\n  \n  blastula::smtp_send(email, \n    from = \"kimfinale@gmail.com\",\n    to = \"jonghoon.kim@ivi.int\",\n    subject = \"Daily summary of PubMed search\",\n    credentials = blastula::creds_file(\"gmail_creds\")\n  )\n\n} else {\n  \"No articles matched your query.\" \n}\n\n\nRegister the file using the Windows task scheduler\n\n\nlibrary(taskscheduleR)\n# Schedule the script to run daily at a specific time\ntaskscheduler_create(\n  taskname = \"PubMed_ChatGPT_Summary\",\n  rscript = \"~/myblog/pubmed_chatgpt.R\",\n  schedule = \"WEEKLY\", starttime = \"22:00\", startdate = \"02/06/2024\")"
  },
  {
    "objectID": "blog/posts/profile-likelihood/index.html",
    "href": "blog/posts/profile-likelihood/index.html",
    "title": "Confidence interval using profile likelihood",
    "section": "",
    "text": "감염병 수리 모형 모수(parameter)의 신뢰구간 (confidence interval)구하기 - profile likelihood\n수리 모형을 이용하여 연구를 하게되면 관찰값을 이용하여 모형의 모수를 보정하는 과정을 거치게 된다. 이 과정을 소위 결과 (관찰값)로 부터 원인 (모형)을 알아내는 과정이라 하여 inverse problem 이라 부르기도 한다. 이 글에서는 \\(SEIR\\) 모형과 중국 우한 에서의 초기 코로나-19 발열자 자료를 이용하여 모형의 모수 (기초재감염지수)와 신뢰구간을 구해본다. 모수는 푸아송 (Poisson) 분포를 이용한 최대 우도 (maximum likelihood) 방법으로 그리고 신뢰구간은 profile likelihood 방법을 사용한다.\n아래에 SEIR 모형의 R 코드는 이전에 사용했던 모형에 변수 \\(C\\)를 추가하였는데 이는 누적 발열자수를 나타내고 일별 발열자 수를 쉽게 구하기 위함이다.\n\n# ODE-based SEIR model\nseir &lt;- function(t, y, params) {\n  S &lt;- y[1]; E &lt;- y[2]; I &lt;- y[3]; R &lt;- y[4]; C &lt;- y[5]\n  beta &lt;- params[\"beta\"]\n  sigma &lt;- params[\"sigma\"]\n  gamma &lt;- params[\"gamma\"]\n  \n  muSE &lt;- beta * I / (S + E + I + R)\n  muEI &lt;- sigma\n  muIR &lt;- gamma\n  \n  dS &lt;- - muSE*S\n  dE &lt;-  muSE*S - muEI*E\n  dI &lt;-  muEI*E - muIR*I\n  dR &lt;-  muIR*I\n  dC &lt;-  muEI*E ## cumulative symtom onset\n  \n  return(list(c(dS, dE, dI, dR, dC)))\n}\n\n\n# daily symptomatic case\ndaily_case &lt;- function(params=NULL) { \n  y0 &lt;- c(S = 11e6 - 1, E = 0, I = 1, R = 0, C = 1) # initial values (Wuhan population size)\n  times &lt;- seq(from = 0, to = 35, by = 1)\n  if(is.null(params)){\n    params &lt;- c(beta = 2.5/4.5, sigma = 1/5.2, gamma = 1/4.5)\n  }\n  out &lt;- ode(y = y0, times = times, func = seir, parms = params)\n  x &lt;- as.data.frame(out) \n  n &lt;- nrow(x)\n  daily = c(0, diff(x[,\"C\"]))\n  return (daily)\n} \n\n우한에서 발생한 초기 일별 코로나19 환자수는 Kucharski et al. (2020) Lancet 에 보고된 자료를 기반으로 하였다.\n\nwuhan &lt;- \n  data.frame(date = seq(as.Date(\"2019-12-13\"), \n                        as.Date(\"2020-01-16\"), by = \"day\"),\n             case = c(0,0,0,0,0,0,0,2,2,3,0,1,1,0,0,1,0,1,2,\n                      3,4,3,3,1,2,5,6,8,3,8,8,5,17,7,13))\n\n일별 발열자수 \\(y_t\\)가 푸아송 분포를 따른다고 가정하고 우도 함수를 아래와 같이 정의 한다.\n\\[ y_t \\sim \\mathrm{Poisson}(Y_t)\\] \\[\\mathcal{L}(\\theta) = \\prod_{t=1}^{n} f(y_t \\vert \\theta) = \\prod_{t=1}^{n} \\frac{Y_t^{y_t} e^{-Y_t}}{y_t!}\\]\n우도 계산식을 아래와 같이 R로 구현할 수 있다. 물론 우도함수는 수치 안정성을 위해서 log 를 취한 값을 사용하고 (즉 log likelihood) 최적화 알고리듬은 최소값을 찾기 때문에 음의값으로 치환한 negative log likelihood를 사용한다.\n\nnegloglik &lt;- function(par) {\n  params &lt;- c(beta = par, sigma = 1/5.2, gamma = 1/4.5)\n  model &lt;- daily_case(params = params)\n  - sum(dpois(x = wuhan$case, lambda = model, log = TRUE)) # sum of negative log likelihood\n}\n\nSEIR 모형에는 세 개의 모수 (\\(\\beta, \\sigma, \\gamma\\))가 있는데 \\(\\sigma, \\gamma\\)는 각각 잠복기와 회복까지 걸리는 시간을 나타내고 환자들을 관찰하여 그 값을 추정할 수 있는 경우가 많다. 이에 반해 \\(\\beta\\)는 수리 모형의 예측값을 관찰된 유행 곡선과 비교하여 추정한다. 이 과정이 negloglik 함수에 구현된 것이고 optim 함수를 사용하여 negloglik를 최소화하는 \\(\\beta\\)를 구한다.\n\\[ \\hat{\\theta} = \\underset{\\theta}{\\mathrm{argmax}}~\\{{\\mathrm{log} \\mathcal{L}(\\theta)} \\}\\]\n\nlibrary(deSolve) # negloglik includes ODE model to be integrated using deSolve\nfit &lt;- optim(negloglik, par=c(0.1), method=\"Brent\", lower=0, upper=10)\n(theta &lt;- fit$par)\n\n[1] 0.5735032\n\ngamma &lt;- 1/4.5; (R0 &lt;- theta/gamma)\n\n[1] 2.580765\n\n\n95% 신뢰 구간은 Log likelihood 가 asymptotically 아래의 조건을 만족한다는 사실을 이용하여 계산할 수 있다 (Wilks’ theorem). \\[ 2 (\\mathrm {log} \\mathcal {L} (\\hat{\\theta}) - \\mathrm{log}\\mathcal{L}(\\theta_0)) \\sim \\chi^2_1\\]\n\nprof_b &lt;- expand.grid(b = seq(0.5, 0.7, length = 1000))\nprof_b$loglik &lt;- -sapply(prof_b$b, negloglik)\nmaxloglik &lt;- - fit$value\ncutoff &lt;- maxloglik - qchisq(p=0.95,df=1)/2\n(limits &lt;- range(subset(prof_b, loglik &gt; cutoff)$b)) # 95% confidence interval\n\n[1] 0.5518519 0.5940941\n\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nextrafont::loadfonts()\nlibrary(dplyr)\nprof_b |&gt; as.data.frame() |&gt; \n  ggplot(aes(b,loglik))+\n  geom_line()+\n  geom_vline(xintercept=fit$par, color=\"steelblue\", linewidth=1)+\n  geom_vline(xintercept=limits, color=\"steelblue\", linetype=\"dotted\", linewidth=1)+\n  geom_hline(yintercept=maxloglik, color=\"steelblue\", linewidth=1)+\n  geom_hline(yintercept=cutoff, color=\"steelblue\", linewidth=1)+\n  scale_y_continuous(limits= c(maxloglik-6, maxloglik+1))+\n  labs(x=expression(beta), y=\"Log likelihood\")\n\n\n\n# ggsave(\"profile_lik.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)\n\nbbmle 패키지는 confint 함수로 profile likelihood를 이용하여 신뢰구간을 구하는 방법을 제공해준다.\n\nlibrary(bbmle)\nbbfit &lt;- mle2(minuslogl=negloglik, start=list(par=0.1), method=\"L-BFGS-B\", lower=c(par=1e-6), control=list(maxit=5000))\nconfint(bbfit)\n\n    2.5 %    97.5 % \n0.5517962 0.5942085"
  },
  {
    "objectID": "blog/posts/prevalence_incidence_sirmodel/index.html",
    "href": "blog/posts/prevalence_incidence_sirmodel/index.html",
    "title": "Prevalence vs. incidence for the SIR model",
    "section": "",
    "text": "SIR model with constant incidence rate\nIn the following, I implemented a simple model of disease incidence and recovery. Susceptibles are infected at a constant rate, go through the natural history of infection, and finally recover at a constant rate. Infection and recovery follow the exponential decay. This is to explore the relationship between the incidence and prevalence. If the prevalence of a disease is less than 10%, the relationship is often described as follows (e.g., see the link): \\[\n\\text{Prevalence} = \\text{IR} \\times \\text{Average Duration of a Disease}\n\\] I have computed the prevalence in three ways. In particular, I assumed a short-lived infection (e.g., 2 weeks) of infectious diseases but with a long-lasting immunity. Therefore, prevalence may mean the proportion of the population immune to infection in this context. First, I use the above method. Second, I developed a simple set of differential equations to model disease incidence and recovery. Finally, I derived analytical solutions to the differential equations.\n\n# Prevalence according to the above equation\nir &lt;- 100/100000 # incidence rate per 100,000 person-years\ndur_R &lt;- 20 # duration of infection-derived immunity (20 years).\nprev &lt;- ir * dur_R \n\n# Prevalence according to the differential equation\n# Note that the incidence rate (or force of infection) remains constant in this model unlike the classic SIR model.\nsir_beta_const &lt;- function(t, u, p){\n  N &lt;- sum(u)\n  du1 &lt;- - p[1]*u[1] + p[3]*u[3]\n  du2 &lt;- + p[1]*u[1] - p[2]*u[2]\n  du3 &lt;- + p[2]*u[2] - p[3]*u[3]\n    \n  return(list(c(du1,du2,du3))) \n}\nlibrary(deSolve)\nu0 &lt;- c(0.99, 0.01, 0)\ntspan &lt;- seq(from=0, to=300)\ndur_I &lt;- 14/365\nr &lt;- -log(1-ir) # instantaneous rate for the yearly incidence rate, ir\np &lt;- c(r, 1/dur_I, 1/dur_R)\noutdf &lt;- as.data.frame(ode(y=u0, times=tspan, func=sir_beta_const, parms=p))\n\n## to explore visually as well\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(outdf,aes(x=time))+\n  geom_line(aes(y=`1`, color=\"S\")) +\n  geom_line(aes(y=`2`, color=\"I\")) +\n  geom_line(aes(y=`3`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n# Prevalence according to analytic solutions for the ODE model\n# Refer to the equations below\ngamma &lt;- 1/dur_I\nmu &lt;- 0 # natural death rate, which was not implemented in the ODE model\nomega &lt;- 1/dur_R\n# according the following equations\nr_eq &lt;- (r*gamma)/((gamma + mu)*(mu + omega) + r*(gamma + mu + omega))\n\n# Compare three values\nr_eq\n\n[1] 0.01961672\n\ntail(outdf$`3`, 1)\n\n[1] 0.01961672\n\nprev\n\n[1] 0.02\n\n\nThe following Mathematica commands produce the steady state for S, I (written as Y below), R\n\n# FullSimplify[Solve[{\\[Mu]*(S+Y+R)-(r +\\[Mu]) *S + \\[Omega]* R==0, r *S-(\\[Mu]+\\[Gamma]) *Y == 0, \\[Gamma]* Y-(\\[Omega]+\\[Mu])*R==0,S+Y+R==1, \\[Mu]&gt;0,r&gt;0,\\[Gamma]&gt;0,\\[Omega]&gt;0, S&gt;0,Y&gt;0,R&gt;0,S\\[Element]Reals,Y\\[Element]Reals,R\\[Element]Reals}, {S,Y,R}]]\n\n\\[S = \\frac{(\\gamma +\\mu ) (\\mu +\\omega )}{(\\gamma +\\mu ) (\\mu +\\omega )+r (\\gamma +\\mu +\\omega )}\\]\n\\[I =\\frac{r (\\mu +\\omega )}{(\\gamma +\\mu ) (\\mu +\\omega )+r (\\gamma +\\mu +\\omega )}\\] \\[R = \\frac{\\gamma  r}{(\\gamma +\\mu ) (\\mu +\\omega )+r (\\gamma +\\mu +\\omega )}\\]"
  },
  {
    "objectID": "blog/posts/POLYMOD-multiple-regression/index.html",
    "href": "blog/posts/POLYMOD-multiple-regression/index.html",
    "title": "Mulitple regression: POLYMOD data",
    "section": "",
    "text": "This post describes my attempt to reproduce Table 1 of the paper, Social Contacts and Mixing Patterns Relevant to the Spread of Infectious Diseases. Data were downloaded from Social Contact Data, which was hosted in zenodo. I used the version 1.1. While I wasn’t successful at reproducing the table exatly but still wanted to document the processes that I went through. Potential reasons include: a) results of numerical computations vary depending on the system and the software and b) datasets that were analyzed might be slightly different because of the missing values.\n\nData preparation\n\nlibrary(data.table)\nd1 &lt;- fread(\"2008_Mossong_POLYMOD_participant_common.csv\")\nd2 &lt;- fread(\"2008_Mossong_POLYMOD_contact_common.csv\")\nlibrary(dplyr)\n# count the number of contacts for each participant using part_id variable\nd2 |&gt; group_by(part_id) |&gt;\n  summarize(contacts = n()) -&gt; d2_contacts\nd12 &lt;- left_join(d1, d2_contacts, by=\"part_id\")\n# add household information\nd3 &lt;- fread(\"2008_Mossong_POLYMOD_hh_common.csv\")\nd123 &lt;- left_join(d12, d3, by=\"hh_id\")\n# add day of week information\nd4 &lt;- fread(\"2008_Mossong_POLYMOD_sday.csv\")\ndat &lt;- left_join(d123, d4, by=\"part_id\")\n\n\n\nData manipulation\nCategorize the age group into different 10 age groups: 0-4, 5-9, 10-14, 15-19, and 20 to 70 by 10 years and 70 and above\n\nclassify_age &lt;- function(d){\n  d$age_grp &lt;- 99\n  for (i in 1:nrow(d)) {\n    if(!is.na(d$part_age[i])){\n      if(d$part_age[i] &lt; 5){\n        d$age_grp[i] &lt;- 0\n      }\n      else if (d$part_age[i] &gt;= 5 && d$part_age[i] &lt; 20){\n        for(j in 1:3){\n          if(d$part_age[i] &gt;= 5*j && d$part_age[i] &lt; (5*j+5)){\n            d$age_grp[i] &lt;- j\n          }\n        }\n      }\n      else if (d$part_age[i] &gt;= 20 && d$part_age[i] &lt; 70){\n        for (k in 1:5){\n          if (d$part_age[i] &gt;= (10+10*k) && d$part_age[i] &lt; (20+10*k)){\n            d$age_grp[i] &lt;- k+3\n          }\n        }\n      } \n      else {\n        d$age_grp[i] &lt;- 9\n      }\n    } \n  }\n  d\n}\n\ndat &lt;- classify_age(dat)\n\nVisualize the distribution of the number of contacts\n\nlibrary(ggplot2)\ndat |&gt; ggplot(aes(x=contacts)) + \n  geom_histogram(binwidth=5)\n\n\n\n\nCompare the number of participants by age group (the third column)\n\ndat |&gt; \n  group_by(age_grp) |&gt; \n  summarize(npart=n(),\n            avg_contacts = round(sum(contacts, na.rm=T) / npart, digits=2)) -&gt; dat_ag\n\ndat_ag$npart_true &lt;- c(660,661,713,685,879,815,908,906,728,270,65)\ndat_ag$avg_contacts_true &lt;- c(10.21,14.81, 18.22,17.58,13.57,14.14,13.83,12.30,9.21,6.89,9.63)\ndat_ag\n\n# A tibble: 11 × 5\n   age_grp npart avg_contacts npart_true avg_contacts_true\n     &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n 1       0   660        10.2         660             10.2 \n 2       1   661        14.8         661             14.8 \n 3       2   713        18.2         713             18.2 \n 4       3   685        17.6         685             17.6 \n 5       4   879        13.6         879             13.6 \n 6       5   815        14.1         815             14.1 \n 7       6   908        13.8         908             13.8 \n 8       7   906        12.3         906             12.3 \n 9       8   728         9.21        728              9.21\n10       9   270         6.89        270              6.89\n11      99    65         9.63         65              9.63\n\n\nCategorize the household size\n\nclassify_hh &lt;- function(d){\n  d$hh_size_grp &lt;- ifelse(d$hh_size &gt; 4, 5, ifelse(!is.na(d$hh_size), d$hh_size, 99))\n  d\n}\ndat &lt;- classify_hh(dat)\n# data.table::fwrite(dat, \"data/POLYMOD_2017.csv\")\n\nMake categorical variables factor for regression\n\n# dat &lt;- data.table::fread(\"data/POLYMOD_2017.csv\")\n# set the categorical variables as factor for regression\ndat$age_grp &lt;- as.factor(dat$age_grp)\ndat$hh_size_grp &lt;- as.factor(dat$hh_size_grp)\ndat$gender &lt;- ifelse(dat$part_gender==\"M\", 0, ifelse(dat$part_gender==\"F\", 1, 99))\ndat$gender &lt;- as.factor(dat$gender)\ndat$dayofweek &lt;- as.factor(dat$dayofweek)\n# dat &lt;- dat[complete.cases(dat),]\n# data.table::fwrite(dat, \"data/POLYMOD_2017_gender_modified.csv\")\n\n\n\nLinear regression\nWhile I will eventually use the negative binomial regresssion, I tried linear regression with which my colleague who uses Stata is familiar.\n\nlibrary(MASS)\n# to remove missing values\ndat &lt;- dat[complete.cases(dat),]\n\nm &lt;- lm(contacts ~ age_grp + gender + hh_size_grp + country + dayofweek, data = dat)\nsummary(m)\n\n\nCall:\nlm(formula = contacts ~ age_grp + gender + hh_size_grp + country + \n    dayofweek, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.204  -6.129  -1.718   3.880  80.052 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2.5978     0.7067   3.676 0.000239 ***\nage_grp1       4.1734     0.5168   8.076 7.80e-16 ***\nage_grp2       7.7402     0.5072  15.262  &lt; 2e-16 ***\nage_grp3       7.4715     0.5117  14.602  &lt; 2e-16 ***\nage_grp4       4.6392     0.4934   9.402  &lt; 2e-16 ***\nage_grp5       4.5177     0.4981   9.070  &lt; 2e-16 ***\nage_grp6       4.1732     0.4863   8.581  &lt; 2e-16 ***\nage_grp7       3.2931     0.5024   6.554 5.99e-11 ***\nage_grp8       1.2232     0.5501   2.224 0.026206 *  \nage_grp9      -1.1332     0.7209  -1.572 0.115997    \ngender1        0.1746     0.2216   0.788 0.430686    \ngender99       2.0543     2.9437   0.698 0.485271    \nhh_size_grp2   0.8352     0.4244   1.968 0.049133 *  \nhh_size_grp3   1.0815     0.4498   2.404 0.016231 *  \nhh_size_grp4   2.7105     0.4549   5.959 2.67e-09 ***\nhh_size_grp5   3.9035     0.4951   7.884 3.64e-15 ***\ncountryDE     -3.5686     0.4315  -8.270  &lt; 2e-16 ***\ncountryFI     -0.5878     0.4519  -1.301 0.193398    \ncountryGB     -0.3451     0.4530  -0.762 0.446204    \ncountryIT      7.5967     0.4691  16.194  &lt; 2e-16 ***\ncountryLU      5.0942     0.4500  11.321  &lt; 2e-16 ***\ncountryNL      2.9129     0.6754   4.313 1.63e-05 ***\ncountryPL      4.0530     0.4489   9.029  &lt; 2e-16 ***\ndayofweek1     3.7578     0.4317   8.705  &lt; 2e-16 ***\ndayofweek2     4.5691     0.4244  10.767  &lt; 2e-16 ***\ndayofweek3     4.4291     0.4336  10.214  &lt; 2e-16 ***\ndayofweek4     4.5608     0.4277  10.664  &lt; 2e-16 ***\ndayofweek5     4.7862     0.4234  11.303  &lt; 2e-16 ***\ndayofweek6     1.6347     0.4414   3.703 0.000214 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.271 on 7040 degrees of freedom\nMultiple R-squared:  0.2383,    Adjusted R-squared:  0.2352 \nF-statistic: 78.64 on 28 and 7040 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nNegBin regression\nNegative binomial regression was modeled using MASS::glm.nb function\n\nlibrary(MASS)\nm1 &lt;- glm.nb(contacts ~ age_grp, data = dat)\nsummary(m1)\n\n\nCall:\nglm.nb(formula = contacts ~ age_grp, data = dat, init.theta = 2.349538851, \n    link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.33297    0.02847  81.959  &lt; 2e-16 ***\nage_grp1     0.34973    0.03963   8.826  &lt; 2e-16 ***\nage_grp2     0.57379    0.03861  14.862  &lt; 2e-16 ***\nage_grp3     0.53545    0.03908  13.702  &lt; 2e-16 ***\nage_grp4     0.28592    0.03733   7.660 1.86e-14 ***\nage_grp5     0.31980    0.03780   8.460  &lt; 2e-16 ***\nage_grp6     0.29453    0.03707   7.946 1.93e-15 ***\nage_grp7     0.18429    0.03716   4.959 7.09e-07 ***\nage_grp8    -0.09048    0.03958  -2.286   0.0223 *  \nage_grp9    -0.38437    0.05435  -7.072 1.53e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.3495) family taken to be 1)\n\n    Null deviance: 8082.4  on 7068  degrees of freedom\nResidual deviance: 7381.8  on 7059  degrees of freedom\nAIC: 49343\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.3495 \n          Std. Err.:  0.0452 \n\n 2 x log-likelihood:  -49321.1310 \n\nexp(m1$coefficients)\n\n(Intercept)    age_grp1    age_grp2    age_grp3    age_grp4    age_grp5 \n 10.3085271   1.4186911   1.7749820   1.7082181   1.3309824   1.3768472 \n   age_grp6    age_grp7    age_grp8    age_grp9 \n  1.3424987   1.2023627   0.9134947   0.6808798 \n\nm5 &lt;- glm.nb(contacts ~ age_grp + gender + hh_size_grp + country + dayofweek, data = dat)\nsummary(m5)\n\n\nCall:\nglm.nb(formula = contacts ~ age_grp + gender + hh_size_grp + \n    country + dayofweek, data = dat, init.theta = 3.032960306, \n    link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.733148   0.050127  34.575  &lt; 2e-16 ***\nage_grp1      0.320217   0.036004   8.894  &lt; 2e-16 ***\nage_grp2      0.513597   0.035093  14.635  &lt; 2e-16 ***\nage_grp3      0.512688   0.035437  14.468  &lt; 2e-16 ***\nage_grp4      0.359741   0.034520  10.421  &lt; 2e-16 ***\nage_grp5      0.343102   0.034802   9.859  &lt; 2e-16 ***\nage_grp6      0.313841   0.034051   9.217  &lt; 2e-16 ***\nage_grp7      0.253024   0.035267   7.175 7.25e-13 ***\nage_grp8      0.048104   0.039074   1.231 0.218283    \nage_grp9     -0.211659   0.052694  -4.017 5.90e-05 ***\ngender1       0.007832   0.015357   0.510 0.610050    \ngender99      0.077826   0.207001   0.376 0.706941    \nhh_size_grp2  0.098342   0.030472   3.227 0.001250 ** \nhh_size_grp3  0.117260   0.031984   3.666 0.000246 ***\nhh_size_grp4  0.235206   0.032164   7.313 2.62e-13 ***\nhh_size_grp5  0.323285   0.034725   9.310  &lt; 2e-16 ***\ncountryDE    -0.345037   0.030616 -11.270  &lt; 2e-16 ***\ncountryFI    -0.054068   0.031613  -1.710 0.087209 .  \ncountryGB    -0.023021   0.031609  -0.728 0.466429    \ncountryIT     0.488765   0.032064  15.244  &lt; 2e-16 ***\ncountryLU     0.340195   0.030948  10.993  &lt; 2e-16 ***\ncountryNL     0.219279   0.046632   4.702 2.57e-06 ***\ncountryPL     0.281896   0.030944   9.110  &lt; 2e-16 ***\ndayofweek1    0.278478   0.030363   9.172  &lt; 2e-16 ***\ndayofweek2    0.336024   0.029800  11.276  &lt; 2e-16 ***\ndayofweek3    0.328357   0.030383  10.807  &lt; 2e-16 ***\ndayofweek4    0.326135   0.029980  10.879  &lt; 2e-16 ***\ndayofweek5    0.365918   0.029655  12.339  &lt; 2e-16 ***\ndayofweek6    0.143777   0.031178   4.611 4.00e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(3.033) family taken to be 1)\n\n    Null deviance: 9903.4  on 7068  degrees of freedom\nResidual deviance: 7310.7  on 7040  degrees of freedom\nAIC: 47832\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  3.0330 \n          Std. Err.:  0.0626 \n\n 2 x log-likelihood:  -47771.7930 \n\nexp(m5$coefficients)\n\n (Intercept)     age_grp1     age_grp2     age_grp3     age_grp4     age_grp5 \n   5.6584394    1.3774273    1.6712925    1.6697736    1.4329588    1.4093123 \n    age_grp6     age_grp7     age_grp8     age_grp9      gender1     gender99 \n   1.3686718    1.2879141    1.0492799    0.8092407    1.0078631    1.0809342 \nhh_size_grp2 hh_size_grp3 hh_size_grp4 hh_size_grp5    countryDE    countryFI \n   1.1033402    1.1244116    1.2651689    1.3816597    0.7081939    0.9473682 \n   countryGB    countryIT    countryLU    countryNL    countryPL   dayofweek1 \n   0.9772421    1.6303023    1.4052212    1.2451789    1.3256406    1.3211173 \n  dayofweek2   dayofweek3   dayofweek4   dayofweek5   dayofweek6 \n   1.3993730    1.3886852    1.3856028    1.4418376    1.1546267"
  },
  {
    "objectID": "blog/posts/particle-filter/index.html",
    "href": "blog/posts/particle-filter/index.html",
    "title": "Particle filter using R",
    "section": "",
    "text": "A simple particle filter in R\nThe following example was adapted from the post in RPubs.\n\nSimulate the data\nGenerate \\(y_{1:T}\\) as a sequence of noisy observations of a latent variable \\(x_{1:T}\\).\n\n# create a data set: x (latent variable) and y (observation)\nset.seed(42) # to make it reproducible  (lots of random numbers follow)\nT &lt;- 50 # number of observations\nx &lt;- rep(NA, T) # latent variable\ny &lt;- rep(NA, T) # observed values\nsx &lt;- 2.2 # standard deviation for x\nsy &lt;- 0.3 # standard deviation for y\nx[1] &lt;- rnorm(1, 0, 1)\ny[1] &lt;- rnorm(1, x[1], sy)\n\nfor (t in seq(2, T)) {\n  x[t] &lt;- rnorm(1, x[t-1], sx)\n  y[t] &lt;- rnorm(1, x[t], sy)\n}\nx_true &lt;- x\nobs &lt;- y\n\n\n\nImplement a particle filter (sequential Monte Carlo)\n\n# particle filter -----------------------------------------------------------\nT &lt;- length(y) # number of observations\nN &lt;- 100 # number of particles\n# to store prior distributions for variables correspond to latent variable x\nx_prior &lt;- matrix(nrow=N, ncol = T) \nx_post &lt;- matrix(nrow=N, ncol = T)  # posterior distributions\nweights &lt;- matrix(nrow=N, ncol = T) # weights used to draw posterior sample\nW &lt;- matrix(nrow =  N, ncol = T) # normalized weights\nA &lt;- matrix(nrow =  N, ncol = T) # indices based on the normalized weights\nx_prior[, 1] &lt;- rnorm(N, 0, sx)# initial X from a normal distribution\n# calculate weights, normal likelihood\nweights[, 1] &lt;- dnorm(obs[1], x_prior[, 1], sy)\nW[, 1] &lt;- weights[, 1]/sum(weights[, 1])# normalise weights\n# indices based on the weighted resampling with replacement \nA[, 1] &lt;- sample(1:N, prob = W[1:N, 1], replace = T) \nx_post[, 1] &lt;- x_prior[A[, 1], 1] # posterior distribution using the indices\n\nfor (t in seq(2, T)) {\n  x_prior[, t] &lt;- rnorm(N, x_post[, t-1], sx) # prior x_{t} based on x_{t-1}\n  weights[, t] &lt;- dnorm(obs[t], x_prior[, t], sy) # calculate weights \n  W[, t] &lt;- weights[, t]/sum(weights[, t]) # normalise weights\n  A[, t] &lt;- sample(1:N, prob = W[1:N, t], replace = T) # indices\n  x_post[, t] &lt;- x_prior[A[, t], t] # posterior samples\n}\n\n\n\n\nSummarize results\nCalculate the mean and 2.5\\(^\\textrm{th}\\) and 97.\\(^\\textrm{th}\\) percentile of the posterior sample as a means to get 95% credible interval.\n\nx_means &lt;- apply(x_post, 2, mean) # posterior mean\nx_quantiles &lt;- apply(x_post, 2, function(x) quantile(x, probs = c(0.025, 0.975))) # 95% credible interval\ndf &lt;- data.frame(t = seq(1, T),\n                 x_mean = x_means,\n                 x_lb = x_quantiles[1, ],\n                 x_ub = x_quantiles[2, ],\n                 x_true = x_true, # latent variables\n                 y = y) # observed values\n\n\nPlot the results\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nggplot(df, aes(x = t)) +\n  geom_ribbon(aes(ymin = x_lb, ymax = x_ub, fill=\"95% CrI\"), alpha=0.5) +\n  geom_line(aes(y=x_mean, color=\"Posterior mean\")) +\n  geom_line(aes(y=x_true, color=\"True\")) +\n  geom_point(aes(y=x_mean, color=\"Posterior mean\")) +\n  geom_point(aes(y=x_true, color=\"True\")) +\n  labs(y=\"values\", x=\"index\") + \n  scale_colour_manual(\"\", values=c(\"Posterior mean\"=\"firebrick\",\n                                   \"True\"=\"darkgrey\")) +\n  scale_fill_manual(\"\", values=\"firebrick\")+\n  theme(legend.position = \"bottom\")\n\n\n\n# ggsave(\"particle_filter.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "blog/posts/oral_cholera_vacc_cov/index.html",
    "href": "blog/posts/oral_cholera_vacc_cov/index.html",
    "title": "Template",
    "section": "",
    "text": "Oral cholera vaccine\nAssumption - Vaccination is implemented in a all-or-nothing fashion - Two-dose efficacy is \\(VE\\) - The first-dose efficacy is \\(ve_1\\)\nWhat the efficacy of the second vaccine dose, \\(ve_2\\), has to be if the want the same number of people is averted,\n\nset.seed(42) # to make it reproducible\nN = 1000\nVE = 0.8 # vaccine efficacy under two dose regimen\nvacc_cov = 0.6 # vaccine coverage\nve1 = 0.4 # efficacy of the first dose\n\nvacc_protected_VE = VE*vacc_cov*N\nvacc_protected_ve1 = ve1*vacc_cov*N\nvacc_protected_ve_ve2 = vacc_protected_ve1 + (VE-ve1)*vacc_cov*N\n\n\n\nImplementing the first and the seocond dose separately\nSuppose that a population size is \\(N\\) and the vaccine coverage, \\(0&lt;=v&lt;=1\\). We introduce a parameter, \\(\\pi\\), to represent the proportion of the first-dose recipients who receive the second dose. If \\(\\pi=1\\), the number of vaccine recipients is \\(Nv\\) and the number of two-dose recipients is also \\(Nv\\) and the number of vaccinees who only received a single dose is zero. More generally, however, if \\(\\pi &lt; 1\\), the number of complete two-dose recipients is \\(Nv\\pi\\) and those who have only received one dose is \\(2Nv(1-\\pi)\\).\n\nset.seed(42) # to make it reproducible\nN = 1000\nve2 = 0.8 # vaccine efficacy under two dose regimen\nve1 = 0.4 # efficacy of the first dose\nvacc_cov = 0.6 # vaccine coverage\npi = 0.95 # proportion of the first-dose recipients who again received the second dose\nvacc_protected_ve2 = ve2*vacc_cov*N\nvacc_protected_ve1 = ve1*vacc_cov*N\nvacc_protected_ve1_ve2 = \n  pi*(vacc_protected_ve1 + (ve2-ve1)*vacc_cov*N) + 2*(1-pi)*ve1*vacc_cov*N\n\nvacc_protected_ve1\n\n[1] 240\n\nvacc_protected_ve1_ve2\n\n[1] 480\n\n\nThese are estimates and algebraic relationship wouldn’t hold. $$ \\[\\begin{align}\nc_2 &= v_1 \\pi \\\\\nc_{1+} &= v_1 \\pi + v_1(1-\\pi) + v_2(1-\\pi v_1/v_2)\n\n\\end{align}\\] $$ \\(c_2\\) and \\(c_{1+}\\) represent coverage for complete two-dose regiment and at least one dose, respectively. \\(\\pi\\) represents the proportion of the vaccinees who received the first dose and went on to receive the second dose.\nEstimates from Pezzolli et al. (2020)\n\ncov_first = 90.3 # v_1\ncov_second = 88.2# v_2\ncov_two &lt;- 69.9 #c_2\ncov_one_plus &lt;- 84.6 #c_1+\n\nBoundary conditions for the pi for the vr1plus must not be bigger than 1\n\n# create a function that calculates \nocv_round_cov_calc = function(vc1=0.95, vc2=0.95, vr1plus=NULL, vr2=NULL, pi=0.7){\n  \n  if (pi &lt; (vc1+vc2-1)/vc1) {\n    stop(paste0(\"pi must be larger than (vc1+vc2-1)/vc1, \", (vc1+vc2-1)/vc1))\n  } else if (pi &gt; (vc2/vc1)) {\n    stop(paste0(\"pi must be smaller than vc2/vc1, \", vc2/vc1))\n  }\n  vr2 = vc1*pi  \n  # vr1plus = vc1*pi + vc1*(1-pi) + vc2*(1-pi*vc1/vc2)\n  vr1plus = vr2 + vc1*(1-pi) + vc2*(1-pi*vc1/vc2) \n  vr1 = vr1plus - vr2\n  # TODO there are several limiting conditions\n  # vaccine coverage is \n  return (list(vr1plus=vr1plus, vr2=vr2, vr1=vr1))\n}\nocv_round_cov_calc(vc1=0.56, vc2=0.46, vr1plus=NULL, vr2=NULL, pi=0.80)\n\n$vr1plus\n[1] 0.572\n\n$vr2\n[1] 0.448\n\n$vr1\n[1] 0.124\n\n\nWhen the first dose is distributed, apply the When the second dose is distributed, identify the number of two-dose and one-dose recipients and compare that number with the previous round\n\n\nSummarize results"
  },
  {
    "objectID": "blog/posts/ode-in-stan/index.html",
    "href": "blog/posts/ode-in-stan/index.html",
    "title": "ODE-based SIR models in Stan",
    "section": "",
    "text": "Stan은 통계 모형 뿐 아니라 ODE 모형을 시물레이션하고 모수를 추정하는 데에도 유용하다. 이 포스팅에서는 일별 감염자 자료가 주어졌을 경우 Stan을 이용하여 SIR 모형의 두 개의 모수 (\\(\\beta, \\gamma\\))를 추정하는 과정을 기술하겠다. 먼저 deSolve 패키지 양식을 따라 SIR 모형을 아래와 같이 구현하고 모형에서 예측되는 일별 감염자 자료 (dayinc) 를 평균으로 하는 거짓 관찰값을 만든다 (yobs).\n\nsir &lt;- function(t, state, parameters) {\n  with(as.list(c(state, parameters)),{\n    # rate of change\n    N &lt;- S + I + R\n    dS &lt;- - beta*S*I/N \n    dI &lt;- + beta*S*I/N - gamma*I\n    dR &lt;- + gamma*I\n    dCI &lt;- + beta*S*I/N \n    \n    # return the rate of change\n    list(c(dS, dI, dR, dCI))\n  }) # end with(as.list ...\n}\n\ny0 &lt;- c(S=999, I=1, R=0, CI=0)\nparms &lt;- c(beta=0.6, gamma=0.4)\ntimes &lt;- seq(0, 40, by = 1)\n\nlibrary(dplyr)\ndeSolve::ode(y=y0, times=times, func=sir, parms=parms) %&gt;% \n  as.data.frame() -&gt; out\n\ndayinc &lt;- diff(out$CI)\nset.seed(42)\nyobs &lt;- rpois(length(dayinc), lambda=dayinc)\n\ndf &lt;- data.frame(time=1:length(dayinc), \n                 model=dayinc,\n                 obs=yobs)\nlibrary(ggplot2)\n# the ggplot theme was adopted from the following website: https://mpopov.com/tutorials/ode-stan-r/\n\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(df)+ \n  geom_line(aes(time, model, color=\"Model\"), linetype=\"dashed\")+\n  geom_point(aes(time, model, color=\"Model\"))+\n  geom_line(aes(time, obs, color=\"Observation\"), linetype=\"dashed\")+\n  geom_point(aes(time, obs, color=\"Observation\"))+\n  labs(x=\"Time (day)\", y=\"Daily incidence\", title=\"Incidence from the SIR model\")+\n  scale_color_manual(\"\", values=c(\"Model\"=\"black\",\"Observation\"=\"firebrick\"))+\n  theme(legend.position=\"bottom\")\n\n\n\n\n아래와 같이 Stan 모형을 만든다. Posterior predictive check 을 하기 위해 generated quantities 블록에 ypred 변수를 넣었다.\n\nstan_code &lt;- \"functions {\n  vector sir(real t,        // time\n             vector y,      // state\n             vector theta  // parameters\n             ) {      \n    vector[4] dydt;\n        \n    real S = y[1];\n    real I = y[2];\n    real R = y[3];\n    real N = S + I + R;\n    \n    real beta = theta[1];\n    real gamma = theta[2];\n    \n    dydt[1] = - beta * S * I / N;\n    dydt[2] = beta * S * I / N - gamma * I;\n    dydt[3] = gamma * I;\n    dydt[4] = beta * S * I / N;\n    \n    return dydt;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; T;\n  real t0;\n  array[T] real ts; \n  vector[4] y0;\n  int y_obs[T];\n}\n\nparameters {\n  vector&lt;lower=0&gt;[2] theta; // [beta, gamma]\n}\n\nmodel {\n  array[T] vector[4] mu = ode_rk45(sir, y0, t0, ts, theta);\n  real dayinc[T]; // daily incidence\n  dayinc[1] = mu[1, 4] + 1e-12;\n  for (t in 2:T){\n    dayinc[t] = mu[t, 4] - mu[t-1, 4] + 1e-12; \n  }\n  theta ~ exponential(1); // both parameters are on the positive real line\n  y_obs ~ poisson(dayinc); // likelihood\n}\n\ngenerated quantities {\n  array[T] vector[4] mu = ode_rk45(sir, y0, t0, ts, theta);\n  real dayinc[T];\n  dayinc[1] = mu[1, 4] + 1e-12;\n  for (t in 2:T){\n    dayinc[t] = mu[t,4] - mu[t-1,4] + 1e-12;\n  }\n  int ypred[T]; // posterior predictive \n  for (t in 1:T) {\n    ypred[t] = poisson_rng(dayinc[t]);\n  }\n}\n\"\n\n아래와 같이 Stan 모형을 이용해서 샘플링을 한다.\n\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\n# this is for the stan model in a separate file\n# mod &lt;- stan_model(file=paste0(getwd(),\"/stan/sir_stan.stan\"),\n#                   verbose=TRUE)\nmod &lt;- stan_model(model_code=stan_code, verbose=TRUE)\nT &lt;- 40 # end time unit for the ODE model, also the number of data points\ndata &lt;- list(T=T, t0=0.0, ts=1:T, y0=c(999,1,0,0), y_obs=yobs)\nsmp &lt;- sampling(object=mod, data=data, seed=42, chains=4, iter=2000)\n# saveRDS(smp, \"outputs/stan_smp_20230801.rds\")\n\n모수의 posterior 분포를 살펴보자.\n\n# smp &lt;- readRDS(\"outputs/stan_smp_20230801.rds\")\nsmp &lt;- readRDS(\"stan_smp_20230801.rds\") # file is under the content/post/the_relevant_post_name/index_files/figure_html\ndf &lt;- as.data.frame(smp)\npr &lt;- c(0.5,0.025,0.975)\nd &lt;- as.data.frame(t(apply(df[,grepl(\"^theta.*\", names(df))],\n                           2, quantile, probs=pr)))\nd$name &lt;- c(\"beta\", \"gamma\")\nd$true &lt;- c(0.6, 0.4)\nggplot(d)+ \n  geom_errorbar(aes(x=name, ymin=`2.5%`, ymax=`97.5%`), width=0.0)+\n  geom_point(aes(x=name, y=`50%`, color=\"Estimates\"), size=2)+\n  geom_point(aes(x=name, y=true, col=\"True value\"), size=3)+\n  scale_color_manual(values=c(\"Estimates\"=\"black\",\"True value\"=\"firebrick\"))+\n  labs(x=\"\", y=\"\", title=\"Median estimates with 95% CrI\")+\n  theme(legend.position=\"bottom\", legend.title=element_blank())+\n  scale_x_discrete(breaks=c(\"beta\",\"gamma\"),\n                   labels=c(expression(beta),expression(gamma)))+\n  coord_flip()\n\n\n\n\n마지막으로 posterior predictive check을 통해서 모수 추정을 위해 사용했던 자료와 비교해 보자.\n\n# pr &lt;- c(0.5,0.025,0.975)\nd &lt;- as.data.frame(t(df[,grepl(\"^ypred.*\", names(df))]))\nd$time &lt;- 1:40\ndlong &lt;- tidyr::pivot_longer(d, cols=-time)\ndayincdf &lt;- data.frame(inc=dayinc, time=1:40)\nyobsdf &lt;- data.frame(obs=yobs, time=1:40)\n\nggplot(dlong)+ \n  geom_line(aes(time, value, group=name, color=\"Posterior predictive\"))+\n  geom_line(data=dayincdf, aes(time, inc, color=\"Model\"))+\n  geom_point(data=yobsdf, aes(time, obs, color=\"Observation\"))+\n  geom_line(data=yobsdf, aes(time, obs, color=\"Observation\"), linetype=\"dashed\")+\n  labs(x=\"Time (day)\", y=\"Daily incidence\", title=\"Posterior predictive check\")+\n  scale_color_manual(\"\", values=c(\"Model\"=\"black\",\"Posterior predictive\"=\"grey\",\"Observation\"=\"firebrick\"))+\n  theme(legend.position=\"bottom\")"
  },
  {
    "objectID": "blog/posts/neural_network_basic/index.html",
    "href": "blog/posts/neural_network_basic/index.html",
    "title": "A very basic implementation of a neural network",
    "section": "",
    "text": "I am documenting my learning of a neural network. The contents are mostly based on the e-book.\nLoad the torch library.\n\nrequire(torch)\n\n\nData\n\n# input dimensionality (number of input features)\ndim_in &lt;- 3\n# number of observations in training set\nn &lt;- 200\n\nx &lt;- torch_randn(n, dim_in)\ncoefs &lt;- c(0.2, -1.3, -0.5)\ny &lt;- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1) # column matrix\n\nWeights and biases\n\\[f(\\bf{X})=\\bf{XW}+b\\]\nUsing two layers with corresponding parameters, w1, b1, w2 and b2.\n\\[f(\\bf{X})=(\\bf{XW_1}+b_1)\\bf{W_2}+b_2\\]\ny_pred &lt;- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)\n\n# dimensionality of hidden layer\ndim_hidden &lt;- 32\n# output dimensionality (number of predicted features)\ndim_out &lt;- 1\n\n# weights connecting input to hidden layer\nw1 &lt;- torch_randn(dim_in, dim_hidden, requires_grad = TRUE)\n# weights connecting hidden to output layer\nw2 &lt;- torch_randn(dim_hidden, dim_out, requires_grad = TRUE)\n\n# hidden layer bias\nb1 &lt;- torch_zeros(1, dim_hidden, requires_grad = TRUE)\n# output layer bias\nb2 &lt;- torch_zeros(1, dim_out, requires_grad = TRUE)\n\nPredicted values from the above network is computed as follows and using Rectified Linear Unit (ReLU) as the activation function\n\ny_pred &lt;- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)\n\nThen the loss function can be created as follows\n\nloss &lt;- (y_pred - y)$pow(2)$mean()\n\n\nlearning_rate &lt;- 1e-2\n\n### training loop ----------------------------------------\n\nfor (epoch in 1:200) {\n  ### -------- Forward pass --------\n  y_pred &lt;- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)\n  \n  ### -------- Compute loss -------- \n  loss &lt;- (y_pred - y)$pow(2)$mean()\n  if (epoch %% 10 == 0)\n    cat(\"Epoch: \", epoch, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### -------- Backpropagation --------\n  # compute gradient of loss w.r.t. all tensors with\n  # requires_grad = TRUE\n  loss$backward()\n  \n  ### -------- Update weights -------- \n  # Wrap in with_no_grad() because this is a part we don't \n  # want to record for automatic gradient computation\n   with_no_grad({\n     w1 &lt;- w1$sub_(learning_rate * w1$grad)\n     w2 &lt;- w2$sub_(learning_rate * w2$grad)\n     b1 &lt;- b1$sub_(learning_rate * b1$grad)\n     b2 &lt;- b2$sub_(learning_rate * b2$grad)  \n     \n     # Zero gradients after every pass, as they'd\n     # accumulate otherwise\n     w1$grad$zero_()\n     w2$grad$zero_()\n     b1$grad$zero_()\n     b2$grad$zero_()  \n   })\n}\n\nEpoch:  10    Loss:  3.000276 \nEpoch:  20    Loss:  2.144468 \nEpoch:  30    Loss:  1.749418 \nEpoch:  40    Loss:  1.538223 \nEpoch:  50    Loss:  1.413543 \nEpoch:  60    Loss:  1.33866 \nEpoch:  70    Loss:  1.294799 \nEpoch:  80    Loss:  1.265488 \nEpoch:  90    Loss:  1.244047 \nEpoch:  100    Loss:  1.226817 \nEpoch:  110    Loss:  1.212944 \nEpoch:  120    Loss:  1.201177 \nEpoch:  130    Loss:  1.190159 \nEpoch:  140    Loss:  1.178311 \nEpoch:  150    Loss:  1.167546 \nEpoch:  160    Loss:  1.157191 \nEpoch:  170    Loss:  1.147406 \nEpoch:  180    Loss:  1.13854 \nEpoch:  190    Loss:  1.131134 \nEpoch:  200    Loss:  1.123894 \n\n\nEvaluate the model visually\n\n# png(\"obs_pred.png\")\ny_pred &lt;- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)\nplot(y, y_pred, xlab=\"Observed\", ylab=\"Predicted\", \n     main=\"Neural network from scratch\")\nabline(a=0, b=1, col=\"red\")\n\n\n\n# dev.off()\nsum((as.numeric(y) - as.numeric(y_pred))^2)\n\n[1] 224.638\n\n\nThe same model can be created in a more compactly way using a sequential module and using the activation function.\n\nnet &lt;- nn_sequential(\n  nn_linear(dim_in, dim_hidden),\n  nn_relu(),\n  nn_linear(dim_hidden, dim_out)\n)\n\nTrain using the Adam optimizer, a popular choice.\n\nopt &lt;- optim_adam(net$parameters)\n# opt &lt;- optim_sgd(net$parameters, lr=0.001)\n\n\n### training loop --------------------------------------\n\nfor (epoch in 1:200) {\n  # forward pass\n  y_pred &lt;- net(x)\n  # compute loss \n  loss &lt;- nnf_mse_loss(y_pred, y)\n  if (epoch %% 10 == 0) { \n    cat(\"Epoch: \", epoch, \", Loss: \", loss$item(), \"\\n\")\n  }\n  # back propagation \n  opt$zero_grad()\n  loss$backward()\n  # update weights\n  opt$step()\n}\n\nEpoch:  10 , Loss:  3.195003 \nEpoch:  20 , Loss:  2.957336 \nEpoch:  30 , Loss:  2.741568 \nEpoch:  40 , Loss:  2.544529 \nEpoch:  50 , Loss:  2.363058 \nEpoch:  60 , Loss:  2.193356 \nEpoch:  70 , Loss:  2.034059 \nEpoch:  80 , Loss:  1.885832 \nEpoch:  90 , Loss:  1.748948 \nEpoch:  100 , Loss:  1.624851 \nEpoch:  110 , Loss:  1.513974 \nEpoch:  120 , Loss:  1.417417 \nEpoch:  130 , Loss:  1.33595 \nEpoch:  140 , Loss:  1.269105 \nEpoch:  150 , Loss:  1.216185 \nEpoch:  160 , Loss:  1.176016 \nEpoch:  170 , Loss:  1.147551 \nEpoch:  180 , Loss:  1.128549 \nEpoch:  190 , Loss:  1.116211 \nEpoch:  200 , Loss:  1.108102 \n\n\nCompare the prediction and observation\n\ny_pred_s &lt;- net(x)\nplot(y, y_pred, xlab=\"Observed\", ylab=\"Predicted\", \n     main=\"Neural network: A sequential module\")\nabline(a=0, b=1, col=\"red\")\n\n\n\n# Mean squared error, L2 loss\nsum((as.numeric(y) - as.numeric(y_pred))^2)\n\n[1] 221.6205\n\n\nCompared with the linear model\n\nxdf &lt;- as.data.frame(as.matrix(x))\nnames(xdf) &lt;- c(\"x1\",\"x2\", \"x3\")\nydf &lt;- as.data.frame(as.matrix(y))\nnames(ydf) &lt;- c(\"y\")\ndat &lt;- cbind(xdf, ydf)\nm &lt;- lm(y~x1+x2+x3, data=dat)\ny_pred_lm &lt;- predict(m, xdf)\nydf2 &lt;- cbind(ydf, y_pred_lm)\nplot(ydf2[,1], ydf2[,2], xlab=\"Observed\", ylab=\"Predicted\", \n     main=\"Linear regresssion\")\nabline(a=0, b=1, col=\"red\")\n\n\n\n# Mean squared error, L2 loss\nsum((ydf$y - y_pred_lm)^2)\n\n[1] 218.2733"
  },
  {
    "objectID": "blog/posts/nb-censored-regression/index.html",
    "href": "blog/posts/nb-censored-regression/index.html",
    "title": "Negative binomial regression with censored data: POLYMOD data",
    "section": "",
    "text": "This post describes my attempt to reproduce Table 1 of the paper, Social Contacts and Mixing Patterns Relevant to the Spread of Infectious Diseases. Data were downloaded from Social Contact Data, which was hosted in zenodo. I used the version 1.1. In summary, I wasn’t successful at reproducing the table exactly but still wanted to document the processes that I went through.\n\nData preparation\n\nlibrary(data.table)\nlibrary(dplyr)\nd1 &lt;- fread(\"2008_Mossong_POLYMOD_participant_common.csv\")\n# d1 &lt;- fread(\"data/2008_Mossong_POLYMOD_participant_common.csv\")\nd2 &lt;- fread(\"2008_Mossong_POLYMOD_contact_common.csv\")\n# d2 &lt;- fread(\"data/2008_Mossong_POLYMOD_contact_common.csv\")\n# count the number of contacts for each participant using part_id variable\nd2 |&gt; group_by(part_id) |&gt;\n  summarize(contacts = n()) -&gt; d2_contacts\nd12 &lt;- left_join(d1, d2_contacts, by=\"part_id\")\n# add household information\nd3 &lt;- fread(\"2008_Mossong_POLYMOD_hh_common.csv\")\n# d3 &lt;- fread(\"data/2008_Mossong_POLYMOD_hh_common.csv\")\nd123 &lt;- left_join(d12, d3, by=\"hh_id\")\n# add day of week information\nd4 &lt;- fread(\"2008_Mossong_POLYMOD_sday.csv\")\n# d4 &lt;- fread(\"data/2008_Mossong_POLYMOD_sday.csv\")\ndat &lt;- left_join(d123, d4, by=\"part_id\")\n\n\n\nData manipulation\nCategorize the age group into different 10 age groups: 0-4, 5-9, 10-14, 15-19, and 20 to 70 by 10 years and 70 and above\n\nage_grp_label &lt;- c(\"0-4\",\"5-9\",\"10-14\",\"15-19\",\"20-29\",\n                   \"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70+\")\n\ndat$age_grp &lt;- ifelse(is.na(dat$part_age), \"Missing\", \n                      ifelse(dat$part_age &lt; 20,\n                             age_grp_label[floor(dat$part_age/5) + 1], \n                             ifelse(dat$part_age &lt; 80,\n                                    age_grp_label[floor(dat$part_age/10) + 3],\n                                    age_grp_label[10])))\n\nCompare the number of participants by age group (the third column of Table 1)\n\ndat |&gt; group_by(age_grp) |&gt; summarize(npart=n()) -&gt; npart_ag\n# hard-coded using the values in Table 1.\nnpart_ag$true &lt;- c(660,661,713,685,879,815,908,906,728,270,65) \nnpart_ag\n\n# A tibble: 11 × 3\n   age_grp npart  true\n   &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n 1 0-4       660   660\n 2 10-14     713   661\n 3 15-19     685   713\n 4 20-29     879   685\n 5 30-39     815   879\n 6 40-49     908   815\n 7 5-9       661   908\n 8 50-59     906   906\n 9 60-69     728   728\n10 70+       270   270\n11 Missing    65    65\n\n\nCategorize the household size\n\nclassify_household &lt;- function(d){\n  d$hh_size_grp &lt;- \"Missing\"\n  d$hh_size_grp &lt;- ifelse(d$hh_size &gt; 5, \"6+\", as.character(d$hh_size))\n  return(d)\n}\ndat &lt;- classify_household(dat)\n\nClassify gender\n\nclassify_gender &lt;- function(d) {\n  d$gender &lt;- \"Missing\"\n  d$gender &lt;- ifelse(d$part_gender == \"M\", \"M\", \n                     ifelse(d$part_gender == \"F\", \"F\", d$gender))\n  \n  return(d)\n}\n\ndat &lt;- classify_gender(dat)\n\nClassify day of week\n\ndayofweek_label &lt;- c(\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\n                     \"Friday\",\"Saturday\")\n\nclassify_dayofweek &lt;- function(d) {\n  d$dayofweek_f &lt;- \"Missing\"\n  for (i in 1:nrow(d)) {\n    day = d$dayofweek[i]\n    if (!is.na(day)) {\n      d$dayofweek_f[i] &lt;- dayofweek_label[day+1]\n    }\n  }\n  return(d)\n}\ndat &lt;- classify_dayofweek(dat)\n# change names for conveninence\ndat$dayofweek_integer &lt;- dat$dayofweek\ndat$dayofweek &lt;- dat$dayofweek_f\n\nMake categorical variables factor for regression Set reference groups relevel(x, ref=ref) as in Table 1\n\n# set the categorical variables as factor for regression\ndat$age_grp &lt;- factor(dat$age_grp, levels=c(age_grp_label,\"Missing\"))\ndat$age_grp &lt;- relevel(dat$age_grp, ref=\"0-4\")\ndat$gender &lt;- factor(dat$gender, levels=c(\"F\",\"M\",\"Missing\"))\ndat$gender &lt;- relevel(dat$gender, ref = \"F\")\ndat$dayofweek &lt;- factor(dat$dayofweek, levels=c(dayofweek_label,\"Missing\"))\ndat$dayofweek &lt;- relevel(dat$dayofweek, ref = \"Sunday\")\ndat$hh_size_grp &lt;- as.factor(dat$hh_size_grp)\ndat$hh_size_grp &lt;- relevel(dat$hh_size_grp, ref=\"1\")\ndat$country &lt;- factor(dat$country, levels=c(\"BE\",\"DE\",\"FI\",\"GB\",\"IT\",\"LU\",\"NL\",\"PL\"))\ndat$country &lt;- relevel(dat$country, ref=\"BE\")\n\n# fwrite(dat, \"POLYMOD_2017.csv\")\n\nAssign weights to individual participants based on the supplementary Table 2. My approach was to identify a row and a column for the relevant weight based on the age and household size. Weight of an age group for the sample was calculated by dividing the proportion of the age group in the population (in the census) with the proportion of the age group in the sample.\n\nfind_age_row_column &lt;- function(d) {\n  d$age_row &lt;- NA\n  for (i in 1:nrow(d)) {\n    ag &lt;- d$part_age[i]\n    if(!is.na(ag)){\n      for (j in 1:14) {\n        if (ag &gt;= (j-1)*5 & ag &lt; (j-1)*5+5) {\n          d$age_row[i] &lt;- j\n          break\n        }\n        else if (ag &gt;= 70) {\n          d$age_row[i] &lt;- 15\n          break\n        }\n        else{}\n      }\n    }\n  }\n  \n  d$hh_col &lt;- NA\n  for (i in 1:nrow(d)) {\n    hs &lt;- d$hh_size[i]\n    if(!is.na(hs)){\n      for (j in 1:4) {\n        if (hs == j) {\n          d$hh_col[i] &lt;- j\n          break\n        }\n        else if (hs &gt; 4) {\n          d$hh_col[i] &lt;- 5\n        }\n        else{}\n      }\n    }\n  }\n  return(d)\n}\n\ndat &lt;- find_age_row_column(dat)\n# wlist &lt;- rio::import_list(\"data/sampling_weight.xlsx\")\nwlist &lt;- rio::import_list(\"sampling_weight.xlsx\")\n\nclassify_weight &lt;- function(d){\n  d$wt &lt;- NA\n  cnames &lt;- names(wlist)\n  for (i in 1:length(cnames)) {\n    wtable &lt;- wlist[[i]]\n    w1 &lt;- wtable[wtable$`Household size` == \"Ratio C/S\",] # sampling weight\n    W &lt;- w1[!is.na(w1$`Household size`),] # remove the first row\n    # View(W)\n    for (j in 1:nrow(d)){\n      if(d$country[j] == cnames[i]) {\n        d$wt[j] &lt;- W[d$age_row[j], d$hh_col[j]+2]\n      }\n    }\n  }\n  return(d)\n}\n\n# grep(\"-\", as.character(d$wt), value = T)\n# hist(as.numeric(d$wt))\ndat &lt;- classify_weight(dat)\ndat$wt &lt;- as.numeric(dat$wt)\n\n\n\nData for fitting only complete cases\nThere are missing values for contacts ($n$=36) and weight ($n$=65). It is not clear how those observations were treated in the model. This may be a reason why I can’t reproduce the results in Table 1.\n\ndat_ &lt;- dat\n## would imputation for the 36 observations make a difference?\n# dat$contacts_ori &lt;- dat$contacts\n# dat$contacts &lt;- ifelse(is.na(dat$contacts), round(mean(dat$contacts, na.rm=T)), dat$contacts)\n\nmodel_var &lt;- c(\"contacts\", \"age_grp\", \"gender\", \"dayofweek\", \"hh_size_grp\", \"country\", \"wt\")\ndat &lt;- dat[,..model_var]\n# dat &lt;- dat[complete.cases(dat),]\ndat &lt;- dat[!is.na(contacts),]\n\n\n\nNegBin regression: no censoring and no weighting\n\nlibrary(MASS)\nm &lt;- glm.nb(contacts ~ age_grp + gender + dayofweek + hh_size_grp + country, \n            data = dat)\n# summary(m4)\nexp(m$coefficients)\n\n       (Intercept)         age_grp5-9       age_grp10-14       age_grp15-19 \n         5.6764958          1.4022341          1.6768688          1.6824225 \n      age_grp20-29       age_grp30-39       age_grp40-49       age_grp50-59 \n         1.4458386          1.4168460          1.3868642          1.2940456 \n      age_grp60-69         age_grp70+     age_grpMissing            genderM \n         1.0520198          0.8120547          1.0338752          0.9941243 \n     genderMissing    dayofweekMonday   dayofweekTuesday dayofweekWednesday \n         1.3434436          1.3179053          1.3998594          1.3876252 \n dayofweekThursday    dayofweekFriday  dayofweekSaturday   dayofweekMissing \n         1.3858679          1.4403113          1.1542837          1.2179674 \n      hh_size_grp2       hh_size_grp3       hh_size_grp4       hh_size_grp5 \n         1.0990550          1.1255471          1.2592436          1.3360603 \n     hh_size_grp6+          countryDE          countryFI          countryGB \n         1.4601697          0.7036079          0.9483997          0.9797998 \n         countryIT          countryLU          countryNL          countryPL \n         1.6351704          1.4081732          1.2493105          1.3223343 \n\n\nRegression that account for censored number of contacts. The paper reads: The data were right censored at 29 contacts for all countries because of a limited number of possible diary entries in some countries\n\\[\n\\text{log }L = \\sum_{i=1}^n w_i\\left(\\delta_i~\\text{log} \\left(P\\left(Y=y_i|X\\right)\\right) + \\left(1-\\delta_i\\right)~\\text{log} \\left(1-\\sum_{i=1}^{28}P(Y=y_i|X)\\right)\n\\right)\n\\] , where\n\\[\n\\begin{equation}\n  \\delta_i=\\begin{cases}\n    1, & \\text{if$~y_i&lt;29$}.\\\\\n    0, & \\text{otherwise}.\n  \\end{cases}\n\\end{equation}\n\\]\n\n\nTake censoring into account\n\nX = model.matrix(m)\n# X &lt;- model.matrix(~ age_grp + gender + dayofweek + hh_size_grp + country, data=dat)\n\n# ini = c(coef(m1), log_theta = log(summary(m1)$theta))\ninit = c(coef(m), size=summary(m)$theta)\n\nnegll_censor &lt;- function(par, y, X, ul=400) {\n  # parameters\n  size = par[length(par)]\n  beta = par[-length(par)]\n  # create indicator depending on chosen limit\n  indicator = y &lt; ul\n  # linear predictor\n  mu = exp(X %*% beta)\n  # log likelihood\n  ll = sum(indicator * dnbinom(y, mu=mu, size=size, log=T) +\n             (1-indicator) * log(1-pnbinom(ul, mu=mu, size=size)), na.rm=T)\n  return(-ll)\n}\n\n# you can check if two methods (glm.nb vs. optim) match by setting ul high (e.g., 100)\nfit1 &lt;- optim(par=init,\n            negll_censor,\n            y = dat$contacts,\n            X = X,\n            ul = 29,\n            method = \"Nelder-Mead\",\n            control = list(maxit=1e3, reltol=1e-10))\nexp(fit1$par)\n\n       (Intercept)         age_grp5-9       age_grp10-14       age_grp15-19 \n         5.6047431          1.4335817          1.7328973          1.7131792 \n      age_grp20-29       age_grp30-39       age_grp40-49       age_grp50-59 \n         1.4411616          1.4214491          1.3811132          1.2825334 \n      age_grp60-69         age_grp70+     age_grpMissing            genderM \n         1.0518372          0.8182114          1.0369387          0.9825757 \n     genderMissing    dayofweekMonday   dayofweekTuesday dayofweekWednesday \n         1.3945323          1.3504467          1.4340414          1.4146111 \n dayofweekThursday    dayofweekFriday  dayofweekSaturday   dayofweekMissing \n         1.4265521          1.4638406          1.1583928          1.2481137 \n      hh_size_grp2       hh_size_grp3       hh_size_grp4       hh_size_grp5 \n         1.0879976          1.1266973          1.2623833          1.3638927 \n     hh_size_grp6+          countryDE          countryFI          countryGB \n         1.4878961          0.6934848          0.9561751          1.0065923 \n         countryIT          countryLU          countryNL          countryPL \n         1.6915581          1.3931403          1.2521232          1.3348934 \n              size \n        18.9737736 \n\n\n\n\nTake censoring & weighting into account\n\nX = model.matrix(m)\n# X &lt;- model.matrix(~ age_grp + gender + dayofweek + hh_size_grp + country, data=dat) # full matrix\n# ini = c(coef(m1), log_theta = log(summary(m1)$theta))\ninit = c(coef(m), size=summary(m)$theta)\n\nnegll_censor_weight &lt;- function(par, y, X, wt, ul=400) {\n  # parameters\n  size = par[length(par)]\n  beta = par[-length(par)]\n  # create indicator depending on chosen limit\n  indicator = y &lt; ul\n  # linear predictor\n  mu = exp(X %*% beta)\n  # log likelihood\n  ll = sum(wt*(indicator * dnbinom(y, mu=mu, size=size, log=T)\n  + (1-indicator) * log(1-pnbinom(ul, mu=mu, size=size))), na.rm=T)\n\n  return(-ll)\n}\n\nfit2 &lt;- optim(par=init,\n            negll_censor_weight,\n            y = dat$contacts,\n            X = X,\n            wt = dat$wt,\n            ul = 29,\n            method = \"Nelder-Mead\",\n            control = list(maxit=1e3, reltol=1e-10))\n\nexp(fit2$par)\n\n       (Intercept)         age_grp5-9       age_grp10-14       age_grp15-19 \n         5.6120605          1.4242420          1.7386774          1.7031201 \n      age_grp20-29       age_grp30-39       age_grp40-49       age_grp50-59 \n         1.4590198          1.4488638          1.3920936          1.3145593 \n      age_grp60-69         age_grp70+     age_grpMissing            genderM \n         1.0690678          0.8092529          0.9944973          0.9919151 \n     genderMissing    dayofweekMonday   dayofweekTuesday dayofweekWednesday \n         1.0153858          1.3261171          1.3904167          1.3853431 \n dayofweekThursday    dayofweekFriday  dayofweekSaturday   dayofweekMissing \n         1.4001935          1.4212925          1.1905515          1.2452143 \n      hh_size_grp2       hh_size_grp3       hh_size_grp4       hh_size_grp5 \n         1.1014090          1.1254771          1.2788309          1.3717150 \n     hh_size_grp6+          countryDE          countryFI          countryGB \n         1.4784786          0.6889027          0.9561525          0.9925714 \n         countryIT          countryLU          countryNL          countryPL \n         1.6631404          1.4198698          1.3256452          1.3635879 \n              size \n        17.7725195 \n\n\n\nda &lt;- data.frame(\n  parm=c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-29\", \"30-39\", \"40-49\", \n\"50-59\", \"60-69\", \"70+\", \"Missing\", \n\"F\",\"M\",\"Missing\",\n\"1\", \"2\", \"3\", \"4\", \"5\", \"6+\",\n\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \n\"Saturday\", \"Missing\",\n\"BE\", \"DE\", \"FI\", \"GB\", \"IT\", \"LU\", \"NL\", \"PL\"),\nest = c(1.00, 1.42, 1.73, 1.68, 1.45, 1.45, 1.38, 1.31, 1.06, 0.81,0.91, \n        1.00, 0.99, 1.57, \n        1.00, 1.17, 1.20, 1.36, 1.46, 1.56,\n        1.00, 1.33, 1.39, 1.38, 1.41, 1.43, 1.20, 1.24,\n        1.00, 0.70, 0.94, 0.99, 1.66, 1.42, 1.34, 1.37))\n\nda$myest = round(c(1.00, exp(fit2$par)[2:11], \n             1.00, exp(fit2$par)[12:13],\n             1.00, exp(fit2$par)[21:25],\n             1.00, exp(fit2$par)[14:20],\n             1.00, exp(fit2$par)[26:32]), digits=2)\nda\n\n        parm  est myest\n1        0-4 1.00  1.00\n2        5-9 1.42  1.42\n3      10-14 1.73  1.74\n4      15-19 1.68  1.70\n5      20-29 1.45  1.46\n6      30-39 1.45  1.45\n7      40-49 1.38  1.39\n8      50-59 1.31  1.31\n9      60-69 1.06  1.07\n10       70+ 0.81  0.81\n11   Missing 0.91  0.99\n12         F 1.00  1.00\n13         M 0.99  0.99\n14   Missing 1.57  1.02\n15         1 1.00  1.00\n16         2 1.17  1.10\n17         3 1.20  1.13\n18         4 1.36  1.28\n19         5 1.46  1.37\n20        6+ 1.56  1.48\n21    Sunday 1.00  1.00\n22    Monday 1.33  1.33\n23   Tuesday 1.39  1.39\n24 Wednesday 1.38  1.39\n25  Thursday 1.41  1.40\n26    Friday 1.43  1.42\n27  Saturday 1.20  1.19\n28   Missing 1.24  1.25\n29        BE 1.00  1.00\n30        DE 0.70  0.69\n31        FI 0.94  0.96\n32        GB 0.99  0.99\n33        IT 1.66  1.66\n34        LU 1.42  1.42\n35        NL 1.34  1.33\n36        PL 1.37  1.36"
  },
  {
    "objectID": "blog/posts/max-likelihood/index.html",
    "href": "blog/posts/max-likelihood/index.html",
    "title": "Maximum Likelihood and Profile Likelihood for the SEIR model",
    "section": "",
    "text": "통계학은 많은 부분 확률모형의 모수를 추정하는 (inferential statistics) 과정이고 모수 추정방법으로 가장 많이 사용되는 방법이 maximum likelihood (ML)이다. 이번 포스트는 2014년 출간된 Cole et al.의 Maximum Likelihood, Profile Likelihood, and Penalized Likelihood: A Primer을 차용하여 maximum likelihood (ML) 와 profile likelihood에 대하여 기술하여 보고자 한다.\n\n최대 가능도 (ML)\n잠복기를 추정하기 위해 증상 발현일과 기존 감염자와의 접촉일을 묻는 설문조사를 했다고 하자. \\(n\\) 명을 인터뷰하고 \\(n\\) 개의 관찰값 \\(y_1, y_2, ..., y_n\\) 을 얻었다고 하자. 잠복기의 분포에 대한 확률모형 \\(f (y|\\boldsymbol{\\theta})\\) 은 주어진 모수\\(\\boldsymbol{\\theta}=(\\theta_1, \\theta_2, ..., \\theta_j)\\) 하에서 \\(Y=y\\)가 될 확률을 나타낸다.\n최대 가능도 방법은 미지의 모수 하에서 관찰값의 확률의 나타낸다. 확률 모형 \\(f(y|\\boldsymbol{\\theta})\\)이 주어진 모수 \\(\\boldsymbol{\\theta}\\) 하에서 \\(Y\\)의 확률을 나타내는 반면 최대 가능도 방법은 \\(Y\\)를 관찰값에 고정한 채 \\(\\boldsymbol{\\theta}\\)의 함수로 표현하게 된다. 따라서 확률모형과는 다르게 다음과 같은 식 \\(\\mathcal{L}(\\boldsymbol{\\theta};y_i)\\) 을 사용한다. 즉, 우리가 관심있어 하는 것은 확률모형 \\(f (y|\\boldsymbol{\\theta})\\)이 \\(Y\\) 가 아니고 \\(\\boldsymbol{\\theta}\\) 에 따라 어떻게 변하는가 하는 것이다. \\(\\mathcal{L}(\\boldsymbol{\\theta};y_i)\\) 를 \\(i\\) 번째 관찰값이 가능도에 영향을 미치는 정도라 하고 관찰값이 상호독립적이라고 가정하면 관찰값 전체의 가능도는 아래와 같이 표현할 수 있다.\n\\[\\mathcal{L}(\\boldsymbol{\\theta};\\boldsymbol{y}) = \\prod_{i=1}^{n} \\mathcal{L}(\\boldsymbol{\\theta};y_i) = \\prod_{i=1}^{n} f(y_i;\\boldsymbol{\\theta})\\]\n위 식에서 \\(\\boldsymbol{y}=(y_1, y_2, ..., y_n)\\)을 나타낸다.\n\\(\\mathcal{L}(\\boldsymbol{\\theta};\\boldsymbol{y})\\) 는 \\(\\boldsymbol{\\theta}\\)에 대한 확률을 알 수는 없기 때문에 확률모형이 아닌 가능도 (혹은 우도) 함수라고 한다. ML은 가능도 함수를 최대로 만들어 주는 \\(\\boldsymbol{\\theta}\\)로 모수에 대한 추정치를 정의한다.\n\\[\\hat{\\theta} = \\textrm{argmax}_{\\theta}\\{{\\mathrm{log} \\mathcal{L}(\\theta)}\\}\\]\n위 식에서 \\(\\mathrm{log}\\)를 사용한 이유는 가능도 값이 매우 작은 수가되는 경우가 많고 따라서 컴퓨터를 이용한 계산상의 안정성을 위해서 (i.e., arithmetic underflow 가 일어나지 않게 하기 위해) 실제로는 \\(\\mathrm{log} \\mathcal{L}(\\theta)\\)를 사용하기 때문이다. 추가적으로 많은 최적화 알고리듬의 경우 최소화가 기본값으로 설정되어 있어 최대 가능도법을 구현할 때는 \\(-\\mathrm{log} \\mathcal{L}(\\theta)\\)를 사용하는 경우가 많다.\n최대 가능도법을 이용하여 푸아송 분포의 모수를 추정하는 과정을 살펴보자.\n\n\n푸아송 분포 모수 추정\n위에서 언급했던 잠복기의 예를 살펴보자. 잠복기는 Weibull, Gamma, 혹은 Lognormal 등 두 개의 모수를 가지는 확률모형이 많이 사용되는 데 아래 예에서는 계산상의 편의를 위해서 하나의 모수를 가지는 푸아송 분포를 사용하였다.\n\nset.seed(1220)\nn &lt;- 50 # number of observations\nlamb &lt;- 23 # true parameter value\ny &lt;- rpois(n, lambda=lamb) # observations\nnll_theta &lt;- function(theta){\n  - sum(dpois(y, lambda=theta, log=T)) # negative log likelihood\n}\nres = optimize(f=nll_theta, interval=c(0,1e6))\nres$minimum #\\hat{\\theta} compare w/ lamb\n\n[1] 24.2\n\nexp(- res$minimum) # likelihood\n\n[1] 3.090828e-11\n\n\n다음 번 포스팅에는 ML로 추정된 모수의 신뢰구간을 구하는 방법을 샆펴보자."
  },
  {
    "objectID": "blog/posts/Learning_ChatGPT_4_UniversalApproximationTheorem/index.html",
    "href": "blog/posts/Learning_ChatGPT_4_UniversalApproximationTheorem/index.html",
    "title": "Learning ChatGPT 4: Universal Approximation Theorem",
    "section": "",
    "text": "Neural networks can approximate any function according to the Universal Approximation Theorem. A more precise statement of the theorem is that neural networks with a single hidden layer can be used to approximate any continuous function to any desired precision by the e-book by Michael Nielsen. This e-book provides intuitive explanations about the proof. In what follows, I wrote what I was able to understand based on those materials and added a few R code chunks to supplement.\nFor a simple network with one hidden layer of two neurons, n1 and n2, what is being computed in a hidden neuron is \\(\\sigma(w x + b)\\), where \\(\\sigma (z) \\equiv 1/(1+e^{-z})\\) is the sigmoid function. And what happens in the output neuron is \\(w_{31} n1 + w_{32} n2 + b\\). Here, \\(w_{31} \\text{and} w_{32}\\) refer to weights that correspond to two neurons, n1 and n2, respectively.\nThe basic idea of the theorem is that weighted sum of outputs of neurons can approximate any functions. This can be visualized by studying a special case in which output from each neuron is close to a step function and weighted sum of a pair of step functions can create a bump of any height and width. With numerous those bumps, one can approximate any continuous function at any desired precision.\n\nlibrary(igraph)\nplot(graph_from_literal(Input--+n1, Input--+n2, n1--+Output, n2--+Output))\n\n\n\n\nIn what follows, I will explain how a neural network can create a bump. Before that, let’s get familiar with what is being computed in a single neuron. In the following codes, some combination of weight, \\(w\\) , and bias, \\(b\\), generate a typical sigmoid function.\n\nx &lt;- seq(0, 1, 0.01)\nw &lt;- 20\nb &lt;- -5\nplot(x, 1/(1+exp(-(w*x+b))), type=\"l\", xlab=\"Input\", ylab=\"n1\")\n\n\n\n\nWith high values for \\(w\\), the output becomes close to a step function and the change point occurs at \\(-b/w\\). For instance, let’s look at the plot for \\(w=1000, b=-300\\).\n\nx &lt;- seq(0, 1, 0.01)\nw &lt;- 1e3\nb &lt;- -300\nplot(x, 1/(1+exp(-(w*x+b))), type=\"l\", xlab=\"Input\", ylab=\"n1\")\n\n\n\n\nOne can use the following codes to have a feel for how the weight and bias change the value in the neuron.\nlibrary(manipulate)\nmanipulate(\n  plot(x, 1/(1+exp(-(w*x+b))), type=\"l\", xlab=\"Input\", ylab=\"n1\"),\n  w = slider(-1e3,1e3, initial = 20), \n  b = slider(-1e3,1e3, initial = -5))\nNow let’s create another step function in which the change point occurs at 0.4\n\nx &lt;- seq(0, 1, 0.01)\nw1 &lt;- 1000\nb1 &lt;- -300\ny1 &lt;- 1/(1+exp(-(w1*x+b1)))\nplot(x, y1, type=\"l\")\n\nw2 &lt;- 1000\nb2 &lt;- -400\ny2 &lt;- 1 / (1+exp(-(w2*x+b2)))\nlines(x, y2, col=2)\n\n\n\n\nWhat happens in the Output node is \\(\\omega_{31} n1 + \\omega_{32} n2 + b\\). By setting \\(w1\\) and \\(w2\\) at different sign, we can create a bump.\n\nw31 &lt;- 1        \nw32 &lt;- -1       \nb3 &lt;- 0\nplot(x, y2*w32 + y1*w31 + b3, type=\"l\", xlab=\"x\", ylab=\"Output\")\n\n\n\n\nAgain, one may use the following codes to have a feel for how two weights and the bias change the output in the Output.\nmanipulate(\n  plot(x, y2*w32 + y1*w31 + b3, type=\"l\", xlab=\"x\", ylab=\"Output\"),\n  w31 = slider(-1e2,1e2, initial = 1), \n  w32 = slider(-1e2,1e2, initial = -1),\n  b3 = slider(-1e2,1e2, initial = 0))"
  },
  {
    "objectID": "blog/posts/learning_ChatGPT_1/index.html",
    "href": "blog/posts/learning_ChatGPT_1/index.html",
    "title": "Learning ChatGPT 1: Probabilities for the next word",
    "section": "",
    "text": "Inspired by the blog post by Stephen Wolfram about the workings of the GPT-2 system, I decided to learn a bit about ChatGPT myself. Luckily, GPT-2 is now available for R. My first task is simply to learn to run the model and generate the probability table for the words that can follow the text, “The best thing about AI is to be able to”. The following contents are mainly based on the blog post by Stephen Wolfram. Additional resources include OpenAI Github page for gpt-2 and the paper describing GPT-2 paper.\nLoad the libraries and R implementation of GPT-2.\n\nrequire(tok)\nrequire(torch)\nsource('GPT.R')\n\nCreate a tokenizer to process inputs.\n\ntok &lt;- tok::tokenizer$from_pretrained(\"gpt2\")\n\nLoad a GPT-2 model\n\ntorch::with_device(device = \"meta\",{\n  Model0 &lt;- GPT(\n    block_size = 1024,\n    n_embd = 768,\n    N_Layers = 12,\n    nvoc = 50257,\n    Head = 12\n  )\n})\n\nApply the model weights\n\nModel0$load_state_dict(state_dict = torch_load(\"Model-weights.pt\"),\n                       .refer_to_state_dict = TRUE)\n\nCreate a function to list up top_k words with their probabilities.\n\ntop_k_words &lt;- function(model = NULL,\n                             device = NULL,\n                             raw_text = NULL, \n                             temperature = NULL, \n                             top_k = 10){\n  \n  idx = tok$encode(raw_text)$ids\n  paste0(\"Input text is \", raw_text)\n  idx = torch::torch_tensor(idx+1, dtype=torch::torch_int(), device=device0)\n  idx = torch::torch_unsqueeze(idx, 1)\n  idx_cond = idx    \n  logits = model$eval()(idx_cond)\n  logits = logits[, min(idx$size(2),1024), ] / temperature\n  \n  logits = logits$topk(top_k)\n  probs = torch::nnf_softmax(logits[[1]],-1)\n  df &lt;- data.frame(token = NA,\n                   probability=as.numeric(probs))\n  \n  for (i in 1:top_k) {\n    idx_next &lt;- logits[[2]][,as.integer(i)]$unsqueeze(1)\n    token &lt;- tok$decode(as.integer(idx_next$cpu()-1))\n    df$token[i] &lt;- token\n  }\n  return(df)\n}\n\nHave the model run on GPU\n\nModel0 = if (torch::cuda_is_available()) Model0$cuda() else Model0$cpu()\ndevice0 = if (torch::cuda_is_available()) \"cuda\" else \"cpu\"\n\nGenerate the results\n\nset.seed(1) # for reproducibility, GPT-2 output is random.\ndf &lt;- top_k_words(model = Model0,\n                  device = device0,\n                  raw_text = \"The best thing about AI is its ability to\", \n                  temperature = 0.8,\n                  top_k = 5)\n\nnames(df) &lt;- c(\"Token\",\"Probability\")\nknitr::kable(df)\n\n\n\n\nToken\nProbability\n\n\n\n\nlearn\n0.2770396\n\n\npredict\n0.2047849\n\n\nmake\n0.1826125\n\n\nunderstand\n0.1745481\n\n\ndo\n0.1610149"
  },
  {
    "objectID": "blog/posts/julia_labelledarrays_namedtupletools/index.html",
    "href": "blog/posts/julia_labelledarrays_namedtupletools/index.html",
    "title": "LabelledArrays and NamedTupleTools make it easy to use the ODE model in Julia",
    "section": "",
    "text": "SEIR model using LabelledArrays\nThe LabelledArrays package makes it easy to use the ODE model in Julia. It offers a method to manage variables via keys instead of indices. Variables can be constructed using the @LArray macro or LVector.\nHowever, using arrays to store a mix of information types is not optimal for performance. I frequently need to store different variable types within a single parameter, which prompts a warning, highlighting that combining variable types in an array could reduce performance and suggesting tuples as a more efficient alternative.\n\n┌ Warning: Utilizing arrays or dictionaries to store parameters of diverse types can compromise performance. │ Using tuples is advised for better efficiency. └ @ SciMLBase C:.kim.julia_warnings.jl:32\n\nTuples, being essentially immutable, presents a challenge since some of model parameters that must be adjustible or estimated. The NamedTupleTools package offers a convenient solution with its merge function, enabling easy updates to the values within a Tuple.\nIn conclusion, the best approach seems to be leveraging LabelledArrays for state variables and NamedTupleTools for parameters. Let’s delve into this approach using the SEIR model as a case study.\n\nusing LabelledArrays, OrdinaryDiffEq, Plots, NamedTupleTools, BenchmarkTools\n\nfunction seir(du, u, p, t)\n    N = sum(u)\n    du.S = - p.β * u.S * u.I / N\n    du.E = + p.β * u.S * u.I / N - p.ϵ * u.E\n    du.I = + p.ϵ * u.E - p.γ* u.I\n    du.R = + p.γ* u.I \nend\n\nseir (generic function with 1 method)\n\n\nu0 = @LArray [0.99, 0.0, 0.01, 0.0] (:S, :E, :I, :R);\np_la = @LArray [0.5, 1/2, 1/4] (:β, :ϵ, :γ);\np_la = LVector(p_la, β=0.8, str=\"string\", int=9, la=LVector(a=1,b=2,c=3), tp=(a=1,b=2,c=3));\ntspan = (0.0, 100.0);\nprob = ODEProblem(seir, u0, tspan, p_la);\nsol = solve(prob, Tsit5(), saveat=1);\n\ns = [sol[i].S for i in 1:101];\ne = [sol[i].E for i in 1:101];\ni = [sol[i].I for i in 1:101];\nr = [sol[i].R for i in 1:101];\n\nplot(sol.t, [s e i r])\n\n\n\n\n# NamedTupleTools approach\np_nt = (β=0.5, ϵ=1/2, γ=1/4);\np_nt = merge(p_nt, (β=0.8, str=\"string\", int=9, la=LVector(a=1,b=2,c=3), tp=(a=1,b=2,c=3)));\n\nusing BenchmarkTools\ntime_la = @benchmark solve(ODEProblem(seir, u0, tspan, p_la), Tsit5(), saveat=1);\ntime_nt = @benchmark solve(ODEProblem(seir, u0, tspan, p_nt), Tsit5(), saveat=1);\ntime_la\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  180.800 μs … 345.339 ms  ┊ GC (min … max): 0.00% … 99.84%\n Time  (median):     234.550 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   400.856 μs ±   3.459 ms  ┊ GC (mean ± σ):  9.26% ±  3.45%\n\n  █▅▃▃▂▂▁ ▄▁ ▄▃▁ ▃▂▄▃▃▃▂▂▁▁▁▁▁                                  ▁\n  █████████████████████████████████▇▇▇▇▆▆▅▆▅▅▃▅▅▅▅▅▄▅▅▅▄▅▅▅▄▅▅▅ █\n  181 μs        Histogram: log(frequency) by time       1.35 ms &lt;\n\n Memory estimate: 107.62 KiB, allocs estimate: 5775.\n\ntime_nt\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):   59.800 μs … 273.123 ms  ┊ GC (min … max):  0.00% … 99.92%\n Time  (median):      91.100 μs               ┊ GC (median):     0.00%\n Time  (mean ± σ):   161.812 μs ±   2.731 ms  ┊ GC (mean ± σ):  16.86% ±  1.00%\n\n  █▇▆▆▆▄▂▁▂▂ ▃▅▆▄▄▂▃▃▃▂▂▃▂▁▁▂▂▂▁▁▁▁▁ ▁▁                         ▂\n  ████████████████████████████████████████▇██▇▆▆▇▆▆▆▄▄▅▆▆▆▅▄▃▅▅ █\n  59.8 μs       Histogram: log(frequency) by time        483 μs &lt;\n\n Memory estimate: 29.45 KiB, allocs estimate: 730."
  },
  {
    "objectID": "blog/posts/infectious_disease_names/index.html",
    "href": "blog/posts/infectious_disease_names/index.html",
    "title": "Origins of major human infectious diseases",
    "section": "",
    "text": "Major human infecious disease are believed to have arisen after agriculture revolutionOrigins of major human infectious diseases. By the way, this emergence of infectious diseases are one reason that agricultural revolution is callled one of the major mistakes of the human species by Diamond and\nby Bill Gates is a comprehensive and accessible guide on how to tackle the urgent issue of climate change. Gates begins by laying out the scope of the problem, explaining that the world needs to reduce its greenhouse gas emissions to zero to avoid a catastrophe. I’ve compiled important numbers from the book to understand the climate change issues.\n\n51 Billion Tonnes: This is the amount of greenhouse gases, measured in CO2 equivalent (\\(\\mathrm{CO_{2}e}\\)), that humanity adds to the atmosphere every year.\nZero: According to Bill Gates, we need to bring this number down to zero to avoid a climate disaster. And he thinks it is possible through our technological advances. Watch his TED talk, Innovating to zero!. He is a great speaker!\n\n\n\n\nPercentage\nItem\n\n\n\n\n27%\nHow we generate electricity\n\n\n31%\nHow we make things\n\n\n18%\nHow we grow our food\n\n\n16%\nHow we move around\n\n\n6%\nHow we keep warm or cool"
  },
  {
    "objectID": "blog/posts/Idiosyncrasy_generality/index.html",
    "href": "blog/posts/Idiosyncrasy_generality/index.html",
    "title": "Idiosyncrasies and generalities",
    "section": "",
    "text": "Idiosyncrasies and generalities\nDebates in the population ecology Bjørnstad and Grenfell.\n\nRelative importance of “noise” (small-scale, high-frequency stochastic influences) versus climatic forcing (larger scale, often lower-frequency signals) versus nonlinear interactions between individuals of the same or different species.\nThe impact of intrinsic (i.e., intraspecific) processes, as opposed to extrinsic or community-level interactions\nNested within 2, “dimensionality” of population fluctuations; given that most populations are embedded in rich communities and affected by numerous interspecific interactions, can simple (low-dimensional) models involving one or a few species capture the patterns of fluctuations?\n\n“… To understand any system, we need to appreciate its idiosyncrasies; to encompass broad patterns, we need to extract generalities…”\nViewing infectious disease epidemiology as a subset of ecology can be highly beneficial. By doing so, we can apply all the components that influence the dynamics of animal species to infectious disease dynamics, to varying degrees.\nIn the context of COVID-19, the modeling of confirmed cases has primarily focused on the variation in transmission rates. Often, the variation within reasonable limits in the transmission rate alone is sufficient to produce the daily number of cases. However, there is a potential risk of overlooking other influential factors, which could lead to biased estimates for transmission.\nBy considering infectious disease dynamics from an ecological perspective, we can broaden our understanding and ensure that relevant factors beyond transmission rates are taken into account. This approach can enhance the accuracy of our modeling and contribute to more effective public health interventions in combating diseases like COVID-19."
  },
  {
    "objectID": "blog/posts/how-to-write-a-paper/index.html",
    "href": "blog/posts/how-to-write-a-paper/index.html",
    "title": "Writing a paper: Start with an outline",
    "section": "",
    "text": "연구자의 업무 중에 연구 만큼 중요한 것이 글쓰기, 특히 논문 쓰기이다. 논문으로 쓰여지지 못한 연구는 타인에게는 존재하지 않는 것이나 다름 없는 것이다.. Writing a paper by George M. Whitesides 에 논문 쓰기에 유용한 팁이 있어 여기에 기록으로 남긴다. 한 마디로 요약하면 outline (개요)을 이용하는 것이다. 개요를 연구과제의 초기에 작성하여 연구의 계획표로 활용하며 공저자 (주로 제 1저자와 책임저자) 간에 논문에 대한 의견 교환시 개요를 사용하는 것이다. 그리고, 표, 수식, 그림 등이 거의 최종 상태에 가까워지면 개요를 바탕으로 논문 쓰기를 시작한다. 이렇게 하면 불필요한 실험 및 쓰기 등을 줄일 수 있다.\n지금까지 연구 과정을 돌아보니 프로젝트 초기에 논문의 개요를 작성하는 과정을 대체로 하긴 했었고 최종 논문 쓰기에 효과적임을 느끼고는 있었다. 그런데 개요를 이용하여 논문의 구조를 고민하고 그림이나 표를 최종 상태로 만든 후에 텍스트를 작성하는 일은 하지 않았던 것 같다. 대체로 개요, 그림, 그리고 표가 최종 상태가 되기 전에 텍스트를 작성하여, Whitesides 가 적었듯이 최종 논문에 사용되지 않은 텍스트가 많았던 것 같다. 개요를 이용하여 논문의 구조를 고민하고, 그림, 표를 최종본으로 만드는 과정에 노력을 기울이고 텍스트를 더하는 일은 그 이후에 하는 것이 더 효과적일 것 같다."
  },
  {
    "objectID": "blog/posts/generation_interval/index.html",
    "href": "blog/posts/generation_interval/index.html",
    "title": "Generation interval, growth rate, reproduction number",
    "section": "",
    "text": "Growth rate, generation interval, and reproduction number\nWallinga and Lipsitch wrote a highly cited paper about the reproduction number. It discusses how to derive reproduction number, \\(R\\), given the growth rate, \\(r\\), and the generation interval, \\(T_c\\).\n\\[\n\\begin{align}\nR &= 1 + r/b\\\\\nR &= (1 + r/b_1)(1 + r/b_2)\\\\\nR &= \\frac{(1 + r/b_1)^x}{\\sum_{i=1}^y(1 + r/b_2)^{-i}}\\\\\n\\end{align}\n\\]\nI can seem to use the third equation to reproduce the answer to the example the authors provided. On page 603, the authors gave the example of Influenza A where the generation interval has a mean of 2.85 days with the standard deviation of 0.93 days. The epidemic growth rate \\(r = 0.2\\). “The \\(R=1.57\\) for the SIR epidemic model, a value of \\(R=1.65\\) for the SEIR epidemic model and a value of \\(R=1.66\\) the more complicated epidemic model with one latent stage and two infectious stages (equation (3.3), with \\(x=1, y=2\\))”\nI tried to reproduce the results.\n\nTc = 2.85\nsigma = 0.93\nr = 0.2\n# for the SIR model\nb = 1/Tc\n(R = 1 + r/b)\n\n[1] 1.57\n\n# for the SEIR model, assume that b1 and b2 are the same\nb1 = b2 = 2/Tc\n(R = (1 + r/b1)*(1 + r/b2))\n\n[1] 1.651225\n\n\nFor the model with \\(x=1, y=2\\), the \\(R\\) is not the same as provided. \\(R\\) can vary depending on how we set \\(b_1\\) and \\(b_2\\), but it is smaller than one.\n\n# for the SEIR model with x=1, y=2 (i.e., two consecutive . assume that b1 and b2 are the same\nx = 1\ny = 2\nb1 = 2/Tc\nb2 = 4/Tc\n\nnumer = function(x) (1 + r/b1)^x\ndenom = function(y) sum(sapply(1:y, function(i) (1+r/b2)^(-i)))\n\n(R = numer(x=x)/denom(y=y))\n\n[1] 0.7828791\n\n\nThe study also refers to Wearing et al. (2005) study and the result is below. Again, it is not \\(R=1.66\\)\n\n# as presented in Wearing et al.(2005)\n\nm = 1\nn = 2\nb1 = 2/Tc\nb2 = 4/Tc\n\nnumer = r*(r/(b1*m)+1)^m\ndenom = b2*(1-(r/(b2*n)+1)^(-n))\n(R = numer / denom)\n\n[1] 1.423909\n\n\nI implemented moment generating function to see if I can reproduce the results this way. One important aspect is that how we should set the rate to get the generation interval we want.\n\nmgf = function(b, r) b / (b + r)\n\n1/mgf(b=1/Tc, r=r)\n\n[1] 1.57\n\nmgf_recursion = function(c, b, r){\n  l = length(b)\n  if (l == 1) {\n    c * mgf(b, r)\n  } \n  else {\n    c[l] * mgf(b[l], r) * mgf_recursion(c=rep(1,length(b[1:(l-1)])), b=b[1:(l-1)], r=r)\n  }\n}\n\nc = c(0,1/2,1/2)\nb = c(3,3,3)/Tc\n\nR0_recursion = function(c,b,r){\n  out &lt;- rep(NA, length(b))\n  for (i in seq_along(b)) {\n    out[i] = mgf_recursion(c[1:i], b[1:i], r)\n  }\n  1/sum(out)\n}\n\nR0_recursion(c=c(1), b=c(1)/Tc, r=0.2)\n\n[1] 1.57\n\nR0_recursion(c=c(0,1), b=c(2,2)/Tc, r=0.2)\n\n[1] 1.651225\n\n# for y=2 (i.e., Gamma distribution with shape=2)\n# use the relationship \n# average time to infection = beta*(1+alpha)/2 where beta and alpha represent scale and shape\n# beta*(1+alpha)/2 = Tc/2\n# rate = b = (1+alpha)/Tc\nR0_recursion(c=c(0,1/2,1/2), b=c(2,3,3)/Tc, r=0.2)\n\n[1] 1.661816"
  },
  {
    "objectID": "blog/posts/exp_erlang_lognormal/index.html",
    "href": "blog/posts/exp_erlang_lognormal/index.html",
    "title": "Implementing incubation period of cholera in the ODE model",
    "section": "",
    "text": "Modeling incubation period in the ODE model\nIn the realm of infectious disease modeling, accurately simulating the incubation period–the time between exposure to a pathogen and the onset of symptoms–is crucial for understanding and predicting the spread of diseases. Classic ordinary differential equation (ODE) models often employ the exponential distribution to represent this incubation period mostly for the sake of convenience. However, the exponential distribution is not an optimal distribution to model the waiting time for the incubation period, which is often modeled as a log-normal distribution as shown in the study by Lessler et al.. In this post, I show that waiting time may be modeled more realistically by adding an extra compartment (e.g., \\(SE_1E_2IR\\) rather than \\(SEIR\\)) and thus making the waiting time Erlang-distributed.\nIn particular, in the following R codes, I explored the incubation period of cholera from the study by Azman et al.. Azman writes: We estimate the median incubation period of toxigenic cholera to be 1.4 days (95% Credible Interval (CI), 1.3-1.6) with a dispersion of 1.98 (95% CI 1.87-2.11). Five percent of cholera cases will develop symptoms by 0.5 days (95% CI 0.4-0.5), and 95% by 4.4 days (95% CI 3.9-5.0) after infection.\nBased on the description, I set the parameters for the log-normal distribution as follows.\n\na = 1.40 # median\nb = 1.98 # disperson\nround(qlnorm(c(0.05,0.95), meanlog = log(a), sdlog = log(b)), digits=1)\n\n[1] 0.5 4.3\n\n\nThere is a slight mismatch for the 95th percentile (4.3 vs 4.4 [reported]). This is, however, possible considering that the estimates are from Bayesian posterior samples and I can’t account for the correlation between the parameter estimates.\nUsing this log-normal distribution, I generate samples (\\(n=1000\\)), to which exponential and Erlang distributions are fit.\n\ndat = rlnorm(1e3, meanlog = log(a), sdlog = log(b))\n\nfit_exp = fitdistrplus::fitdist(dat, distr=\"exp\")\nfit_gam = fitdistrplus::fitdist(dat, distr=\"gamma\")\n\nround(qexp(c(0.05, 0.95), rate=fit_exp$estimate[[1]]), digits=1)\n\n[1] 0.1 5.3\n\nround(qgamma(c(0.05, 0.95), shape=fit_gam$estimate[[1]], rate=fit_gam$estimate[[2]]), digits=1)\n\n[1] 0.4 4.0\n\n\nBy examining \\(5^{th}\\) and \\(95^{th}\\) percentiles, we realize that the best-fit exponential distribution is too wide compared with the log-normal distribution whereas the Gamma distribution looks better.\nHowever, in the context of ODE models, Gamma distribution is not easy to implement except for a subset in which the shape parameter is an integer (i.e., Erlang distribution). Therefore, I fit the rate parameter of the Gamma distribution while the shape parameter is set to an integer. Based on the previous fitting results (shape parameter = 2.348677), I only test the cases where the shape parameter is 2 or 3.\n\nerlang_nll = function(p, shp=3){\n  - sum(dgamma(dat, shape=shp, rate=p, log=TRUE))\n}\ner_shp2 = optimize(erlang_nll, c(1e-6,10), shp=2)\ner_shp3 = optimize(erlang_nll, c(1e-6,10), shp=3)\n\nConsidering the negative log likelihood values, we realize that the Gamma distribution with shape of 2 provides a better fit than the one with shape of 3.\nWe plot the results as a visual summary.\n\nn = 1000 # the number of\nx = seq(0, 12, length.out=n)\ndf = data.frame(\n  val = rep(x, 4),\n  dist = rep(c(\"Log-normal\", \"Exponential\", \"Gamma\", \"Erlang (shape=2)\"), \n             each=n), \n  density = \n    c(dlnorm(x, meanlog=log(1.4), sdlog=log(1.98)), \n      dexp(x, rate=fit_exp$estimate[[1]]), \n      dgamma(x, shape=fit_gam$estimate[[1]], \n                 rate=fit_gam$estimate[[2]]), \n      dgamma(x, shape=2, rate=er_shp2$minimum)))\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(df)+\n  geom_line(aes(x=val, y=density, color=dist))+\n  labs(y=\"Density\", x=\"\", color=\"\")+\n  theme(legend.position = \"bottom\")\n\n\n\n\nThe above figure clearly shows that the Erlang distribution has a better representation of reality (in this case, the log-normal distributed incubation period) than the commonly used exponential distribution."
  },
  {
    "objectID": "blog/posts/drawing-map/index.html",
    "href": "blog/posts/drawing-map/index.html",
    "title": "ggplot2로 지도 그리기",
    "section": "",
    "text": "ggplot2를 이용하여 지도 그리기를 해보자. 지도는 shapefile에 담겨져 있다고 가정하자. shapefile을 읽는 방법은 여러가지가 있을 수 있는데 sf 패키지의 st_read 혹은 read_sf 함수를 이용한 후 ggplot2의 geom_sf를 이용하여 그리는 것이 가장 쉬운 것 같다.\n\nlibrary(sf)\nkor &lt;- st_read(\"gadm41_KOR_1.shp\")\n\nReading layer `gadm41_KOR_1' from data source \n  `C:\\Users\\jonghoon.kim\\Documents\\myblog\\blog\\posts\\drawing-map\\gadm41_KOR_1.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 17 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 124.6097 ymin: 33.11236 xmax: 131.8715 ymax: 38.61177\nGeodetic CRS:  WGS 84\n\n# kor &lt;- read_sf(dsn=\"C:/Users/jonghoon.kim/Documents/myblog/posts/drawing-map\", layer=\"gadm41_KOR_1\")\nset.seed(42)\n# normalized number of characters of the name of the admin unit (level 1)\nchar_len &lt;- sapply(kor$NAME_1, nchar)\nkor$prop_char &lt;- char_len / max(char_len)\n\nlibrary(ggplot2)\nplt &lt;- ggplot(kor)+\n  geom_sf(aes(fill=prop_char))+\n  scale_fill_viridis_c(name=\"Normalized\\ncharacter length\", limits=c(0,1)) +\n  theme_minimal()+\n  theme(legend.position=\"right\")\n\nplt\n\n\n\n# use color brewer \nlibrary(RColorBrewer)\npal &lt;- brewer.pal(9,\"YlOrBr\")\nplt &lt;- plt + \n  scale_fill_gradientn(name=\"Normalized\\ncharacter length\", colors=pal, limits=c(0,1))\n  \nplt\n\n\n\n# Clear some background stuff\nplt &lt;- plt +\n  theme(panel.background = element_blank(), # bg of the panel\n        plot.background = element_blank(), # bg of the plot\n        legend.background = element_blank(), # get rid of legend bg\n        legend.box.background = element_blank(),\n        panel.spacing = unit(c(0,0,0,0), \"null\"),\n        plot.margin = unit(c(0,0,0,0), \"null\"),\n        axis.line = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        panel.border = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"bottom\",\n        plot.title = element_text(hjust=0.5, size=12))\nplt"
  },
  {
    "objectID": "blog/posts/diffeqr/index.html",
    "href": "blog/posts/diffeqr/index.html",
    "title": "diffeqr: R interface to the Julia’s DifferentialEquations.jl",
    "section": "",
    "text": "Julia DifferentialEquations.jl provides an impressive collection of differential equation solvers. The DE solvers available in the package are reliable and a lot faster than what’s avaiable in R. It’s now possible to access the solvers in R thanks to the diffeqr package. The following codes were adapted from the diffeqr GitHub page.\n\ndeSolve package\nI am using the SIR model as an example and for speed comparison, first solving equations using the deSolve package.\n\nsir_deSolve &lt;- function(t, u, p){\n  N &lt;- sum(u)\n  du1 &lt;- - p[1]*u[1]*u[2]/N\n  du2 &lt;- + p[1]*u[1]*u[2]/N - p[2]*u[2]\n  du3 &lt;- + p[2]*u[2]\n    \n  return(list(c(du1, du2, du3))) \n}\n\nLet’s solve the model using the deSolve::ode function.\n\nlibrary(deSolve)\nu0 &lt;- c(99, 1, 0)\ntspan &lt;- seq(from=0, to=50, by=1)\np &lt;- c(0.4, 0.2)\n\noutdf &lt;- as.data.frame(ode(y=u0, times=tspan, func=sir_deSolve, parms=p))\nsaveRDS(outdf, \"outdf.rds\")\n\n\noutdf &lt;- readRDS(\"outdf.rds\")\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(outdf,aes(x=time))+\n  geom_line(aes(y=`1`, color=\"S\")) +\n  geom_line(aes(y=`2`, color=\"I\")) +\n  geom_line(aes(y=`3`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n\n\n\ndiffeqr package\nNow let’s use the diffeqr package. Once the de &lt;- diffeqr::diffeq_setup is executed, the functions for DifferentialEquations.jl are available through de$. diffeqr has slightly different conventions for the ODE model.\n\nlibrary(diffeqr)\nde &lt;- diffeqr::diffeq_setup()\n\nsir &lt;- function(u, p, t){\n  N = sum(u)\n  du1 = - p[1]*u[1]*u[2]/N\n  du2 = + p[1]*u[1]*u[2]/N - p[2]*u[2]\n  du3 = + p[2]*u[2]\n    \n  return(c(du1,du2,du3))\n}\n\nu0 &lt;- c(100, 1, 0.0)\ntspan &lt;- c(0.0, 50.0)\np &lt;- c(0.4, 0.2)\nprob &lt;- de$ODEProblem(sir, u0, tspan, p)\nsol &lt;- de$solve(prob, de$Tsit5(), saveat=1)\n\nmat &lt;- sapply(sol$u,identity)\nudf &lt;- as.data.frame(t(mat))\ntudf &lt;- cbind(data.frame(t=sol$t), udf)\nsaveRDS(tudf, \"tudf.rds\")\n\n\ntudf &lt;- readRDS(\"tudf.rds\")\ntudflong = tidyr::pivot_longer(tudf, cols=2:4, \n                               names_to=\"var\", \n                               values_to=\"count\")\n\nggplot(tudf,aes(x=t))+\n  geom_line(aes(y=V1, color=\"S\")) +\n  geom_line(aes(y=V2, color=\"I\")) +\n  geom_line(aes(y=V3, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n\nThe ODE model can be sped up after compiling using the just-in-time (JIT) compiler.\n\nprob_jit &lt;- diffeqr::jitoptimize_ode(de, prob)\n# sol &lt;- de$solve(prob_jit, de$Tsit5(), saveat=1);\n\nThe ODE model can be sped up even further by running it on the GPU. The GitHub page is more geared toward the case in which runing the model multiple times over different initial conditions, which is called ensemble solve. This is why the term ensemble is used below. However, we use the same initial conditions as our sole purpose is to run the model multiple times and compare the elapsed time.\n\nprob_func &lt;- function (prob, i, rep){\n  de$remake(prob, u0=u0, p=p)\n}\nprob_ens &lt;- de$EnsembleProblem(prob_jit, prob_func=prob_func, safetycopy=FALSE)\n# sol &lt;- de$solve(prob_ens, de$Tsit5(), de$EnsembleSerial(), trajectories=1, saveat=1);\n# to take the full advantage we need the following.\ndegpu &lt;- diffeqr::diffeqgpu_setup(\"CUDA\")\n# de$solve(prob_ens, degpu$GPUTsit5(), degpu$EnsembleGPUKernel(degpu$CUDABackend()), trajectories=niter, saveat=1);\n\nI do not describe here but further performance enhancements are possible if your problem can make use of parallel computing.\n\nlibrary(microbenchmark)\nniter &lt;- 1e3\n\nbenchmark = microbenchmark(\n  deSolve = ode(y=u0, times=seq(0,50,by=1), func=sir_deSolve, parms=c(0.4,0.3)), \n  diffeqr = de$solve(prob, de$Tsit5(), saveat=1),\n  diffeqr_jit = de$solve(prob_jit, de$Tsit5(), saveat=1),\n  diffeqr_ens = de$solve(prob_ens, de$Tsit5(), de$EnsembleSerial(), trajectories=1, saveat=1),\n  times=niter\n)\n\nbenchmark\nsaveRDS(benchmark, \"benchmark.rds\")\n\n\nbenchmark &lt;- readRDS(\"benchmark.rds\")\nlibrary(dplyr)\nbenchmark |&gt; \n  group_by(expr) |&gt; \n  summarize(lower_sec = quantile(time/1000, probs=0.025),\n            median_sec = quantile(time/1000, probs=0.5),\n            upper_sec = quantile(time/1000, probs=0.975))\n\n# A tibble: 4 × 4\n  expr        lower_sec median_sec upper_sec\n  &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 deSolve         523.       577.       799.\n2 diffeqr         647.       689.       893.\n3 diffeqr_jit      77.9       99.4      166.\n4 diffeqr_ens     168.       214.       332.\n\n\n\n# GPU version requires a different framework to test the speed\nniter &lt;- 1000\ntelapsed_gpu &lt;- system.time(de$solve(prob_ens, degpu$GPUTsit5(), \n                                 degpu$EnsembleGPUKernel(degpu$CUDABackend()), \n                                 trajectories=niter, saveat=1))\nsaveRDS(telapsed_gpu, \"telapsed_gpu.rds\")\n\n\nbenchmark &lt;- readRDS(\"benchmark.rds\")\ntelapsed_gpu &lt;- readRDS(\"telapsed_gpu.rds\")\n\ndf &lt;- data.frame(subroutine=benchmark$expr, time=benchmark$time)\n\nggplot(df) + \n  geom_violin(aes(x=subroutine, y=time))+\n  scale_y_log10(limits=c(0.01, 1e8))+\n  geom_hline(aes(yintercept=telapsed_gpu[3]), color=\"firebrick\")+\n  labs(y=\"Elaspsed time\", x=\"Subroutine\")+\n  coord_flip()+\n  annotate(\"text\", y=telapsed_gpu[3]+5, \n           x=\"diffeqr\",\n           label=\"GPU diffeqr\", color=\"firebrick\")\n\n\n\n# ggsave(\"diffeqr_benchmark.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "blog/posts/crop-raster-polygon/index.html",
    "href": "blog/posts/crop-raster-polygon/index.html",
    "title": "Extract raster based on a polygon",
    "section": "",
    "text": "raster 이미지의 일부를 추출해보자. 특히, shapefile에 담겨져 있는 polygon에 해당하는 raster 를 추출해보자. raster 패키지의 crop 과 mask 함수를 이용할 수 있다.\n\n# Create some data using meuse \nlibrary(raster)\ndata(meuse)\ncoordinates(meuse) &lt;- ~x+y\nproj4string(meuse) &lt;- CRS(\"+init=epsg:28992\")\ndata(meuse.grid)\ncoordinates(meuse.grid) = ~x+y\nproj4string(meuse.grid) &lt;- CRS(\"+init=epsg:28992\")\ngridded(meuse.grid) = TRUE    \nr &lt;- raster(meuse.grid) \nr[] &lt;- runif(ncell(r))\n\n# Create a polygon\nf &lt;- rgeos::gBuffer(meuse[10,], byid=FALSE, id=NULL, width=250, \n                         joinStyle=\"ROUND\", quadsegs=10)   \n\n# Plot full raster and polygon                       \nplot(r)\nplot(f,add=T)\n\n\n\n# Crop using extent, rasterize polygon and finally, create poly-raster\n#          **** This is the code that you are after ****  \ncr &lt;- crop(r, extent(f), snap=\"out\")                    \nfr &lt;- rasterize(f, cr)   \nlr &lt;- mask(x=cr, mask=fr)\n\n# Plot results\nplot(lr)\nplot(f,add=T)"
  },
  {
    "objectID": "blog/posts/counterintuitive/index.html",
    "href": "blog/posts/counterintuitive/index.html",
    "title": "Counterintuitive effects in disease transmission dynamics",
    "section": "",
    "text": "\\(R_0\\) and the prevalence of infection\nAn article by Heesterbeek et al. provides a few examples on the counterintuitive behavior of a dynamical system of disease transmission. The first example was about the relationship between the intensity of transmission and the prevalence of infection, citing the work by Anderson et al.. One message from the study by Anderson et al. is that the effect of intervention should not be measured by the prevalence of infection. A similar message can be drawn from a simple relationship between \\(R_0\\) and the prevalence of infection. As shown in the figure below, reducing \\(R_0\\) from 15 to 10 may require a lot of efforts and its effect on prevalence of infection may be small.\n\nR0 &lt;- seq(1, 20, by=0.1)\n# R0*s = 1 is assumed, i.e., endemic steady state\nprev &lt;- 1 - 1/R0\n\ndf &lt;- data.frame(R0=R0, Prev=prev)\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(df,aes(x=R0))+\n  geom_line(aes(y=prev))+ \n  ggtitle(expression(\"Prevalence of infection vs.\" ~italic(R)[0]))+\n  scale_y_continuous(limits=c(0,1))+\n  labs(x=expression(italic(R)[0]), y=\"Prevalence\")\n\n\n\n# ggsave(\"R0_prevalence.png\", units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "blog/posts/climate-disaster/index.html",
    "href": "blog/posts/climate-disaster/index.html",
    "title": "Important figures from the book, How to avoid a climate diaster? by Bill Gates",
    "section": "",
    "text": "How to Avoid a Climate Disaster: The Solutions We Have and the Breakthroughs We Need by Bill Gates is a comprehensive and accessible guide on how to tackle the urgent issue of climate change. Gates begins by laying out the scope of the problem, explaining that the world needs to reduce its greenhouse gas emissions to zero to avoid a catastrophe. I’ve compiled important numbers from the book to understand the climate change issues.\n\n51 Billion Tonnes: This is the amount of greenhouse gases, measured in CO2 equivalent (\\(\\mathrm{CO_{2}e}\\)), that humanity adds to the atmosphere every year.\nZero: According to Bill Gates, we need to bring this number down to zero to avoid a climate disaster. And he thinks it is possible through our technological advances. Watch his TED talk, Innovating to zero!. He is a great speaker!\n\n\n\n\nPercentage\nItem\n\n\n\n\n27%\nHow we generate electricity\n\n\n31%\nHow we make things\n\n\n18%\nHow we grow our food\n\n\n16%\nHow we move around\n\n\n6%\nHow we keep warm or cool"
  },
  {
    "objectID": "blog/posts/cholera_seir_eqn/index.html",
    "href": "blog/posts/cholera_seir_eqn/index.html",
    "title": "Solutions for the steady states of the SEIR model",
    "section": "",
    "text": "In this post, I compared the numerical solutions obtained using deSolve::ode with the algebraic solutions derived from Mathematica for the steady states of a SEIR model. The model considers the waning of natural immunity over time in a dynamically changing population.\n\nSEIR model with waning immunity and vital dynamics\n\ndS/dt = \\mu (S+E+I+R) - \\beta SI - \\mu S + \\omega R\\\\\ndE/dt = \\beta SI - (\\mu+\\kappa)E\\\\\ndI/dt = \\kappa E - (\\mu+\\gamma)I\\\\\ndR/dt = \\gamma I - (\\mu+\\omega) R\n Following Mathematica command produces the solutions for the steady states.\n\nFullSimplify[\n Solve[{\\[Mu]*(S + E1 + I1 + R) - (\\[Beta]*I1 + \\[Mu])*S + \\[Omega]*\n      R == 0, \\[Beta]*I1*S - (\\[Mu] + \\[Kappa])*E1 == \n    0, \\[Kappa]*E1 - (\\[Mu] + \\[Gamma])*I1 == \n    0, \\[Gamma]*I1 - (\\[Omega] + \\[Mu])*R == 0, \n   S + E1 + I1 + R == 1, \\[Mu] &gt; 0, \\[Beta] &gt; 0, \\[Kappa] &gt; \n    0, \\[Gamma] &gt; 0, \\[Omega] &gt; 0, S &gt; 0, E1 &gt; 0, I1 &gt; 0, R &gt; 0, \n   S \\[Element] Reals, E1 \\[Element] Reals, I1 \\[Element] Reals, \n   R \\[Element] Reals}, {S, E1, I1, R}]]\n\ns = \\frac{(\\gamma +\\mu ) (\\kappa +\\mu )}{\\beta  \\kappa } \\\\\ne = -\\frac{(\\gamma +\\mu ) (\\mu +\\omega ) ((\\gamma +\\mu ) (\\kappa +\\mu )-\\beta  \\kappa )}{\\beta  \\kappa  (\\gamma  (\\kappa +\\mu +\\omega )+(\\kappa +\\mu ) (\\mu +\\omega ))} \\\\\ni = \\frac{(\\mu +\\omega ) (\\beta  \\kappa -(\\gamma +\\mu ) (\\kappa +\\mu ))}{\\beta  \\omega  (\\gamma +\\kappa +\\mu )+\\beta  (\\gamma +\\mu ) (\\kappa +\\mu )} \\\\\nr = \\frac{\\beta  \\gamma  \\kappa -\\gamma  (\\gamma +\\mu ) (\\kappa +\\mu )}{\\beta  \\omega  (\\gamma +\\kappa +\\mu )+\\beta  (\\gamma +\\mu ) (\\kappa +\\mu )}\n I used latex2r package to convert the LaTex expressions from the Mathematica to R codes\n\nlibrary(latex2r)\ns &lt;- latex2r(\"\\\\frac{(\\\\gamma +\\\\mu ) (\\\\kappa +\\\\mu )}{\\\\beta  \\\\kappa }\")\ne &lt;- latex2r(\"-\\\\frac{(\\\\gamma +\\\\mu ) (\\\\mu +\\\\omega ) ((\\\\gamma +\\\\mu ) (\\\\kappa +\\\\mu )-\\\\beta  \\\\kappa )}{\\\\beta  \\\\kappa  (\\\\gamma  (\\\\kappa +\\\\mu +\\\\omega )+(\\\\kappa +\\\\mu ) (\\\\mu +\\\\omega ))}\")\ni &lt;- latex2r(\"\\\\frac{(\\\\mu +\\\\omega ) (\\\\beta  \\\\kappa -(\\\\gamma +\\\\mu ) (\\\\kappa +\\\\mu ))}{\\\\beta  \\\\omega  (\\\\gamma +\\\\kappa +\\\\mu )+\\\\beta  (\\\\gamma +\\\\mu ) (\\\\kappa +\\\\mu )}\")\nr &lt;- latex2r(\"\\\\frac{\\\\beta  \\\\gamma  \\\\kappa -\\\\gamma  (\\\\gamma +\\\\mu ) (\\\\kappa +\\\\mu )}{\\\\beta  \\\\omega  (\\\\gamma +\\\\kappa +\\\\mu )+\\\\beta  (\\\\gamma +\\\\mu ) (\\\\kappa +\\\\mu )}\")\n\nkappa &lt;- 1.115044 / 2 # mean late period is still around 1.4 days, \ngamma &lt;- 1/2\nmu &lt;- 1/(65*365)\nomega &lt;- 1/(4*365)\nbeta &lt;- 0.6\n\nstates &lt;- list(s=s, e=e, i=i, r=r)\nval = sapply(states, function(x) eval(parse(text = x)))\nval[[\"e\"]]/(val[[\"i\"]]+val[[\"e\"]])\n\n[1] 0.4728244\n\ngamma/(gamma+kappa)\n\n[1] 0.4728034\n\n(gamma+mu)/(gamma+kappa+mu)\n\n[1] 0.4728244\n\n(1/kappa)/(1/kappa + 1/gamma) \n\n[1] 0.4728034\n\n\nNumerical solutions based on deSolve::ode.\n\nseir_vital &lt;- function(t, u, p){\n  N &lt;- sum(u)\n  beta &lt;- p[\"beta\"]\n  kappa &lt;- p[\"kappa\"]\n  gamma &lt;- p[\"gamma\"]\n  omega &lt;- p[\"omega\"]\n  mu &lt;- p[\"mu\"]\n  \n  du1 &lt;- - beta*u[1]*u[3] + omega*u[4] - mu*u[1] + mu*N\n  du2 &lt;- + beta*u[1]*u[3] - kappa*u[2] - mu*u[2]\n  du3 &lt;- + kappa*u[2] - gamma*u[3] - mu*u[3]\n  du4 &lt;- + gamma*u[3] - omega*u[4] - mu*u[4]\n  \n  return(list(c(du1,du2,du3,du4))) \n}\n\nlibrary(deSolve)\nu0 &lt;- c(0.99, 0, 0.01, 0)\np &lt;- c(kappa=1.115044/2, gamma=1/2, mu=1/(65*365), \n      omega=1/(4*365), beta=0.6)\ntspan &lt;- seq(from=0, to=300*100)\noutdf &lt;- as.data.frame(ode(y=u0, times=tspan, func=seir_vital, parms=p))\nnames(outdf) &lt;- c(\"day\",\"s\",\"e\",\"i\",\"r\")\ntail(outdf, 1)\n\n        day         s            e            i         r\n30001 30000 0.8334656 0.0002166031 0.0002415018 0.1660763\n\n\n\n\nSE_1E_2I_1I_2R_1R_2 model\nThis model incorporates two consecutive compartments for the Exposed (E), Infectious (I), and Recovered (R) states, effectively changing the distribution of waiting times in each states from an exponential to an Erlang distribution with a shape parameter of 2, offering a more realistic representation.\n\ndS/dt = \\mu (S+E_1+I_1+R_1+E_2+I_2+R_2) - \\beta S(I_1+I_2) - \\mu S + 2 \\omega R_2\\\\\ndE_1/dt = \\beta S(I_1+I_2) - (\\mu+2\\kappa)E1\\\\\ndE_2/dt = 2\\kappa E_1 - (\\mu+2\\kappa)E2\\\\\ndI_1/dt = 2 \\kappa E_2 - (\\mu+2\\gamma)I_1\\\\\ndI_2/dt = 2\\gamma I_1 - (\\mu+2\\gamma)I_2\\\\\ndR_1/dt = 2 \\gamma I_2 - (\\mu+2\\omega) R_1\\\\\ndR_2/dt = 2 \\omega R_1 - (\\mu+2\\omega) R_2\n\nThe following Mathematica command produces the solutions for the steady states.\n\nFullSimplify[\n Solve[{\\[Mu]*(S + E1 + E2 + I1 + I2 + R1 + \n        R2) - (\\[Beta]*(I1 + I2) + \\[Mu])*S + 2*\\[Omega]*R2 == \n    0, \\[Beta]*(I1 + I2)*S - (\\[Mu] + 2*\\[Kappa])*E1 == 0, \n   2*\\[Kappa]*E1 - (2*\\[Kappa] + \\[Mu])*E2 == 0, \n   2*\\[Kappa]*E2 - (\\[Mu] + 2*\\[Gamma])*I1 == 0, \n   2*\\[Gamma]*I1 - (2*\\[Gamma] + \\[Mu])*I2 == 0, \n   2*\\[Gamma]*I2 - (2*\\[Omega] + \\[Mu])*R1 == 0, \n   2*\\[Omega]*R1 - (\\[Mu] + 2*\\[Omega])*R2 == 0, \n   S + E1 + E2 + I1 + I2 + R1 + R2 == 1, \\[Mu] &gt; 0, \\[Beta] &gt; \n    0, \\[Kappa] &gt; 0, \\[Gamma] &gt; 0, \\[Omega] &gt; 0, S &gt; 0, E1 &gt; 0, \n   E2 &gt; 0, I1 &gt; 0, I2 &gt; 0, R1 &gt; 0, R2 &gt; 0, S \\[Element] Reals, \n   E1 \\[Element] Reals, E2 \\[Element] Reals, I1 \\[Element] Reals, \n   I2 \\[Element] Reals, R1 \\[Element] Reals, R2 \\[Element] Reals}, {S,\n    E1, E2, I1, I2, R1, R2}]]\n\n\ns= \\frac{(2 \\gamma +\\mu )^2 (2 \\kappa +\\mu )^2}{4 \\beta  \\kappa ^2 (4 \\gamma +\\mu )}\\\\\n \ne_1 = - \\frac{(2 \\gamma +\\mu )^2 (2 \\kappa +\\mu ) (\\mu +2 \\omega )^2 \\left((2 \\gamma +\\mu )^2 (2 \\kappa +\\mu )^2-4 \\beta  \\kappa ^2 (4 \\gamma +\\mu )\\right)}{4 \\beta  \\kappa ^2 (4 \\gamma +\\mu ) \\left(4 \\gamma ^2 (2 \\kappa +\\mu +2 \\omega ) (2 \\kappa  (\\mu +4 \\omega )+\\mu  (\\mu +2 \\omega ))+4 \\gamma  (2 \\kappa +\\mu )^2 (\\mu +2 \\omega )^2+\\mu  (2 \\kappa +\\mu )^2 (\\mu +2 \\omega )^2\\right)}\\\\\n \ne_2 = -\\frac{(2 \\gamma +\\mu )^2 (\\mu +2 \\omega )^2 \\left((2 \\gamma +\\mu )^2 (2 \\kappa +\\mu )^2-4 \\beta  \\kappa ^2 (4 \\gamma +\\mu )\\right)}{2 \\beta  \\kappa  (4 \\gamma +\\mu ) \\left(4 \\gamma ^2 (2 \\kappa +\\mu +2 \\omega ) (2 \\kappa  (\\mu +4 \\omega )+\\mu  (\\mu +2 \\omega ))+4 \\gamma  (2 \\kappa +\\mu )^2 (\\mu +2 \\omega )^2+\\mu  (2 \\kappa +\\mu )^2 (\\mu +2 \\omega )^2\\right)}\\\\\n \ni_1 = - \\frac{(2 \\gamma +\\mu ) (\\mu +2 \\omega )^2 \\left((2 \\gamma +\\mu )^2 (2 \\kappa +\\mu )^2-4 \\beta  \\kappa ^2 (4 \\gamma +\\mu )\\right)}{\\beta  (4 \\gamma +\\mu ) \\left(4 \\gamma ^2 (2 \\kappa +\\mu +2 \\omega ) (2 \\kappa  (\\mu +4 \\omega )+\\mu  (\\mu +2 \\omega ))+4 \\gamma  (2 \\kappa +\\mu )^2 (\\mu +2 \\omega )^2+\\mu  (2 \\kappa +\\mu )^2 (\\mu +2 \\omega )^2\\right)}\\\\\n \ni_2 = -\\frac{2 \\gamma  (\\mu +2 \\omega )^2 \\left(4 \\beta  \\kappa ^2 (4 \\gamma +\\mu )-(2 \\gamma +\\mu )^2 (2 \\kappa +\\mu )^2\\right)}{\\beta  (4 \\gamma +\\mu ) \\left(4 \\gamma ^2 (2 \\kappa +\\mu +2 \\omega ) (2 \\kappa  (\\mu +4 \\omega )+\\mu  (\\mu +2 \\omega ))+4 \\gamma  (2 \\kappa +\\mu )^2 (\\mu +2 \\omega )^2+\\mu  (2 \\kappa +\\mu )^2 (\\mu +2 \\omega )^2\\right)}\\\\\n \nr_1 = -\\frac{4 \\gamma ^2 (\\mu +2 \\omega ) \\left((2 \\gamma +\\mu )^2 (2 \\kappa +\\mu )^2-4 \\beta  \\kappa ^2 (4 \\gamma +\\mu )\\right)}{\\beta  (4 \\gamma +\\mu ) \\left(4 \\gamma ^2 (2 \\kappa +\\mu +2 \\omega ) (2 \\kappa  (\\mu +4 \\omega )+\\mu  (\\mu +2 \\omega ))+4 \\gamma  (2 \\kappa +\\mu )^2 (\\mu +2 \\omega )^2+\\mu  (2 \\kappa +\\mu )^2 (\\mu +2 \\omega )^2\\right)}\\\\\n \nr_2 = -\\frac{8 \\gamma ^2 \\omega  \\left((2 \\gamma +\\mu )^2 (2 \\kappa +\\mu )^2-4 \\beta  \\kappa ^2 (4 \\gamma +\\mu )\\right)}{\\beta  (4 \\gamma +\\mu ) \\left(4 \\gamma ^2 (2 \\kappa +\\mu +2 \\omega ) (2 \\kappa  (\\mu +4 \\omega )+\\mu  (\\mu +2 \\omega ))+4 \\gamma  (2 \\kappa +\\mu )^2 (\\mu +2 \\omega )^2+\\mu  (2 \\kappa +\\mu )^2 (\\mu +2 \\omega )^2\\right)}\n\nAgain, LaTex expressions were converted to R codes using the latex2r package.\n\ns &lt;- latex2r(\"\\\\frac{(2 \\\\gamma +\\\\mu )^2 (2 \\\\kappa +\\\\mu )^2}{4 \\\\beta  \\\\kappa ^2 (4 \\\\gamma +\\\\mu )}\")\ne1 &lt;- latex2r(\"-\\\\frac{(2 \\\\gamma +\\\\mu )^2 (2 \\\\kappa +\\\\mu ) (\\\\mu +2 \\\\omega )^2 \\\\left((2 \\\\gamma +\\\\mu )^2 (2 \\\\kappa +\\\\mu )^2-4 \\\\beta  \\\\kappa ^2 (4 \\\\gamma +\\\\mu )\\\\right)}{4 \\\\beta  \\\\kappa ^2 (4 \\\\gamma +\\\\mu ) \\\\left(4 \\\\gamma ^2 (2 \\\\kappa +\\\\mu +2 \\\\omega ) (2 \\\\kappa  (\\\\mu +4 \\\\omega )+\\\\mu  (\\\\mu +2 \\\\omega ))+4 \\\\gamma  (2 \\\\kappa +\\\\mu )^2 (\\\\mu +2 \\\\omega )^2+\\\\mu  (2 \\\\kappa +\\\\mu )^2 (\\\\mu +2 \\\\omega )^2\\\\right)}\")\n\ne2 &lt;- latex2r(\"-\\\\frac{(2 \\\\gamma +\\\\mu )^2 (\\\\mu +2 \\\\omega )^2 \\\\left((2 \\\\gamma +\\\\mu )^2 (2 \\\\kappa +\\\\mu )^2-4 \\\\beta  \\\\kappa ^2 (4 \\\\gamma +\\\\mu )\\\\right)}{2 \\\\beta  \\\\kappa  (4 \\\\gamma +\\\\mu ) \\\\left(4 \\\\gamma ^2 (2 \\\\kappa +\\\\mu +2 \\\\omega ) (2 \\\\kappa  (\\\\mu +4 \\\\omega )+\\\\mu  (\\\\mu +2 \\\\omega ))+4 \\\\gamma  (2 \\\\kappa +\\\\mu )^2 (\\\\mu +2 \\\\omega )^2+\\\\mu  (2 \\\\kappa +\\\\mu )^2 (\\\\mu +2 \\\\omega )^2\\\\right)}\")\n\ni1 &lt;- latex2r(\"-\\\\frac{(2 \\\\gamma +\\\\mu ) (\\\\mu +2 \\\\omega )^2 \\\\left((2 \\\\gamma +\\\\mu )^2 (2 \\\\kappa +\\\\mu )^2-4 \\\\beta  \\\\kappa ^2 (4 \\\\gamma +\\\\mu )\\\\right)}{\\\\beta  (4 \\\\gamma +\\\\mu ) \\\\left(4 \\\\gamma ^2 (2 \\\\kappa +\\\\mu +2 \\\\omega ) (2 \\\\kappa  (\\\\mu +4 \\\\omega )+\\\\mu  (\\\\mu +2 \\\\omega ))+4 \\\\gamma  (2 \\\\kappa +\\\\mu )^2 (\\\\mu +2 \\\\omega )^2+\\\\mu  (2 \\\\kappa +\\\\mu )^2 (\\\\mu +2 \\\\omega )^2\\\\right)}\")\n\ni2 &lt;- latex2r(\"\\\\frac{2 \\\\gamma  (\\\\mu +2 \\\\omega )^2 \\\\left(4 \\\\beta  \\\\kappa ^2 (4 \\\\gamma +\\\\mu )-(2 \\\\gamma +\\\\mu )^2 (2 \\\\kappa +\\\\mu )^2\\\\right)}{\\\\beta  (4 \\\\gamma +\\\\mu ) \\\\left(4 \\\\gamma ^2 (2 \\\\kappa +\\\\mu +2 \\\\omega ) (2 \\\\kappa  (\\\\mu +4 \\\\omega )+\\\\mu  (\\\\mu +2 \\\\omega ))+4 \\\\gamma  (2 \\\\kappa +\\\\mu )^2 (\\\\mu +2 \\\\omega )^2+\\\\mu  (2 \\\\kappa +\\\\mu )^2 (\\\\mu +2 \\\\omega )^2\\\\right)}\")\n\nr1 &lt;- latex2r(\"-\\\\frac{4 \\\\gamma ^2 (\\\\mu +2 \\\\omega ) \\\\left((2 \\\\gamma +\\\\mu )^2 (2 \\\\kappa +\\\\mu )^2-4 \\\\beta  \\\\kappa ^2 (4 \\\\gamma +\\\\mu )\\\\right)}{\\\\beta  (4 \\\\gamma +\\\\mu ) \\\\left(4 \\\\gamma ^2 (2 \\\\kappa +\\\\mu +2 \\\\omega ) (2 \\\\kappa  (\\\\mu +4 \\\\omega )+\\\\mu  (\\\\mu +2 \\\\omega ))+4 \\\\gamma  (2 \\\\kappa +\\\\mu )^2 (\\\\mu +2 \\\\omega )^2+\\\\mu  (2 \\\\kappa +\\\\mu )^2 (\\\\mu +2 \\\\omega )^2\\\\right)}\")\n\nr2 &lt;- latex2r(\"-\\\\frac{8 \\\\gamma ^2 \\\\omega  \\\\left((2 \\\\gamma +\\\\mu )^2 (2 \\\\kappa +\\\\mu )^2-4 \\\\beta  \\\\kappa ^2 (4 \\\\gamma +\\\\mu )\\\\right)}{\\\\beta  (4 \\\\gamma +\\\\mu ) \\\\left(4 \\\\gamma ^2 (2 \\\\kappa +\\\\mu +2 \\\\omega ) (2 \\\\kappa  (\\\\mu +4 \\\\omega )+\\\\mu  (\\\\mu +2 \\\\omega ))+4 \\\\gamma  (2 \\\\kappa +\\\\mu )^2 (\\\\mu +2 \\\\omega )^2+\\\\mu  (2 \\\\kappa +\\\\mu )^2 (\\\\mu +2 \\\\omega )^2\\\\right)}\")\n\nkappa &lt;- 1.115044 / 2 # mean late period is still around 1.4 days, \ngamma &lt;- 1/2 \nmu &lt;- 1/(65*365)\nomega &lt;- 1/(4*365)\nbeta &lt;- 0.6\n\nstates &lt;- list(s=s, e1=e1, e2=e2, i1=i1, i2=i2, r1=r1, r2=r2)\nsapply(states, function(x) eval(parse(text = x)))\n\n           s           e1           e2           i1           i2           r1 \n0.8334490274 0.0001067747 0.0001067707 0.0001190490 0.0001190440 0.0843079955 \n          r2 \n0.0817913389 \n\n\nNumerical solutions are based on deSolve::ode.\n\nseir_2stg_vital &lt;- function(t, u, p){\n  N &lt;- sum(u)\n  beta &lt;- p[\"beta\"]\n  kappa &lt;- p[\"kappa\"]\n  gamma &lt;- p[\"gamma\"]\n  omega &lt;- p[\"omega\"]\n  mu &lt;- p[\"mu\"]\n  \n  du1 &lt;- - beta*u[1]*(u[4]+u[5]) + 2*omega*u[7] - mu*u[1] + mu*N\n  du2 &lt;- + beta*u[1]*(u[4]+u[5]) - 2*kappa*u[2] - mu*u[2]\n  du3 &lt;- + 2*kappa*u[2] - 2*kappa*u[3] - mu*u[3]\n  du4 &lt;- + 2*kappa*u[3] - 2*gamma*u[4] - mu*u[4]\n  du5 &lt;- + 2*gamma*u[4] - 2*gamma*u[5] - mu*u[5]\n  du6 &lt;- + 2*gamma*u[5] - 2*omega*u[6] - mu*u[6]\n  du7 &lt;- + 2*omega*u[6] - 2*omega*u[7] - mu*u[7]\n  \n  return(list(c(du1,du2,du3,du4,du5,du6,du7))) \n}\n\nu0 &lt;- c(0.99, 0, 0, 0.01, 0, 0, 0)\np &lt;- c(kappa=1.115044/2, gamma=1/2, mu=1/(65*365), \n      omega=1/(4*365), beta=0.6)\ntspan &lt;- seq(from=0, to=365*300)\noutdf &lt;- as.data.frame(ode(y=u0, times=tspan, func=seir_2stg_vital, parms=p))\nnames(outdf) &lt;- c(\"day\",\"s\",\"e1\",\"e2\",\"i1\",\"i2\",\"r1\",\"r2\")\ntail(outdf, 1)\n\n          day         s           e1           e2           i1           i2\n109501 109500 0.8334488 0.0001067726 0.0001067686 0.0001190467 0.0001190417\n               r1         r2\n109501 0.08430817 0.08179142"
  },
  {
    "objectID": "blog/posts/branching_process2/index.html",
    "href": "blog/posts/branching_process2/index.html",
    "title": "Branching process model 2",
    "section": "",
    "text": "Branching process model\nIn the branching process model, the number of secondary infections is realized as a random number (e.g., Poission or Negative binomial).\n\nR0_mean &lt;- 2\npopsize = 1000 # population size for each iteration\nnrun &lt;- 1000 # number of iterations to compute the mean\noutbreaks &lt;- vector(\"list\", nrun) # to store outbreak size for each iteration\ninit_inf &lt;- 1 # initially infected people\n\nfor (r in 1:nrun) {\n  pop &lt;- data.frame(id=1:popsize)\n  pop$status &lt;- \"S\"\n  pop$status[1:init_inf] &lt;- \"I\"\n  pop$run_id &lt;- r\n  pop$time_inf &lt;- NA\n  pop$time_inf[1:init_inf] &lt;- 0\n  nS &lt;- sum(pop$status == \"S\")\n  nI &lt;- sum(pop$status == \"I\")\n  N &lt;- nrow(pop)\n  cnt &lt;- init_inf + 1 # infecteds are placed from the first position\n  while (nI &gt; 0 & nS &gt; 0) {\n    row &lt;- which(pop$status == \"I\")\n    nI &lt;- length(row)\n    for (i in seq_len(nI)) {\n      # cat(\"i =\", i, \"\\n\")\n      pop$status[row[i]] &lt;- \"R\"\n      offspring &lt;- rpois(1, lambda=R0_mean*nS/N)\n      nS = nS - offspring\n      for (k in seq_len(offspring)) {\n        pop$status[cnt] &lt;- \"I\"\n        pop$time_inf[cnt] &lt;- pop$time_inf[row[i]] + \n          rgamma(1, shape=2, rate=1/3)\n        cnt &lt;- cnt + 1\n      }\n    }\n  }\n  outbreaks[[r]] &lt;- pop\n}\n\noutbreak_size &lt;- sapply(outbreaks, function(x) nrow(x) - sum(x$status==\"S\"))\nhist(outbreak_size) # minor and major outbreaks\nmajor_outbreaks = outbreak_size &gt; 200\nmean(outbreak_size[major_outbreaks])/popsize # only major outbreaks\n\n[1] 0.7974659\n\noutbks = purrr::list_rbind(outbreaks)\nlibrary(tidyverse)\n\n\n\noutbks[major_outbreaks,] |&gt; \n  filter(!is.na(time_inf)) |&gt; \n  ggplot()+\n    geom_histogram(aes(x=time_inf))"
  },
  {
    "objectID": "blog/posts/area-polygon/index.html",
    "href": "blog/posts/area-polygon/index.html",
    "title": "Polygon 면적 구하기: sf 와 raster 패키지",
    "section": "",
    "text": "shapefile에 담겨져 있는 polygon의 면적을 구해보자 raster 패키지의 area 혹은 sf 패키지의 st_area 함수를 이용할 수 있다.\n\nlibrary(sf)\nkor &lt;- st_read(\"gadm41_KOR_1.shp\")\n\nReading layer `gadm41_KOR_1' from data source \n  `C:\\Users\\jonghoon.kim\\Documents\\myblog\\blog\\posts\\area-polygon\\gadm41_KOR_1.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 17 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 124.6097 ymin: 33.11236 xmax: 131.8715 ymax: 38.61177\nGeodetic CRS:  WGS 84\n\n# another way\n# kor &lt;- read_sf(dsn=\"C:/Users/jonghoon.kim/Documents/myblog/posts/area-polygon\", layer=\"gadm41_KOR_1\")\nset.seed(42)\n# 1e6 to get population density per 1 km^2\nkor$area_sqkm1 &lt;- as.numeric(st_area(kor))/1e6\n\n# raster package way\n# library(raster)\n# kor &lt;- shapefile(\"C:/Users/jonghoon.kim/Documents/myblog/posts/area-polygon/gadm41_KOR_1.shp\")\n# raster pkg works for the Spatial* object\n# kor$area_sqkm2 &lt;- raster::area(as(kor, 'Spatial'))/1e6\n\n# plot population density\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nextrafont::loadfonts()\n\nlabels = expression(atop(\"Population density\", per~1~km^2))\nplt &lt;- ggplot(kor)+\n  geom_sf(aes(fill=area_sqkm1))+\n  scale_fill_viridis_c(name=labels) +\n  theme(legend.position=\"right\")\n\n# ggsave(\"southkorea_map.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)  \nplt\n\n\n\n# use RColorBrewer \nlibrary(RColorBrewer)\npal &lt;- brewer.pal(9,\"YlOrBr\")\nplt + scale_fill_gradientn(name=labels, colors=pal)"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Disease modeling for public health",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nThe Ross-Macdonald Framework: Foundational Mathematical Models in Vector-Borne Disease Epidemiology\n\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2025\n\n\n10 min\n\n\n\n\n\n\n\n\nThe Birth of Epidemic Modeling: Daniel Bernoulli and the Smallpox Inoculation Controversy\n\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2025\n\n\n7 min\n\n\n\n\n\n\n\n\nRegularization in statistical models\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2025\n\n\n4 min\n\n\n\n\n\n\n\n\nUnderstanding correlation, covariation, and least squares regression\n\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2025\n\n\n3 min\n\n\n\n\n\n\n\n\nDistribution Matching Methods: From Theory to Practice\n\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\nBayesian workflow: fake social contact data example\n\n\n\n\n\n\n\nBayesian workflow\n\n\nnegative binomial\n\n\nsocial contact\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2024\n\n\n12 min\n\n\n\n\n\n\n\n\nModeling human behavior: Infectious disease transmission modeling perspective\n\n\n\n\n\n\n\nBayesian workflow\n\n\nnegative binomial\n\n\nsocial contact\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\nKalman filter to estimate R using the FKF package\n\n\n\n\n\n\n\nkalman filter\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\nLearning ChatGPT 4: Universal Approximation Theorem\n\n\n\n\n\n\n\nUniversal Approximation Theorem\n\n\nNeural network\n\n\nArbitrary-Width\n\n\nArbitrary-Depth\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\nParallel simulation in R\n\n\n\n\n\n\n\nparallelism\n\n\nforeach\n\n\ndoParallel\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\nBasic reproduction number: An algorithmic approach\n\n\n\n\n\n\n\nBasic reproduction number\n\n\nMathematica\n\n\nalgorithmic approach\n\n\nnext generation matrix\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\nClaude 3\n\n\n\n\n\n\n\nLLM\n\n\nClaude\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\nLearning ChatGPT 2: Approximating a tower function using a neural net\n\n\n\n\n\n\n\nNeural network\n\n\nAdam\n\n\nmatrix\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\nA very basic implementation of a neural network\n\n\n\n\n\n\n\nGPT-2\n\n\nChatGPT\n\n\nWolfram\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\nVariables selection in statistical models\n\n\n\n\n\n\n\nR\n\n\nvariable selection\n\n\ngeneralized linear model\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\nLearning ChatGPT 1: Probabilities for the next word\n\n\n\n\n\n\n\nGPT-2\n\n\nChatGPT\n\n\nWolfram\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\nLogistic function in R\n\n\n\n\n\n\n\nLogistic function\n\n\nindirect vaccine effectiveness\n\n\noral cholera vaccine\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\nWaning vaccine efficacy on susceptibility\n\n\n\n\n\n\n\nVaccine\n\n\nwaning\n\n\nSEIR\n\n\nErlang distribution\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\nSEIR model with varying number of compartments\n\n\n\n\n\n\n\nSEIR\n\n\nErlang distribution\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\nWaning of vaccine effectiveness\n\n\n\n\n\n\n\nvaccine efficacy\n\n\nclinical trial\n\n\nSEIR\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\nSolutions for the steady states of the SEIR model\n\n\n\n\n\n\n\nSEIR model\n\n\nsteady states\n\n\nODE\n\n\ndeSolve\n\n\nMathematica\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\nVaccine effectiveness\n\n\n\n\n\n\n\nSEIR\n\n\nvaccine efficacy\n\n\ndirect\n\n\nindirect\n\n\ntotal\n\n\noverall\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\nCounterintuitive effects in disease transmission dynamics\n\n\n\n\n\n\n\nInfectious diseases\n\n\nnonlinearity\n\n\ntransmission dynamics\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\nPrevalence vs. incidence for the SIR model\n\n\n\n\n\n\n\nR\n\n\nprevalence\n\n\nincidence\n\n\ndeSolve\n\n\n\n\n\n\n\n\n\n\n\nMar 12, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n감염병의 대유행 가능성\n\n\n\n\n\n\n\nprobability of a large outbreak\n\n\nreproduction number\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\nModeling the waning of the vaccine-derived immunity in the ODE model\n\n\n\n\n\n\n\nvaccine-derived immunity\n\n\nwaning\n\n\ncholera\n\n\nODE\n\n\nexponential\n\n\nGamma\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\nIncremental Cost-Effectiveness Ratio (ICER)\n\n\n\n\n\n\n\ncholera\n\n\nsub-Saharan Africa\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\nImplementing incubation period of cholera in the ODE model\n\n\n\n\n\n\n\nincubation period\n\n\nODE\n\n\ncholera\n\n\nexponential\n\n\nErlang\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\nMass-action assumption: density- vs. frequency-dependent transmission\n\n\n\n\n\n\n\nmass action\n\n\nfrequency-dependent\n\n\ndensity-dependent\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\nLabelledArrays and NamedTupleTools make it easy to use the ODE model in Julia\n\n\n\n\n\n\n\njulia\n\n\nODE\n\n\nLabelledArrays\n\n\nNamedTupleTools\n\n\nSEIR\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\nSIR model benchmarks: deSolve, odin, and diffeqr\n\n\n\n\n\n\n\nODE\n\n\nR\n\n\ndeSolve\n\n\nodin\n\n\ndiffeqr\n\n\nC++\n\n\nJulia\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\ndiffeqr: R interface to the Julia’s DifferentialEquations.jl\n\n\n\n\n\n\n\ndifferential equation\n\n\njulia\n\n\nDifferentialEquations.jl\n\n\ndiffeqr\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\nUniversal differential equation using Julia\n\n\n\n\n\n\n\nuniversal differential equation\n\n\njulia\n\n\nsub-exponential growth\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\nFitting a straight line in Julia: Flux machine learning\n\n\n\n\n\n\n\njulia\n\n\nFlux\n\n\nlinear model\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\nCritical vaccination threshold\n\n\n\n\n\n\n\nvaccine\n\n\npopulation immunity\n\n\ncritical vaccination threshold\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nGeneration interval\n\n\n\n\n\n\n\ngeneration interval\n\n\nreproduction number\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\nGeneration interval, growth rate, reproduction number\n\n\n\n\n\n\n\ngeneration interval\n\n\ngrowth rate\n\n\nreproduction number\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nConvolution\n\n\n\n\n\n\n\nparticle filter\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nIdiosyncrasies and generalities\n\n\n\n\n\n\n\necology\n\n\nidiosyncransy\n\n\ngenerality\n\n\nCOVID-19\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nSIR model in Stan: Euler method\n\n\n\n\n\n\n\nR\n\n\nStan\n\n\nEuler method\n\n\nSIR model\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\nEstimating a time-to-event distribution in Stan\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\nEstimating a time-to-event distribution from right-truncated data\n\n\n\n\n\n\n\nright truncation\n\n\nexponential growth\n\n\nPoisson process\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\nEstimating serial interval: doubly interval-censored data\n\n\n\n\n\n\n\nR\n\n\nserial interval\n\n\ninterval censoring\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nEstimating serial interval for a growing epidemic\n\n\n\n\n\n\n\nR\n\n\nserial interval\n\n\ninterval censoring\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nEstimating serial interval: interval cenoring\n\n\n\n\n\n\n\nR\n\n\nserial interval\n\n\ninterval censoring\n\n\nMLE\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nBranching process model 2\n\n\n\n\n\n\n\nR\n\n\nbranching process\n\n\nfinal epidemic size\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nFinal epidemic size: uniroot vs. optimize\n\n\n\n\n\n\n\nepidemic\n\n\nsize\n\n\nR\n\n\nuniroot\n\n\noptimize\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\n2 min\n\n\n\n\n\n\nSEIR model\n\n\n\nSEIR\ndeterministic\nstochastic\nGillespie's algorithm\n\n\n\n\n\n\n\n`Nov 9, 2023`{=html}\n 8 min \n\n\n\n\n\n\nBranching process model\n\n\n\n\n\n\n\nR\n\n\nbranching process\n\n\nfinal epidemic size\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nNegative binomial regression with censored data: POLYMOD data\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\ncontact\n\n\ncensor\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\nApartment transactions in Korea via API provided by the Ministry of Land, Infrastructure, and Transport\n\n\n\n\n\n\n\nR\n\n\nAPI\n\n\napartment\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nConfidence interval using profile likelihood\n\n\n\n\n\n\n\nSEIR\n\n\nprofile likelihood\n\n\nlikelihood ratio\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nSIR model using SymPy\n\n\n\n\n\n\n\nSIR\n\n\nSymPy\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\nRegression with censored data: AER::tobit and optim\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\ncensor\n\n\ntobit\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nMulitple regression: POLYMOD data\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\ncontact\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nRegression using optim\n\n\n\n\n\n\n\nR\n\n\noptim\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nExtract raster based on a polygon\n\n\n\n\n\n\n\nR\n\n\nraster\n\n\nshapefile\n\n\ncrop\n\n\nmask\n\n\nsf\n\n\n\n\n\n\n\n\n\n\n\nSep 23, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\nodin package\n\n\n\n\n\n\n\nODE\n\n\nR\n\n\nodin\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nPubMed search, ChatGPT summary, and sending an email in Python\n\n\n\n\n\n\n\nChatGPT\n\n\nR\n\n\nxml\n\n\nhttr\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nPubMed search, ChatGPT summary, and sending an email in R\n\n\n\n\n\n\n\nChatGPT\n\n\nR\n\n\nxml\n\n\nhttr\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\nPolygon 면적 구하기: sf 와 raster 패키지\n\n\n\n\n\n\n\nR\n\n\nshapefile\n\n\nggplot2\n\n\nsf\n\n\nraster\n\n\nRColorBrewer\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\nggplot2로 지도 그리기\n\n\n\n\n\n\n\nR\n\n\nmap\n\n\nggplot2\n\n\nsf\n\n\nRColorBrewer\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nWriting a paper: Start with an outline\n\n\n\n\n\n\n\nwriting\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\nImportance sampling\n\n\n\n\n\n\n\nimportance sampling\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nImportant figures from the book, How to avoid a climate diaster? by Bill Gates\n\n\n\n\n\n\n\nparameter estimation\n\n\nR\n\n\nmaximum likelihood\n\n\nprofile likelihood\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nRegression toward the mean\n\n\n\n\n\n\n\nparameter estimation\n\n\nR\n\n\nmaximum likelihood\n\n\nprofile likelihood\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nSurvivor bias\n\n\n\n\n\n\n\nparameter estimation\n\n\nR\n\n\nmaximum likelihood\n\n\nprofile likelihood\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nEstimating the instantaneous reproduction number using the particle filter\n\n\n\n\n\n\n\nparticle filter\n\n\nCOVID-19\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\n12 min\n\n\n\n\n\n\n\n\nMaximum Likelihood and Profile Likelihood for the SEIR model\n\n\n\n\n\n\n\nparameter estimation\n\n\nR\n\n\nmaximum likelihood\n\n\nprofile likelihood\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nBasic reproduction number using SymPy\n\n\n\n\n\n\n\nBasic reproduction number\n\n\nSymPy\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nPopulation Monte Carlo 파퓰레이션 몬테카를로\n\n\n\n\n\n\n\nMonte Carlo\n\n\nR\n\n\nparameter estimation\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\nSimple mathematical models with very complicated dynamics\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nSub-exponential growth\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\nODE-based SIR models in Stan\n\n\n\n\n\n\n\nR\n\n\nStan\n\n\nODE\n\n\nSIR\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n감염재생산지수 계산하기\n\n\n\n\n\n\n\nmodeling\n\n\nreproduction number\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nParticle filter using R\n\n\n\n\n\n\n\nparticle filter\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nMultinomial distribution\n\n\n\n\n\n\n\nmultinomial\n\n\nRcpp\n\n\npomp\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nSEIR model using the Nimble pacakge\n\n\n\n\n\n\n\nnimble\n\n\nMCMC\n\n\nposterior predictive check\n\n\ntrace plot\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\nThe Model and Reality: Bridging Mathematical Frameworks with Real-World Infectious Disease Dynamics\n\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\n모델과 현실: 수학으로 감염병 확산을 읽다\n\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2023\n\n\n14 min\n\n\n\n\n\n\n\n\nOrigins of major human infectious diseases\n\n\n\n\n\n\n\ninfectious disease\n\n\nemergence\n\n\ndensity\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/apartment-data-api/index.html",
    "href": "blog/posts/apartment-data-api/index.html",
    "title": "Apartment transactions in Korea via API provided by the Ministry of Land, Infrastructure, and Transport",
    "section": "",
    "text": "Data preparation\n\nlibrary(XML)\nlibrary(RCurl)\nlibrary(dplyr)\n\nservice_key &lt;- readRDS(\"apartment_key_datagokr.rds\")\ndatlist &lt;- vector(\"list\", 12)\n\n# combine the data in 2022\nfor (m in 1:12){\n  if (m &lt; 10) {\n    dt &lt;- paste0(\"20220\", m)\n  } else {\n    dt &lt;- paste0(\"2022\", m)\n  } \n  uri &lt;- paste0(\"http://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTradeDev?LAWD_CD=11110&DEAL_YMD=\", dt, \"&serviceKey=\", service_key)\n\n  xml_doc &lt;- xmlTreeParse(uri, useInternalNodes = TRUE, encoding = \"UTF-8\")\n  root_node &lt;- xmlRoot(xml_doc)\n  xml_data &lt;- xmlToDataFrame(nodes = getNodeSet(root_node, '//item'))\n\n  datlist[[m]] &lt;- xml_data\n}\n\nd &lt;- do.call('rbind', datlist)\n\nI will plot the price\n\n# million won\nd$price &lt;- as.numeric(gsub('\\\\,', \"\", d$거래금액)) / 100\nd$area_sq_meter &lt;- as.numeric(d$전용면적) # q\nd$area_category &lt;- NA\n\nfor(i in 1:nrow(d)) {\n  ar &lt;- d$area_sq_meter[i]\n  if(ar &lt; 50){\n    d$area_category[i] &lt;- \"&lt;50\"\n  }\n  else if(ar &gt;= 50 & ar &lt; 80) {\n    d$area_category[i] &lt;- \"50-80\"\n  }\n  else if(ar &gt;= 80 & ar &lt; 100) {\n    d$area_category[i] &lt;- \"80-100\"\n  }\n  else if(ar &gt;= 100) {\n    d$area_category[i] &lt;- \"&gt;100\"\n  }\n}\n\nd$area_category &lt;- factor(d$area_category, levels=c(\"&lt;50\", \"50-80\", \"80-100\",\"&gt;100\"))\n                             \nd$levels &lt;- as.numeric(d$층)\n\nlibrary(ggplot2)\n\nd |&gt; as.data.frame() |&gt; \n  ggplot()+\n  geom_point(aes(area_category, price, color=levels)) +\n  labs(x=parse(text=paste0(\"Area~(m^2)\")), y=\"Price (million won)\", color=parse(text=paste0(\"Levels\")))+\n  theme_bw()+\n  ggtitle(\"Apartment price in Jongno-gu, Seoul, 2022\") \n\n\n\n# ggsave(\"apt_price.png\", width=3.4*1.5, height=2.7*1.5, units=\"in\")"
  },
  {
    "objectID": "blog/posts/branching_process/index.html",
    "href": "blog/posts/branching_process/index.html",
    "title": "Branching process model",
    "section": "",
    "text": "Branching process model\nIn the branching process model, the number of secondary infections is realized as a random number (e.g., Poission or Negative binomial).\n\nset.seed(42)\nR0_mean &lt;- 2\npopsize = 1000 # population size for each iteration\nnrun &lt;- 1000 # number of iterations to compute the mean\noutbreaks &lt;- rep(NA, nrun) # to store outbreak size for each iteration\ninit_inf &lt;- 1 # initially infected people\n\nfor (r in seq_len(nrun)) {\n  pop &lt;- data.frame(id=1:popsize)\n  pop$status &lt;- \"S\"\n  pop$status[1:init_inf] &lt;- \"I\"\n  nS &lt;- sum(pop$status == \"S\")\n  nI &lt;- sum(pop$status == \"I\")\n  N &lt;- nrow(pop)\n  cnt &lt;- init_inf + 1 # infecteds are placed from the first position\n  while (nI &gt; 0 & nS &gt; 0) {\n    row &lt;- which(pop$status == \"I\")\n    nI &lt;- length(row)\n    for (i in seq_len(nI)) {\n      pop$status[row[i]] &lt;- \"R\"\n      offspring &lt;- rpois(1, lambda=R0_mean*nS/N)\n      nS = nS - offspring\n      for (k in seq_len(offspring)) {\n        pop$status[cnt] &lt;- \"I\" \n        cnt &lt;- cnt + 1\n      }\n    }\n  }\n  outbreaks[r] = popsize - sum(pop$status == \"S\")\n}\n\nhist(outbreaks) # minor and major outbreaks\n\n\n\nsum(outbreaks&gt;200)/nrun # freq of major outbreaks\n\n[1] 0.792\n\nmean(outbreaks[outbreaks&gt;200])/popsize # outbreak size of the only major outbreaks\n\n[1] 0.7966742\n\nmax(outbreaks) # maximum outbreak size\n\n[1] 853\n\n\n\n\nFinal epidemic size\nTo make sure that my branching process model makes sense, let’s compare the final size of an epidemic. As shown in the previous post, for the \\(SIR\\) model in a well-mixed population, the final epidemic size, \\(R(\\infty)\\) is given as follows: \\[R(\\infty) = 1 − \\text{exp}\\left[-R_0R(\\infty)\\right]\\]\n\n# final size of an epidemic, R\nfinal_size &lt;- function(R, R0){\n  R - 1 + exp(-R0*R)\n}\n# lower bound set at a positive number to avoid R=0, which is also a solution\nuniroot(final_size, interval=c(1e-3,1), R0=R0_mean)$root\n\n[1] 0.7968115\n\n\n\n\nNegative binomial distribution\nWhat would happen if I allow the negative binomial distribution for the offspring\n\nset.seed(42)\nR0_mean &lt;- 2\nR0_size &lt;- 0.2 # loosely based on the estimate for Ebola (see Kucharski et al. 2016 https://wwwnc.cdc.gov/eid/article/22/1/15-1410_article)\npopsize = 1000 # population size for each iteration\nnrun &lt;- 1000 # number of iterations to compute the mean\noutbreaks &lt;- rep(NA, nrun) # to store outbreak size for each iteration\ninit_inf &lt;- 1 # initially infected people\n\nfor (r in seq_len(nrun)) {\n  pop &lt;- data.frame(id=1:popsize)\n  pop$status &lt;- \"S\"\n  pop$status[1:init_inf] &lt;- \"I\"\n  nS &lt;- sum(pop$status == \"S\")\n  nI &lt;- sum(pop$status == \"I\")\n  N &lt;- nrow(pop)\n  cnt &lt;- init_inf + 1 # infecteds are placed from the first position\n  while (nI &gt; 0 & nS &gt; 0) {\n    row &lt;- which(pop$status == \"I\")\n    nI &lt;- length(row)\n    for (i in seq_len(nI)) {\n      pop$status[row[i]] &lt;- \"R\"\n      offspring &lt;- rnbinom(1, mu=R0_mean*nS/N, size=R0_size)\n      nS = nS - offspring\n      for (k in seq_len(offspring)) {\n        pop$status[cnt] &lt;- \"I\" \n        cnt &lt;- cnt + 1\n      }\n    }\n  }\n  outbreaks[r] = popsize - sum(pop$status == \"S\")\n}\n\nhist(outbreaks) # minor and major outbreaks\nsum(outbreaks&gt;200)/nrun # freq of major outbreaks\nmean(outbreaks[outbreaks&gt;200])/popsize # only major outbreaks\nmax(outbreaks) # maximum outbreak size"
  },
  {
    "objectID": "blog/posts/censored-regression-optim/index.html",
    "href": "blog/posts/censored-regression-optim/index.html",
    "title": "Regression with censored data: AER::tobit and optim",
    "section": "",
    "text": "The following example was adapted from the Tobit model in Model Estimation by Example. The dataset contains 200 observations. The academic aptitude variable is apt, the reading and math test scores are read and math, respectively. The variable prog is the type of program the student is in, it is a categorical (nominal) variable that takes on three values, academic (prog = 1), general (prog = 2), and vocational (prog = 3). The variable id is an identification variable. More details of the dataset available at https://stats.oarc.ucla.edu/r/dae/tobit-models/.\n\nlibrary(data.table)\ndat = fread(\"https://stats.idre.ucla.edu/stat/data/tobit.csv\")\ndat[, prog := as.factor(prog)]\ndat\n\n      id read math       prog apt\n  1:   1   34   40 vocational 352\n  2:   2   39   33 vocational 449\n  3:   3   63   48    general 648\n  4:   4   44   41    general 501\n  5:   5   47   43    general 762\n ---                             \n196: 196   44   49    general 539\n197: 197   50   50    general 594\n198: 198   47   51    general 616\n199: 199   52   50    general 558\n200: 200   68   75    general 800\n\n\nFollowing codes were borrowed from the UCLA Advanced Research Computing\n\n# function that gives the density of normal distribution\n# for given mean and sd, scaled to be on a count metric\n# for the histogram: count = density * sample size * bin width\nf &lt;- function(x, var, bw = 15) {\n  dnorm(x, mean = mean(var), sd(var)) * length(var) * bw\n}\nlibrary(ggplot2)\n# setup base plot\np &lt;- ggplot(dat, aes(x = apt, fill=prog))\n# histogram, coloured by proportion in different programs\n# with a normal distribution overlayed\np &lt;- p + stat_bin(binwidth=15) + \n  stat_function(fun = f, size = 1,\n    args = list(var = dat$apt))\n\nggsave(\"apt_censored.png\", p)\n\nLooking at the above histogram, we can see the censoring in the values of apt, that is, there are far more cases with scores of 750 to 800 than one would expect looking at the rest of the distribution. Below is an alternative histogram that further highlights the excess of cases where apt=800.\nNote on the difference between truncation and censoring: With censored variables, all of the observations are in the dataset, but we don’t know the “true” values of some of them. With truncation some of the observations are not included in the analysis because of the value of the variable.\nThe Tobit model can be used for such a case. It is a class of regression models in which the observed range of the dependent variable is censored in some way, according to the [Wikipedia article] (https://en.wikipedia.org/wiki/Tobit_model). The possible maximum score 800, which\n\ntobit = AER::tobit(\n  apt ~ read + math + prog,\n  data = dat,\n  left = -Inf,\n  right = 800\n)\n\nTo account for censoring, likelihood function is modified to so that it reflects the unequal sampling probability for each observation depending on whether the latent dependent variable fell above or below the determined threshold. It appears that this approach was first proposed by James Tobin. For a sample that, as in Tobin’s original case, was censored from below at zero, the sampling probability for each non-limit observation is simply the height of the appropriate density function. For any limit observation, it is the cumulative distribution, i.e. the integral below zero of the appropriate density function. The likelihood function is thus a mixture of densities and cumulative distribution functions, according to the Wikipedia article.\n\\[\n\\text{log }L = \\sum_{i=1}^n w_i\\left(\\delta_i~\\text{log} \\left(P\\left(Y=y_i|X\\right)\\right) + \\left(1-\\delta_i\\right)~\\text{log} \\left(1-\\sum_{i=1}^{l_U}P(Y=y_i|X)\\right)\n\\right)\n\\] , where \\(l_U\\) represents the upper limit and\n\\[\n\\begin{equation}\n  \\delta_i=\\begin{cases}\n    1, & \\text{if }y_i &lt; l_U.\\\\\n    0, & \\text{otherwise}.\n  \\end{cases}\n\\end{equation}\n\\]\n\nLog likelihood accounting for censoring\n\nnegloglik &lt;- function(par, y, X, ul=100) {\n  # parameters\n  sd = par[length(par)]\n  beta = par[-length(par)]\n  # create indicator depending on chosen limit\n  indicator = y &lt; ul\n  # linear predictor\n  mu = X %*% beta\n  # log likelihood\n  loglik = indicator * dnorm(y, mean=mu, sd=sd, log=T) +\n             (1-indicator) * log(1-pnorm(ul, mean=mu, sd=sd))\n\n  sumloglik = sum(loglik, na.rm=T)\n  return(-sumloglik)\n}\n\n\n\noptim\n\n# Setup data and initial values.\nmod = lm(apt ~ read + math + prog, data = dat)\nX = model.matrix(mod)\ninit = c(coef(mod), sigma=summary(mod)$sigma)\n\n# negloglik(par=init, y=acad_apt$apt, X=X, ul=800)\n\nfit &lt;- optim(par = init,\n            fn = negloglik,\n            y = dat$apt,\n            X = X,\n            ul = 800)\n\ncoef(tobit)\n\n   (Intercept)           read           math    proggeneral progvocational \n    209.565971       2.697939       5.914485     -12.714763     -46.143904 \n\n(fit$par)\n\n   (Intercept)           read           math    proggeneral progvocational \n    211.054867       2.813491       5.763177     -12.354492     -45.847701 \n         sigma \n     65.652919 \n\n\n\n\noptim control parameters\nBy adjusting control parameters of the optim function, results can match more closely. Below is such an example.\n\nfit &lt;- optim(par = init,\n            fn = negloglik,\n            y = dat$apt,\n            X = X,\n            ul = 800,\n            method = \"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\n\ncoef(tobit)\n\n   (Intercept)           read           math    proggeneral progvocational \n    209.565971       2.697939       5.914485     -12.714763     -46.143904 \n\n(fit$par)\n\n   (Intercept)           read           math    proggeneral progvocational \n    209.565967       2.697937       5.914486     -12.714777     -46.143838 \n         sigma \n     65.676724"
  },
  {
    "objectID": "blog/posts/claude_api/index.html",
    "href": "blog/posts/claude_api/index.html",
    "title": "Claude 3",
    "section": "",
    "text": "Testing Claude 3 with R\n\nlibrary(httr)\nlibrary(jsonlite)\n\n# Function to make API request\nprompt_claude &lt;- function(prompt = NULL, \n                          api_key = NULL, \n                          model = \"opus\",\n                          max_tokens = 1000) {\n  if (is.null(api_key)) {\n   stop(\"api_key must provided\")\n  }\n  model &lt;- grep(model, c(\"claude-3-haiku-20240307\",\n                         \"claude-3-sonnet-20240229\",\n                         \"claude-3-opus-20240229\"), value=TRUE)\n  if(length(model) == 0){\n    stop(\"The model name is wrong. It must be one of the haiku, sonnet, and opus\")\n  }\n  \n  response &lt;- httr::POST(\n    url = \"https://api.anthropic.com/v1/messages\",\n    httr::add_headers(`x-api-key` = api_key, \n                `anthropic-version` = \"2023-06-01\",\n                `content-type` = \"application/json\"),\n    encode = \"json\",\n    body = jsonlite::toJSON(list(\n     model = model,\n     max_tokens = max_tokens,\n     messages = list(list(role = \"user\", content = prompt))\n    ), auto_unbox = TRUE)\n )\n content &lt;- httr::content(response, as=\"text\")\n json_data &lt;- jsonlite::fromJSON(content, flatten = TRUE)\n response &lt;- json_data$content$text\n \n return(response)\n}\n\napi_key &lt;- Sys.getenv(\"CLAUDE_API_KEY\") # api key was stored in the .Renviron file\nres &lt;- prompt_claude(prompt=\"The cat sat on the\", api_key, model=\"haiku\")\ncat(res)"
  },
  {
    "objectID": "blog/posts/convolution/index.html",
    "href": "blog/posts/convolution/index.html",
    "title": "Convolution",
    "section": "",
    "text": "Convolution\nThe following content was adapted from Grant Sanderson’s YouTube video and the Wikipedia article.\nConvolution is a fundamental concept in mathematics and statistics, playing a crucial role in various applications ranging from signal processing to probability theory. In this blog post, we’ll explore what convolution is, its significance, and how it’s used in mathematics and statistics. Additionally, I’ll include a practical example using R.\nConvolution is a mathematical operation that combines two functions to produce a third function. It’s a way of ‘mixing’ two functions, often used to understand the way a system modifies a signal. In mathematical terms, the convolution of two functions, \\(f\\) and \\(g\\), is defined as:\n\\[\n(f*g)(t) = \\int_0^{\\infty} f(\\tau)g(t-\\tau) \\textrm{d}\\tau\n\\] Let’s consider a simple example of convolution in R. We’ll convolve two functions, a sine wave and a cosine wave, to see how they interact.\n\n# Define the two functions\nf &lt;- function(x) sin(x)\ng &lt;- function(x) cos(x)\n\n# Create a sequence of points\nx &lt;- seq(-pi, pi, length.out = 100)\n\n# Perform the convolution\nconvolved &lt;- convolve(f(x), g(x), type = \"open\")\n\n# Plot the original functions and their convolution\nplot(x, f(x), type='l', col='blue', ylim=c(-50, 50))\nlines(x, g(x), col='red')\nlines(x, convolved[seq(1,199, length.out=100)], col='green')\nlegend(\"topright\", legend=c(\"f(x) = sin(x)\", \"g(x) = cos(x)\", \"Convolved\"), col=c(\"blue\", \"red\", \"green\"), lty=1)\n\n\n\n\nIn compartmental modeling, it can be used to explore the distribution and features of the consecutive compartments. For example, in the SEIR model, the the length of a generation interval is given by the convolution of E and I compartments."
  },
  {
    "objectID": "blog/posts/critical_vacc_threshold/index.html",
    "href": "blog/posts/critical_vacc_threshold/index.html",
    "title": "Critical vaccination threshold",
    "section": "",
    "text": "The following article by Fine provides a great introduction to the critical vaccination threshold.\n\nCritical vaccination threshold\nThe simplest scenario would be to assume that individuals are well-mixed and vaccine recipients are completely protected from infection. For this scenario, the critical threshold for random vaccination, \\(V_c\\), is as follows: \\[\nV_c = 1-1/R_0\n\\] If vaccines are only partially protective and \\(E\\) fraction of the vaccine recipients are completely protected, \\(V_c\\) becomes as follows: \\[\nV_c = \\frac{1-1/R_0}{E}\n\\]\nI used the population World Population Prospects 2022.\n\n\nPlot the results\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nR0 &lt;- seq(1,5,length.out=100)\nVE &lt;- c(0.4, 0.6, 0.8)\nVc &lt;- lapply(VE, function(x) ifelse((1-1/R0)/x &lt;= 1.0, 100*(1-1/R0)/x, NA))\n\ndf &lt;- data.frame(R0=R0, Vc1=Vc[[1]], Vc2=Vc[[2]], Vc3=Vc[[3]])\n\ngg &lt;- ggplot(df, aes(x = R0)) +\n  geom_rect(aes(xmin=1.15, xmax=2.78, ymin=0, ymax=max(df$Vc3)),\n                   fill = \"pink\", alpha=0.01)+\n  geom_line(aes(y=Vc3, linetype=\"80%\")) +\n  geom_line(aes(y=Vc2, linetype=\"60%\")) +\n  geom_line(aes(y=Vc1, linetype=\"40%\")) +\n  scale_linetype_manual(\"Vaccine efficacy\", values=c(\"80%\"=\"solid\",\n                                   \"60%\"=\"dashed\", \n                                   \"40%\"=\"dotted\"))+\n  # labs(y=\"Critical vaccination threshold\", x=expression(R[0])) +\n  labs(y=expression(paste(\"Critical vaccination threshold (%), \", V[C])), \n       x=expression(paste(\"Basic reproduction number, \", R[0]))) +\n  theme(text = element_text(size=16),\n        axis.text = element_text(size=16),\n        legend.text=element_text(size=15), \n        legend.position = \"bottom\")\n\ngg\n\n\n\n# ggsave(\"Vc_R0.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)\n\n\nR0 &lt;- c(1.15, 2.78)\n(Vc &lt;- 1-1/R0)\n\n[1] 0.1304348 0.6402878\n\n\n\n\nIndia simulation\n\nlibrary(data.table)\nlibrary(readxl)\nlibrary(dplyr)\n# d &lt;- read_xlsx(\"C:/Users/jonghoon.kim/Documents/myblog/posts/critical_vacc_threshold/WPP2022_POP_F01_1_POPULATION_SINGLE_AGE_BOTH_SEXES.xlsx\", sheet= \"Medium variant\")\n# d2 &lt;- d[-(1:11),]\n# india &lt;- d2[(d2$...3 == \"India\" & d2$...11 &gt; 2023),]\n# names(india) &lt;- c(1:2, \"country\", 4:10 ,\"year\", paste0(\"age_\", 0:100))\n\n# saveRDS(india, \"C:/Users/jonghoon.kim/Documents/myblog/posts/critical_vacc_threshold/WPP2022_India.rds\")\n\nindia &lt;- readRDS(\"WPP2022_India.rds\")\nindia$year &lt;- as.numeric(india$year)\n# india[,paste0(\"age_\", 0:100)] &lt;- as.numeric(india[,paste0(\"age_\", 0:100)])\nindia &lt;- india %&gt;% mutate_at(paste0(\"age_\", 0:100), as.numeric)\n\nyrs &lt;- 2024:2100\n# simulate\nVE &lt;- c(0.4, 0.6, 0.8)\nprop_immune &lt;- rep(NA, length(yrs))\n\nfor(i in seq_along(yrs)) {\n  numerator &lt;- sum(india[india$year == yrs[i], paste0(\"age_\", 0:(i-1))])\n  denominator &lt;- sum(india[india$year == yrs[i], paste0(\"age_\", 0:100)])\n  prop_immune[i] &lt;- numerator / denominator\n} \n\nexisting_immune &lt;- 20\npop_immune_ve &lt;- lapply(VE, function(z) 100*z*prop_immune)\n# add the existing immunity, assumed to be 15%\npop_immune_ve_added &lt;- lapply(VE, function(z) 100*z*prop_immune + existing_immune)\n\ndf &lt;- data.frame(years=yrs-2024, \n                 pi1=pop_immune_ve[[1]], \n                 pi2=pop_immune_ve[[2]], \n                 pi3=pop_immune_ve[[3]], \n                 pi4=pop_immune_ve_added[[1]], \n                 pi5=pop_immune_ve_added[[2]], \n                 pi6=pop_immune_ve_added[[3]])\n\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\n\ngg &lt;- ggplot(df, aes(x = years)) +\n  geom_rect(aes(xmin=0, xmax=30, ymin=14, ymax=63),\n                   fill = \"pink\", alpha=0.01)+\n  geom_line(aes(y=pi3, linetype=\"80%\")) +\n  geom_line(aes(y=pi2, linetype=\"60%\")) +\n  geom_line(aes(y=pi1, linetype=\"40%\")) +\n  geom_line(aes(y=pi6, linetype=\"80%\"), color=\"dark green\") +\n  geom_line(aes(y=pi5, linetype=\"60%\"), color=\"dark green\") +\n  geom_line(aes(y=pi4, linetype=\"40%\"), color=\"dark green\") +\n  scale_x_continuous(limits=c(0,30))+\n  scale_linetype_manual(\"Vaccine efficacy\", \n                        values=c(\"80%\"=\"solid\",\n                                 \"60%\"=\"dashed\", \n                                 \"40%\"=\"dotted\"))+\n  # labs(y=\"Critical vaccination threshold\", x=expression(R[0])) +\n  labs(y=\"Population immune (%)\", x=\"Years since vaccination\") +\n  theme(text = element_text(size=16),\n        axis.text = element_text(size=16),\n        legend.text=element_text(size=15), \n        legend.position = \"bottom\")\ngg\n\n\n\n# ggsave(\"immune_time.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "blog/posts/density_frequency/index.html",
    "href": "blog/posts/density_frequency/index.html",
    "title": "Mass-action assumption: density- vs. frequency-dependent transmission",
    "section": "",
    "text": "Density-dependent vs. frequency-dependent transmission\nIn models of transmission of directly transmitted pathogens, e.g., COVID-19, the transmission is assumed to occur via so-called mass action principle. It means the rate of newly infected people per unit area, per unit of time is proportional to the product between the numbers (or densities) of susceptible and infectious individuals. The term appears to be first coined by Hamer in 1906 in his paper. However, as indicated by McCallum, there have been some confusion over using the term, mass action. The confusion is around that mass action principle may be implemented in the manner of frequency-dependent or density-dependent transmission.\nIn the density-dependent transmission scenario, the rate of change in the number or density of infected individual can be implemented as follows:\n\\[\\mathrm{d}I = \\beta^* SI - \\gamma I\\].\nIn the frequency-dependent scenario, arguably more widely adopted one, the rate of change in the number or density of infected individual can be implemented as follows:\n\\[\\mathrm{d}I = \\beta SI/N - \\gamma I\\] Two formulations have different implications. The most notable difference is that there is a threshold density, \\(N_T\\) in the density-dependent formulation whereas the second model leads to the basic reproduction ratio, \\(R_0\\), is derived from frequency-dependent. Of course, it is straightforward to interchange between the two types of models.\nBoth \\(\\beta^*\\) and \\(\\beta\\) are transmission coefficients but their unit are different. In the frequency-dependent formulation, \\(\\beta\\) can mean the number of transmissions per unit of time for an infected individual when it contacts with susceptible individuals. Therefore, the whole term, \\(\\beta I S/N\\), could mean that the number of newly infected individuals per unit of time. For density-dependent formulation, \\(\\beta^*\\) can mean the number of transmissions per unit of density per unit of time for an infected individual. The whole term \\(\\beta^* I S\\) can then mean the density of newly infected individuals per unit of time.\nNow let’s implement a simple SIR model in two different formulations.\n\nDensity-dependent transmission\n\nsir_den &lt;- function(t, u, p){\n  new_Istar = p[[\"beta_star\"]]*u[1]*u[2]\n  du1 &lt;- - new_Istar\n  du2 &lt;- + new_Istar - p[[\"gamma\"]]*u[2]\n  du3 &lt;- + p[[\"gamma\"]]*u[2]\n    \n  return(list(c(du1,du2,du3))) \n}\n\nlibrary(deSolve)\npop = 1000\nI0 = 10\nu0 &lt;- c(pop - I0, I0, 0)\ntspan &lt;- seq(from=0, to=50)\np &lt;- c(beta_star=0.0006, gamma=0.2)\n\noutdf &lt;- as.data.frame(ode(y=u0, times=tspan, func=sir_den, parms=p))\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(outdf,aes(x=time))+\n  geom_line(aes(y=`1`, color=\"S\")) +\n  geom_line(aes(y=`2`, color=\"I\")) +\n  geom_line(aes(y=`3`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n# threshold host population size\nNt = p[\"gamma\"] / p[\"beta_star\"]\n# R0 can be computed through N/Nt\nR0 = pop/Nt\n\nfinal_epidemic_size &lt;- function(R0 = 2) {\n  y = function(x) x - 1 + exp(-R0*x)\n  final_size &lt;- uniroot(y, interval=c(1e-6,1-1e-6))$root\n\n  return(final_size)\n\n}\nround(pop*final_epidemic_size(R0=R0), digits=2)\n\n[1] 940.48\n\nround(tail(outdf, 1)$`3`, digits=2)\n\n[1] 939.32\n\n\n\n\nFrequency-dependent formulation\n\nsir_freq &lt;- function(t, u, p){\n  new_I = p[[\"beta\"]]*u[1]*u[2]/sum(u)\n  du1 &lt;- - new_I\n  du2 &lt;- + new_I - p[[\"gamma\"]]*u[2]\n  du3 &lt;- + p[[\"gamma\"]]*u[2]\n    \n  return(list(c(du1,du2,du3))) \n}\n\nlibrary(deSolve)\npop = 1000\nI0 = 10\nu0 &lt;- c(pop - I0, I0, 0)\ntspan &lt;- seq(from=0, to=50)\np &lt;- c(beta=0.6, gamma=0.2)\n\noutdf &lt;- as.data.frame(ode(y=u0, times=tspan, func=sir_freq, parms=p))\n\nggplot(outdf,aes(x=time))+\n  geom_line(aes(y=`1`, color=\"S\")) +\n  geom_line(aes(y=`2`, color=\"I\")) +\n  geom_line(aes(y=`3`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n# R0 threshold\n(R0 = p[[\"beta\"]] / p[[\"gamma\"]])\n\n[1] 3\n\nround(pop*final_epidemic_size(R0=R0), digits=2)\n\n[1] 940.48\n\nround(tail(outdf, 1)$`3`, digits=2)\n\n[1] 939.32"
  },
  {
    "objectID": "blog/posts/double-interval-censoring/index.html",
    "href": "blog/posts/double-interval-censoring/index.html",
    "title": "Estimating serial interval: doubly interval-censored data",
    "section": "",
    "text": "We start simple. Our task is to estimate parameters of a probability density function used to model the serial interval. Suppose dates of onsets of infectors, \\(A\\), and infectees, \\(B\\), are given as specific dates. Then the likelihood function for the serial interval may be written down as follows:\n\\[\\mathcal{L}(X;\\theta) = \\prod_{i=1}^{n} f_{\\theta}(B_i - A_i)\\], where \\(f\\) is the probability density function for the serial interval with the unknown parameters, \\(\\theta\\).\nNow suppose that the dates of symptom onset of the infectors are given as intervals. We can use the following argument also for the case where dates of symptom onset of the infectees are given as intervals. In this case, the above likelihood function may be modified as follows:\n\\[\\mathcal{L}(X;\\theta) = \\prod_{i=1}^{n} \\int_{A^L_i}^{A^R_i} f_{\\theta}(B_i-a) ~\\text{d}a\\] , where \\(A^L, A^R, B\\) present the times for lower end and upper bound on the potential dates of symptom onset of the infector, and the symptom onset time of the infectee, respectively.\nNow suppose that both the dates of onset of the infectors and infectees are given as intervals. This is so called the doubly interval-censored data discussed by Reich et al 2009. The likelihood function may be given as follows:\n\\[\\mathcal{L}(X;\\theta,\\lambda) = \\prod_{i=1}^{n} \\int_{A^L_i}^{A^R_i} \\int_{B^L_i}^{B^R_i} h_{\\lambda}(a) f_{\\theta}(b-a) ~\\text{d}b \\text{d}a\\] , where \\(A^L, A^R, B^L, B^R\\) present the times for left and right boundaires on the possible onset times of the infector, \\(A\\), and the infectee, \\(B\\), respectively. \\(h_{\\lambda}(x)\\) represents the probability density function for the time of symptom onset of the infector, which we assume follows a uniform distribution.\nMore detailed analyses of doubly interval-censored data were discussed by Reich et al 2009. The same concept has recently been applied to estimation of serial interval of CVODI-19 by Nishiura et al. 2020.\nIn the following codes, we create the fake data set and add intervals such that the serial intervals may become shorter.\n\nset.seed(42)\nn &lt;- 100\nshape_true &lt;- 2.2\nscale_true &lt;- 3.3\n\nonset_infector &lt;- sample(20:30, size=n, replace=TRUE)\nonset_infectee &lt;- onset_infector + rgamma(n, shape=shape_true, scale=scale_true)\nnll &lt;- function(parms, x) -sum(dgamma(x, shape=parms[[1]], scale=parms[[2]], log=TRUE))\nres1 = optim(par=c(1,2), fn=nll, x=onset_infectee - onset_infector, method = \"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\n\n# singly interval-censored data\ntau &lt;- sample(1:5, n, replace=TRUE)\nAL &lt;- onset_infector \nAR &lt;- onset_infector + 2*tau # this will lead to shorter serial interval\n\nnll_single_censor &lt;- function(parms, AL, AR, B){\n  -sum(log(pgamma(B-AL, shape=parms[[1]], scale=parms[[2]]) - pgamma(B-AR, shape=parms[[1]], scale=parms[[2]])))\n}\n\nres2 = optim(par=c(1,2), fn=nll_single_censor, AL=AL, AR=AR, \n             B=onset_infectee, method=\"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\n\n# doubly interval-censored data\nBL &lt;- onset_infectee - 2*tau # this will lead to even shorter serial interval\nBR &lt;- onset_infectee\n\nnll_double_censor &lt;- function(parms, AL, AR, BL, BR){\n  -sum(log(dunif(AL, min=AL, max=AR)*(pgamma(BR-AL, shape=parms[[1]], scale=parms[[2]]) - pgamma(BL-AR, shape=parms[[1]], scale=parms[[2]]))))\n}\n\nres3 = optim(par=c(1,2), fn=nll_double_censor, AL=AL, AR=AR,\n             BL=BL, BR=BR, method=\"Nelder-Mead\",\n            control=list(maxit=2e4, reltol=1e-15))\n\nx1 &lt;- rgamma(1e3, shape=res1$par[[1]], scale=res1$par[[2]])\nx2 &lt;- rgamma(1e3, shape=res2$par[[1]], scale=res2$par[[2]])\nx3 &lt;- rgamma(1e3, shape=res3$par[[1]], scale=res3$par[[2]])\nsummary(x1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1677  3.8165  6.4303  7.5411 10.3268 36.0535 \n\nsummary(x2)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.03203  1.37089  3.37988  4.70497  6.48863 38.89572 \n\nsummary(x3)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.00001  0.36330  1.48009  2.92934  4.04038 46.12810 \n\ndf = data.frame(model=rep(c(\"No censoring\", \"Singly interval-censored\",\n                            \"Doubly interval-censored\"), each=1e3),\n                val=c(x1,x2, x3))\n\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\ndf |&gt; ggplot(aes(x=val, fill=model))+\n  geom_density(alpha=0.2) +\n  labs(x=\"value\", y=\"density\")+\n  theme(legend.position = \"bottom\", \n        legend.title = element_blank())\n\n\n\n# ggsave(\"double_interval_censor.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "blog/posts/euler_multinomial/index.html",
    "href": "blog/posts/euler_multinomial/index.html",
    "title": "Multinomial distribution",
    "section": "",
    "text": "A simple particle filter in R\nWhen implementing a model of stochastic disease transmission, one has to deal with a situation in which multiple events are possible. For example, susceptible people may become infected, remain susceptible, or die from other causes. In R, one could use rmultinorm as long as one can assign a probability for each event. Here, however, we implement a function from scratch. One way is to follow the approach of Aaron King, author of the pomp package. His method is implemented in C and I adapted it to R and C++ while removing many of its auxiliary functions (e.g., checking the validity of the inputs).\n\nreulermultinom2 &lt;- function (m=2, size, rate, dt) {\n  trans &lt;- matrix(NA, nrow=m, ncol=length(rate))\n  p &lt;- 0.0 # total event rate\n  if ((size &lt; 0.0) | (dt &lt; 0.0) | (floor(size+0.5) != size)) {\n    for (k in seq_along(rate)) {\n      trans[k] = NaN\n    }\n    return(trans)\n  }\n  if (sum(rate &lt; 0.0) &gt; 0){\n    stop(\"Negative rates  are not allowed\")\n  }\n  else {\n    p &lt;- sum(rate)\n  }\n  if (p &gt; 0.0) {\n    for (i in 1:m) {\n      tmpsize &lt;- rbinom(1, size = size, prob = (1-exp(-p*dt))) # total number of events\n      tmpp &lt;- p\n      for (k in 1:(length(rate)-1)) {\n        trans[i, k] = rbinom(1, tmpsize, rate[k]/tmpp)\n        tmpsize = tmpsize - trans[i, k];\n        tmpp = tmpp - rate[k];\n      }\n      trans[i, length(rate)] = tmpsize;\n    }    \n  } \n  \n  return(trans)\n}\n\nLet’s compare it with the original function provided in the pomp package\n\nx &lt;- t(pomp::reulermultinom(1e5, 100, rate=c(1,2), dt=0.05))\ny &lt;- reulermultinom2(1e5, 100, rate=c(1,2), dt=0.05)\nxy &lt;- as.data.frame(cbind(x, y))\nnames(xy) &lt;- c(\"pomp_var1\", \"pomp_var2\", \"var1\", \"var2\")\napply(xy, 2, summary)\n\n        pomp_var1 pomp_var2    var1     var2\nMin.      0.00000   0.00000  0.0000  0.00000\n1st Qu.   3.00000   7.00000  3.0000  7.00000\nMedian    4.00000   9.00000  4.0000  9.00000\nMean      4.64097   9.27939  4.6419  9.28399\n3rd Qu.   6.00000  11.00000  6.0000 11.00000\nMax.     16.00000  25.00000 15.0000 24.00000\n\n\nThe speed difference is quite substantial.\n\nlibrary(microbenchmark)\nmicrobenchmark(pomp::reulermultinom(100, 100, rate=c(1,2), dt=0.05), reulermultinom2(100, 100, rate=c(1,2), dt=0.05))\n\nUnit: microseconds\n                                                      expr     min       lq\n pomp::reulermultinom(100, 100, rate = c(1, 2), dt = 0.05)  39.702  41.4505\n      reulermultinom2(100, 100, rate = c(1, 2), dt = 0.05) 377.301 384.9005\n      mean  median       uq      max neval cld\n  48.18708  43.651  48.0015   92.801   100  a \n 561.12002 401.851 641.1015 6275.101   100   b\n\n\nRewrite the function in C++ using Rcpp.\n\nRcpp::cppFunction(\"NumericMatrix reulermultinom_cpp(int m, double size, NumericVector rate, double dt) {\n  int ncol = rate.size();\n  NumericMatrix trans(m, ncol);\n  double p = sum(rate); //total event rate\n  for (int i = 0; i &lt; m; i++) { \n    double tmpp = p;\n    double tmpsize = R::rbinom(size, (1-exp(-tmpp*dt))); // total number of events\n    for (int k = 0; k &lt; (ncol-1); k++) {\n      double tr = R::rbinom(tmpsize, rate(k)/tmpp);\n      trans(i, k) = tr;\n      tmpsize = tmpsize - trans(i, k);\n      tmpp = tmpp - rate(k);\n    }\n    trans(i, (ncol-1)) = tmpsize;\n  }    \n  return(trans);\n}\")\n\n\nlibrary(microbenchmark)\nmicrobenchmark(pomp::reulermultinom(1e5, 100, rate=c(1,2), dt=0.05), reulermultinom_cpp(1e5, 100, rate=c(1,2), dt=0.05))\n\nUnit: milliseconds\n                                                        expr     min       lq\n pomp::reulermultinom(1e+05, 100, rate = c(1, 2), dt = 0.05) 24.9228 27.61285\n   reulermultinom_cpp(1e+05, 100, rate = c(1, 2), dt = 0.05) 23.9839 25.87615\n     mean   median      uq      max neval cld\n 51.27772 41.71065 66.0408 131.8367   100   a\n 51.02618 43.38275 67.6930 120.4940   100   a\n\n\n\nx &lt;- t(pomp::reulermultinom(1e5, 100, rate=c(1,2), dt=0.05))\ny &lt;- reulermultinom_cpp(1e5, 100, rate=c(1,2), dt=0.05)\nxy &lt;- as.data.frame(cbind(x, y))\nnames(xy) &lt;- c(\"pomp_var1\", \"pomp_var2\", \"var1\", \"var2\")\napply(xy, 2, summary)\n\n        pomp_var1 pomp_var2     var1     var2\nMin.       0.0000   0.00000  0.00000  0.00000\n1st Qu.    3.0000   7.00000  3.00000  7.00000\nMedian     4.0000   9.00000  4.00000  9.00000\nMean       4.6498   9.28149  4.64416  9.28147\n3rd Qu.    6.0000  11.00000  6.00000 11.00000\nMax.      17.0000  22.00000 15.00000 24.00000\n\nlibrary(tidyr)\nxy |&gt; pivot_longer(cols=1:4, names_to=\"var\") -&gt; xylong\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nextrafont::loadfonts()\nggplot(xylong)+\n  geom_violin(aes(x=var, y=value))+\n  facet_wrap(~var, nrow=1, scales=\"free_x\")\n\n\n\n# ggsave(\"multinomial.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "blog/posts/fit_a_straight_line_julia/index.html",
    "href": "blog/posts/fit_a_straight_line_julia/index.html",
    "title": "Fitting a straight line in Julia: Flux machine learning",
    "section": "",
    "text": "Fitting a straight line in Julia\nThis post is my attempt to learn machine learning in Julia. The contents of this page came from the Flux. Flux is a machine learning package written in Julia.\n\n\nCreate training and test data\n\nusing Flux, Distributions, Random, Statistics\n# create the data\n# true parameter values are 4 and 2\nlinear_model(x) = rand(Normal(4x+2,1))[1]\n\nlinear_model (generic function with 1 method)\n\n\nx_train, x_test = hcat(0:5...), hcat(6:10...)\n\n([0 1 … 4 5], [6 7 … 9 10])\n\ny_train, y_test = linear_model.(x_train), linear_model.(x_test)\n\n([2.4026932318407495 5.949285185429201 … 17.316799499402293 20.00123315299341], [26.267203000424637 29.3186325205348 … 40.34823197526444 42.919475237241265])\n\n\n\n\nCreate a neural network model with a single layer\n\n# y = σ.(W * x .+ bias) \nmodel = Flux.Dense(1 =&gt; 1) # 2 parameters\n\nDense(1 =&gt; 1)       # 2 parameters\n\n# prediction based on the untrained baseline model \nmodel(x_train)\n\n1×6 Matrix{Float32}:\n 0.0  -1.66877  -3.33753  -5.0063  -6.67506  -8.34383\n\n\n# define the loss function to use it for training\nloss(m, x, y) = mean(abs2.(m(x) .- y))\n\nloss (generic function with 1 method)\n\nloss(model, x_train, y_train)\n\n335.3910074651219\n\n\n\n\nTrain the model\nFlux package has the Flux.train! function for model training. The function requires an optimizer, which can be, for example, created using Descent function.\n\nusing Flux: train!\n\nopt = Descent() # gradient descent algorithm \n\nDescent(0.1)\n\nDescent(0.1)\n\nDescent(0.1)\n\n\ndata = [(x_train, y_train)]\n\n1-element Vector{Tuple{Matrix{Int64}, Matrix{Float64}}}:\n ([0 1 … 4 5], [2.4026932318407495 5.949285185429201 … 17.316799499402293 20.00123315299341])\n\n\ntrain!(loss, model, data, opt)\nloss(model, x_train, y_train)\n\n318.3230947174688\n\n\n# check model parameters after going through the data once\nmodel.weight, model.bias\n\n(Float32[9.375584;;], Float32[3.186498])\n\n\n# iteratively train the model\nfor epoch in 1:200\n    train!(loss, model, data, opt)\nend\n\n\n\nExamine the trained model\n\n# check the loss\nloss(model, x_train, y_train)\n\n0.3441841968180955\n\n# check the model parameters\nmodel.weight, model.bias\n\n(Float32[3.637514;;], Float32[2.748934])\n\n# check the model against the test data set\nmodel(x_test)\n\n1×5 Matrix{Float32}:\n 24.574  28.2115  31.849  35.4866  39.1241\n\ny_test\n\n1×5 Matrix{Float64}:\n 26.2672  29.3186  34.5149  40.3482  42.9195"
  },
  {
    "objectID": "blog/posts/generation_interval1/index.html",
    "href": "blog/posts/generation_interval1/index.html",
    "title": "Generation interval",
    "section": "",
    "text": "Generation interval = incubation period + infectious period?\nAlthough not published, I wrote a correspondence to Lancet to commenting the article. In the article, the authors stated that the generation interval is the sum of the incubation period and the infectious period. I argued that this statement holds only for a constant rate of transmission during the exponentially distributed infectious period (i.e., only a very limited case). Then, I showed that the generation interval varies if the infectious period distribution is different from the exponential distribution even with the constant rate of transmission. Following is the whole message. I now think I am glad that it was not published because I realized that Svensson wrote an article in 2007 to show a similar message in a more rigorous and general way. At least the message and arguments I presented were correct.\n\n\nThe main text\nWhen confronted with a novel disease like the coronavirus disease 2019 (COVID-19), the incubation period and the serial interval are two critical epidemiological variables that are measured during the early stages of an outbreak. A transmission model can be constructed based on these variables to project potential scenarios of outbreak expansion or resolution and, importantly, define the basic reproduction number (\\(R_0\\)) for the disease. The recent article by Wu et al., published in The Lancet on January 31 of this year, one month after the first case was reported in Wuhan, China, uses a transmission model parameterized by data available at the time. This study provided important and timely insights into the spread of COVID-19 in China and elsewhere, concluding that, in addition to Wuhan, other major cities in China and well-connected cities outside China were likely to already have sustained localized outbreaks. The model utilized by Wu et al. makes a key statement, which has limited applications and may lead to unjustified assumptions in subsequent model iterations. The dynamic model requires an input on the length of the infectious period, which Wu et al. derived by assuming that the serial interval is equal to the sum of the infectious period and the latent period (on page 4 of the article by Wu et al.). Neither Wu et al. nor the study to which they refer provides a clear rationale for this statement. The serial interval represents the time between the clinical onset of successive cases [3] and it is naturally expressed as the sum of the incubation period and disease age at transmission* 4. Here we treat the incubation period (the time between infection and the onset of symptoms) as the same as latent period (the time between infection and the onset of infectiousness), for simplicity. The disease age at transmission represents the time between the onset of symptoms of a primary case and the time of infection of the associated secondary cases. Therefore, for the mean serial interval to be the same as the sum of mean incubation period and mean infectious period, the mean disease age at transmission must be equal to the mean infectious period. The mean disease age at transmission is indeed as long as the mean of the exponentially-distributed infectious period, which is often used for convenience in building a differential equation-based model (Table 1). However, for other distributions we explored, the mean disease age at transmission is shorter than mean infectious period, and it is only half of the mean infectious period† when the infectious period is constant. The gamma-distributed infectious period can also be easily adopted in a differential-equation based model to represent a more realistic representation of infectious period and in this case, following Wu et al.’s recipe will lead to a shorter serial interval than is intended. Moreover, each of the aforementioned arguments holds when transmission occurs at a constant rate over the infectious period, and violating this assumption of a constant rate (such as when viral load is high during the initial part of the infectious period and tapers off over time 56 will likely induce an even shorter disease age at transmission.\nThe assumption that the serial interval is the sum of incubation (or latent) period and infectious period seems to hold only when the infectious period is exponentially distributed for a constant transmission rate. For a wider application, one has to calculate the disease age at transmission that leads to a correct serial interval for a given distribution of the infectious period. We believe this is an important point and potentially unjustified assumptions in subsequent model iterations may result in providing outbreak responders with inaccurate projections on the characteristics of disease transmission.\nTable 1. Time to infection since symptom onset under infectious period with different distributions.\n\nTable 1. Time to infection since symptom onset under infectious period with different distributions.\n\n\n\n\n\n\n\n\nInfectious period\nTime to transmission (disease age)\n\n\nDistribution\nExpectation\nSample mean‡ [\\(2.5^{th}\\), \\(97.5^{th}\\) percentile]\nExpectation§\n\n\n\n\nExponential\n5\n5\n5\n\n\nGamma(2,5/2)¶\n5\n3.73 [3.75, 3.71]\n3.75\n\n\nUniform(0,10)\n5\n3.34 [3.32, 3.35]\n3.33\n\n\nWeibull(2,5/2)\n5\n3.17 [3.16, 3.19]\n3.18\n\n\nLognormal(2,5/2)\n5\n2.90 [2.89, 2.91]\n2.90\n\n\nConstant at 5\n5\n2.51 [2.49, 2.52]\n2.50\n\n\n\nFootnotes *Taking statistical distribution into account, we can write the serial interval at time \\(t\\) as the convolution of the distributions of disease age, \\(g\\), and incubation period, \\(f\\): \\(s(t)= \\int_0^t g(t-\\tau)f(\\tau)\\textrm{d}\\tau\\). † Interestingly, the Wikipedia article says that the length of a serial interval is equal to the sum of incubation period and the half of the infectious period 7. ‡To simulate disease age, we drew a random infectious period from each distribution and let transmission occur at a constant probability with R0 of 2.2 over the infectious period. We repeated the process 100000 times to calculate a mean and a \\(2.5^{th}\\) and \\(95^{th}\\) percentiles. §The probability density function for the disease age at transmission at time \\(t\\) may be obtained by \\(g(t) = \\frac{P(X&gt;t)}{∫_0^∞ P(X&gt;t)dt} ~\\textrm{for}~ t∈(0,+∞)\\) for infectious period \\(X\\). The expected value can then be calculated as \\(\\int_0^{\\infty} t g(t)\\textrm{d}t\\). ¶Gamma(shape, rate). This distribution can be produced by assuming two successive compartments with the exit rate of 2/5."
  },
  {
    "objectID": "blog/posts/ICER/index.html",
    "href": "blog/posts/ICER/index.html",
    "title": "Incremental Cost-Effectiveness Ratio (ICER)",
    "section": "",
    "text": "The Incremental Cost-Effectiveness Ratio (ICER) is a crucial metric in health economics, offering insights into the value of medical interventions by comparing their costs and effectiveness. Essentially, ICER is used to evaluate the cost-effectiveness of a new healthcare intervention compared to an existing standard of care. It is calculated as the difference in costs between two options divided by the difference in their effectiveness, typically measured in quality-adjusted life years (QALYs). This ratio helps policymakers and healthcare providers make informed decisions on allocating limited resources to maximize health benefits.\nConsider the example of cholera, a severe diarrhoeal disease caused by the Vibrio cholerae bacterium, and the use of the oral cholera vaccine (OCV). In regions where cholera is endemic, introducing or expanding the use of OCV can significantly reduce the incidence of the disease. To assess the cost-effectiveness of OCV, we can compare the ICER of vaccinating a population against cholera to the standard of care, which might include treatments like rehydration solutions or antibiotics for those already infected.\nLet’s explore how to calculate the ICER for introducing OCV in a hypothetical scenario using R. Assume we have data on the cost of vaccinating individuals and the effectiveness of the vaccine in preventing cholera, as well as the cost and effectiveness of the current standard cholera treatment.\n\n# Define the costs and effectiveness of the oral cholera vaccine (OCV) and standard treatment\ncost_OCV &lt;- 10000 # Total cost of vaccinating a population\neffectiveness_OCV &lt;- 0.5 # Reduction in cholera incidence due to vaccination\ncost_standard &lt;- 0 # Total cost of treating cholera without vaccination\neffectiveness_standard &lt;- 0 # Reduction in cholera incidence with standard treatment\n\n# Calculate the incremental cost and effectiveness\nincremental_cost &lt;- cost_OCV - cost_standard\nincremental_effectiveness &lt;- effectiveness_OCV - effectiveness_standard\n\n# Calculate the ICER\nICER &lt;- incremental_cost / incremental_effectiveness"
  },
  {
    "objectID": "blog/posts/ICER/index.html#discounting",
    "href": "blog/posts/ICER/index.html#discounting",
    "title": "Incremental Cost-Effectiveness Ratio (ICER)",
    "section": "Discounting",
    "text": "Discounting\n\n# Discrete version of discounting\nf1 &lt;- function(r, L){\n  s &lt;- rep(NA, L)\n  for (i in 1:L) {\n    yr_discounted &lt;- 1*1/(1+r)^(i-1)\n    s[i] = yr_discounted;\n  }\n  s\n}\ns1 = f1(0.03, 30)\ntail(s1, 1)\n\n[1] 0.4243464\n\nsum(s1)\n\n[1] 20.18845\n\n# continuous version \nf2 &lt;- function(r, L){\n  s &lt;- rep(NA, L)\n  yr0 &lt;- 1/r * (1 - exp(-r*1))\n  for (i in 1:L) {\n    yr_discounted &lt;- yr0*exp(-r*(i-1))\n    s[i] = yr_discounted\n  }\n  s\n}\ns2 = f2(0.03, 30)\ntail(s2, 1)\n\n[1] 0.4127297\n\nsum(s2)\n\n[1] 19.78101\n\n# analytic solution for the sum of the countinus version \nf3 &lt;- function(r, L){\n  1/r * (1 - exp(-r*L))\n}\n\nf3(0.03, 30)\n\n[1] 19.78101\n\n0.6*f3(0.03, 10)\n\n[1] 5.183636"
  },
  {
    "objectID": "blog/posts/importance-sampling/index.html",
    "href": "blog/posts/importance-sampling/index.html",
    "title": "Importance sampling",
    "section": "",
    "text": "Importance sampling\nImportance sampling is a Monte Carlo method for evaluating properties of a particular distribution, while only having samples generated from a different distribution than the distribution of interest.\nSuppose we want to compute the expectation of an arbitrary function \\(f\\) of a random variable \\(Y\\), which is distributed according to the distribution \\(p\\): \\[ E_p[f(Y)] := \\int f(y) p(y) dy\\]\nIn case the integration becomes difficult, we can use a Monte Carlo method. \\[ E^{MC} := \\frac{1}{N} \\sum_{i=1}^N f(y^{(i)})\\] By the law of large numbers, this estimate will almost surely converge to the true value as the number \\(N\\) of particles (i.e., sampled values) increases.\nAlthough this appears straightforward, sampling from the target distribution, \\(p\\) is not always possible or efficient. Importance sampling bypasses this difficulty by sampling particles from an arbitrary “instrumental distribution” \\(q\\) and weighting the particles by accounting for they were sampled from \\(q\\) but not from \\(p\\).\nImportance sampling fundamental identity\n\\[ E_p[f(Y)] := \\int \\frac{f(y)}{q(y)} q(y) p(y) dy = E_q[w(Y) f(Y)]\\] where we define the importance weight \\(w(y) = \\frac{p(y)}{q(y)}\\)\nLet’s see an example in which we create Gamma-distributed sample from the exponentially distributed sample.\n\nset.seed(42)\nr &lt;- 0.01 # low rate for a wide coverage\nx &lt;- rexp(1e4, rate=r)\n# dgamma(x, shape=3, rate=1) target distribution\nwt = dgamma(x, shape=3, rate=1) / dexp(x, rate=r)\n\nplot(x, wt)\n\n\n\nW = wt/sum(wt)\n\nids &lt;- sample(1:length(x), prob=W, replace=T)\nnewx &lt;- x[ids]\nd &lt;- data.frame(wt=wt, x=x, W=W)\nd &lt;- d[order(x),]\ny = rgamma(length(x), shape=3, rate=1)\n\nd &lt;- data.frame(mean_exp=mean(x),\n                sum_wt_x=sum(W*x),\n                mean_important=mean(newx),\n                true_mean = mean(y))\n\nd\n\n  mean_exp sum_wt_x mean_important true_mean\n1 100.6511  3.01221       3.046269  3.035831\n\n\n\ndf &lt;- data.frame(name=rep(c(\"Target dist\",\"Intrumental dist\",\"Importance sample\"), each=1e4), \n                 value=c(y,x,newx))\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nextrafont::loadfonts()\nggplot(df)+\n  geom_histogram(aes(x=value))+\n  facet_wrap(~name, nrow=1, scales=\"free_x\")\n\n\n\n# ggsave(\"importance_sampling.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "blog/posts/interval-censoring/index.html",
    "href": "blog/posts/interval-censoring/index.html",
    "title": "Estimating serial interval: interval cenoring",
    "section": "",
    "text": "Vanilla maximum likelihood estimation\nSuppose dates of onsets of infectors, \\(t^{A}\\), and infectees, \\(t^{B}\\), are given as specific dates. Then the likelihood function for the serial interval may be written down as follows:\n\\[\\mathcal{L} = \\prod_{i=1}^{n} f(t^{B}_i - t^\n{A}_i)\\] , where \\(f\\) is a probability density function for the serial interval, which we assume follows a Gamma distribution.\n\nset.seed(42)\nn &lt;- 100\nshape_true &lt;- 2.2\nscale_true &lt;- 3.3\n\nonset_infector &lt;- sample(20:30, size=n, replace=TRUE)\nonset_infectee &lt;- onset_infector + rgamma(n, shape=shape_true, scale=scale_true)\nnll &lt;- function(parms, x) -sum(dgamma(x, shape=parms[[1]], scale=parms[[2]], log=TRUE))\nres1 = optim(par=c(1,2), fn=nll, x=onset_infectee - onset_infector, method = \"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\n\n\n\nMLE with interval censoring\nNow suppose that the dates of onset of the infectors are given as intervals. In this case, the above likelihood function may be modified as follows:\n\\[\\mathcal{L} = \\prod_{i=1}^{n} \\int_{t^{A}_{Li}}^{t^{A}_{Ri}} g(\\tau) f(t^{B}_i-\\tau) ~\\text{d}\\tau\\] , where \\(t^A_L, t^A_R, t^B\\) present the times for lower end and upper end of the interval for the time of onset of the infector, and the onset time of the infectee, respectively. \\(g(x)\\) represents the probability density function for the time of symptom onset of the infector, which we assume follows a uniform distribution.\nThis is a simplified version of doubly interval-censored data analysis, which was discussed by Reich et al 2009. The same concept has recently been applied to estimation of serial interval of CVODI-19 by Nishiura et al. 2020. I will cover the doubly interval-censored data in a future post.\n\nset.seed(42)\nL &lt;- - sample(1:5, n, replace=TRUE)\nR &lt;- - 4*L # this will lead to potentially shorter serial interval\nAL &lt;- onset_infector + L\nAR &lt;- onset_infector + R\n\n# x\nnll_interval_censor &lt;- function(parms, AL, AR, t){\n  -sum(log(dunif(AL, min=AL, max=AR)*(pgamma(t-AL, shape=parms[[1]], scale=parms[[2]]) - pgamma(t-AR, shape=parms[[1]], scale=parms[[2]]))))\n}\n\nres2 = optim(par=c(1,2), fn=nll_interval_censor, AL=AL, AR=AR, t=onset_infectee, method = \"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\n\nx1 &lt;- rgamma(1e3, shape=res1$par[[1]], scale=res1$par[[2]])\nx2 &lt;- rgamma(1e3, shape=res2$par[[1]], scale=res2$par[[2]])\nsummary(x1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1677  3.7743  6.2386  7.3706 10.0134 36.0535 \n\nsummary(x2)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.05973  2.01266  3.88932  4.85497  6.77875 20.17723 \n\ndf = data.frame(model=rep(c(\"No censoring\", \"Interval censoring\"), each=1e3), val=c(x1,x2))\n\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\ndf |&gt; ggplot(aes(x=val, fill=model))+\n  geom_density(alpha=0.2) +\n  labs(x=\"value\", y=\"density\")+\n  theme(legend.position = \"bottom\", \n        legend.title = element_blank())"
  },
  {
    "objectID": "blog/posts/kalman_filter/index.html",
    "href": "blog/posts/kalman_filter/index.html",
    "title": "Kalman filter to estimate R using the FKF package",
    "section": "",
    "text": "Arroyo-Marioli et al used a Kalman filter approach to estimate. I tried to reproduce in R. Let’s use a SIR model as was used in my previous post to generate growth rate time series.\n\nlibrary(diffeqr)\nde &lt;- diffeqr::diffeq_setup()\n\nsir_julia &lt;- function(u, p, t){\n  N = sum(u)\n  R = ifelse(t &lt; 20, 2, ifelse(t &lt; 40, 0.9, 1.4)) # R varies \n  p[1] = R * p[2]\n  \n  du1 = - p[1]*u[1]*u[2]/N\n  du2 = + p[1]*u[1]*u[2]/N - p[2]*u[2]\n  du3 = + p[2]*u[2]\n  \n  return(c(du1,du2,du3))\n}\n\nu0 &lt;- c(0.99, 0.01, 0.0)\ntspan &lt;- c(0.0, 50.0)\np &lt;- c(0.4, 0.2)\nprob &lt;- de$ODEProblem(sir_julia, u0, tspan, p)\n\n# prob_jit &lt;- diffeqr::jitoptimize_ode(de, prob)\nsol &lt;- de$solve(prob, de$Tsit5(), saveat=1)\n\nmat &lt;- sapply(sol$u, identity)\nudf &lt;- as.data.frame(t(mat))\ntudf &lt;- cbind(data.frame(t=sol$t), udf)\n\nlibrary(ggplot2)\nggplot(tudf, aes(x=t)) +\n  geom_line(aes(y=V1, color=\"S\")) +\n  geom_line(aes(y=V2, color=\"I\")) +\n  geom_line(aes(y=V3, color=\"R\")) +\n  scale_color_manual(\"\", \n                     values=c(\"S\"=\"steelblue\", \"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n\nGenerate daily incidence assuming 100,000 population\n\nI &lt;- 100000 * (tudf$V3 + 0.01) # true number of infected people at time t\ncase_daily &lt;- rpois(length(I)-1, lambda=diff(I)) # observed number of infected people at t\n\n\\[I_t = (1-\\gamma) I_{t-1} + \\text{new cases at }t\\] \\[\\frac{I_t-I_{t-1}}{I_{t-1}}\\equiv r_t = \\gamma(R_t-1) + \\epsilon_i\\] \\[R_t = R_{t-1} + \\eta_i\\]\n\ngamma &lt;- 0.2 # recovery rate, which is the same as p[2] in the ODE model\nI_hat &lt;- rep(NA, length(case_daily)+1) # true number of infected people at time t\nI_hat[1] &lt;- I[1] # cheating, it's okay as this is the simulation check \nfor (i in 2:length(I_hat)) {\n  I_hat[i] &lt;- (1-gamma)*I_hat[i-1] + case_daily[i-1]\n}\n\nIt &lt;- I_hat # observed number of infected people at t\nn &lt;- length(It)\ngr &lt;- (It[2:n] - It[1:(n-1)]) / It[1:(n-1)] # observed growth rate\nt &lt;- 1:50\nR_true &lt;- ifelse(t &lt; 20, 2, ifelse(t &lt; 40, 0.9, 1.4))\ngr_true &lt;- gamma * (R_true - 1)\nR_hat &lt;- gr / gamma + 1\n\nplot(R_hat, R_true[2:(length(R_hat)+1)], xlim=c(0,3), ylim=c(0,3), \n     xlab=\"R_hat\", ylab=\"R_true\")\nabline(a=0, b=1)\n\n\n\n\nInferring R based on the Kalman filter\n\nlibrary(FKF)\ny &lt;- gr\na0 &lt;- y[1]\nP0 &lt;- matrix(1)\n\ndt &lt;- matrix(0)\nct &lt;- matrix(- gamma)  \nZt &lt;- matrix(gamma)\nTt &lt;- matrix(1)\n\nfit.fkf &lt;- optim(c(HHt = var(y, na.rm = TRUE) * .5,\n                   GGt = var(y, na.rm = TRUE) * .5),\n                 fn = function(par, ...)\n                 -fkf(HHt = matrix(par[1]), GGt = matrix(par[2]), ...)$logLik,\n                 yt = rbind(y), a0 = a0, P0 = P0, dt = dt, ct = ct,\n                 Zt = Zt, Tt = Tt)\n\n# recover values\nHHt &lt;- as.numeric(fit.fkf$par[1])\nGGt &lt;- as.numeric(fit.fkf$par[2])\nHHt; GGt\n\n[1] 0.01137336\n\n\n[1] -1.187671e-05\n\n\n\ny_kf &lt;- fkf(a0, P0, dt, ct, Tt, Zt,\n             HHt = matrix(HHt), GGt = matrix(HHt),\n             yt = rbind(y)) # Kalman filtering\n\ny_ks &lt;- fks(y_kf) # Kalman smoothing\n\n\ndata &lt;- data.frame(x = seq(from = 1, to = 50, by = 1),\n                   y = R_true,\n                   y_hat = R_hat,\n                   y_kf = as.numeric(y_kf$att),\n                   y_ks = as.numeric(y_ks$ahatt),\n                   y_ks_ub = as.numeric(y_ks$ahatt) + 1.96*as.numeric(sqrt(y_ks$Vt)),\n                   y_ks_lb = as.numeric(y_ks$ahatt) - 1.96*as.numeric(sqrt(y_ks$Vt)))\n\nggplot(data, aes(x = x)) +\n  geom_line(aes(y = y_hat)) +\n  geom_line(aes(y = y_kf, color = \"Kalman filter\"), linewidth=1) +\n  geom_line(aes(y = y_ks, color = \"Kalman smooth\"), linewidth=1) +\n  geom_line(aes(y = y_ks_ub, color = \"Kalman smooth\", linetype=\"Upper bound\"), linewidth=1) +\n  geom_line(aes(y = y_ks_lb, color = \"Kalman smooth\", linetype=\"Lower bound\"), linewidth=1) +\n  geom_line(aes(y = y, color = \"True\"), linewidth=1.2) +\n  xlab(\"day\") + ylab(\"reproduction number\") +\n  ggtitle(\"Reproduction number inferred with Kalman Filter\") +\n  theme_bw()+\n  scale_color_manual(\"\", values=c(\"True\"=\"firebrick\",\"Kalman filter\"=\"steelblue\",\n                                  \"Kalman smooth\"=\"forestgreen\"))+\n  scale_linetype_manual(\"\", values=c(\"Upper bound\"=\"dotted\",\n                                     \"Lower bound\"=\"dotted\"))"
  },
  {
    "objectID": "blog/posts/learning_ChatGPT_2/index.html",
    "href": "blog/posts/learning_ChatGPT_2/index.html",
    "title": "Learning ChatGPT 2: Approximating a tower function using a neural net",
    "section": "",
    "text": "The blog post by Stephen Wolfram discusses a neural network for approximating some sort of a step function. This post is my attempt to reproduce it. A YouTube video and a book chapter by Michael Nielsen beautifully explain how a neural network can approximate a tower function while explaining the Universal Approximation Theorem.\n\nData\n\nlibrary(torch)\nx &lt;- seq(0, 3,by=0.01)\ny &lt;- ifelse(x &lt; 1, 0.3, ifelse(x &lt; 2, 0.8, 0.4))\n# input and output are turned into a 1-column tensor  \nx &lt;- torch_tensor(as.matrix(x, ncol=1))\ny &lt;- torch_tensor(as.matrix(y, ncol=1))\nplot(x, y, xlab=\"X\", type=\"l\", ylab=\"Y\", lty=\"dashed\", lwd=2)\n\n\n\n\n\n\nNeural network\nOne simple way to create a neural network is to use nn_sequential function of the torch package. Rectified linear unit (ReLU) is used for an activation function.\n\ndim_in &lt;- 1\ndim_out &lt;- 1\ndim_hidden &lt;- 32\n\nnet &lt;- nn_sequential(\n  nn_linear(dim_in, dim_hidden),\n  nn_relu(),\n  nn_linear(dim_hidden, dim_out)\n)\n\n\n\nTraing a neural network\nThe Adam optimizer, which a popular choice, is used.\n\nopt &lt;- optim_adam(net$parameters)\n# opt &lt;- optim_sgd(net$parameters, lr=0.001)\n\n\nnum_epochs &lt;- 1000\nfor (epoch in 1:num_epochs) {\n  y_pred &lt;- net(x)  # forward pass\n  loss &lt;- nnf_mse_loss(y_pred, y)  # compute loss \n  if (epoch %% 100 == 0) { \n    cat(\"Epoch: \", epoch, \", Loss: \", loss$item(), \"\\n\")\n  }\n  # back propagation \n  opt$zero_grad()\n  loss$backward()\n  opt$step()  # update weights\n}\n\nEpoch:  100 , Loss:  0.05918226 \nEpoch:  200 , Loss:  0.04266065 \nEpoch:  300 , Loss:  0.02848286 \nEpoch:  400 , Loss:  0.01916445 \nEpoch:  500 , Loss:  0.01608658 \nEpoch:  600 , Loss:  0.01477717 \nEpoch:  700 , Loss:  0.01393809 \nEpoch:  800 , Loss:  0.01333173 \nEpoch:  900 , Loss:  0.01286871 \nEpoch:  1000 , Loss:  0.01249429 \n\n\nCompare the data and the model predictions\n\nypred &lt;- net(x)\nsprintf(\"L2 loss: %.4f\", sum((ypred-y)^2))\n\n[1] \"L2 loss: 3.7598\"\n\nplot(x, y, type=\"l\", xlab=\"X\", ylab=\"Y\", lty=\"dashed\", lwd=2)\nlines(x, ypred, lwd=2, col=\"firebrick\")\nlegend(\"topright\", \n  legend=c(\"data\",\"neural net\"), \n  col=c(\"black\",\"firebrick\"), \n  lty= c(\"dashed\",\"solid\"),\n  lwd=2, \n  bty = \"n\", \n  cex = 1.0, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.02,0.02))\n\n\n\n\n\n\nEnlarge the neural network\nLet’s repeat an experiment using a larger network - more nodes and layers\n\ndim_hidden &lt;- 64\n\nnet &lt;- nn_sequential(\n  nn_linear(dim_in, dim_hidden),\n  nn_relu(),\n  nn_linear(dim_hidden, dim_hidden),\n  nn_relu(),\n  nn_linear(dim_hidden, dim_hidden),\n  nn_relu(),\n  nn_linear(dim_hidden, dim_out)\n)\n\n\nopt &lt;- optim_adam(net$parameters)\n\n\nnum_epochs &lt;- 1000\nfor (epoch in 1:num_epochs) {\n  y_pred &lt;- net(x)  # forward pass\n  loss &lt;- nnf_mse_loss(y_pred, y)  # compute loss \n  if (epoch %% 100 == 0) { \n    cat(\"Epoch: \", epoch, \", Loss: \", loss$item(), \"\\n\")\n  }\n  # back propagation \n  opt$zero_grad()\n  loss$backward()\n  opt$step()  # update weights\n}\n\nEpoch:  100 , Loss:  0.02049372 \nEpoch:  200 , Loss:  0.005859586 \nEpoch:  300 , Loss:  0.00293437 \nEpoch:  400 , Loss:  0.001943566 \nEpoch:  500 , Loss:  0.001552111 \nEpoch:  600 , Loss:  0.001466194 \nEpoch:  700 , Loss:  0.00113698 \nEpoch:  800 , Loss:  0.001119171 \nEpoch:  900 , Loss:  0.0009291215 \nEpoch:  1000 , Loss:  0.001042918 \n\n\nCompare the data and the model predictions\n\nypred &lt;- net(x)\nsprintf(\"L2 loss: %.4f\", sum((ypred-y)^2))\n\n[1] \"L2 loss: 0.3239\"\n\n# png(\"stepfunc_nn.png\")\nplot(x, y, type=\"l\", xlab=\"X\", ylab=\"Y\", lty=\"dashed\", lwd=2)\nlines(x, ypred, lwd=2, col=\"firebrick\")\nlegend(\"topright\", \n  legend=c(\"data\",\"neural net\"), \n  col=c(\"black\",\"firebrick\"), \n  lty= c(\"dashed\",\"solid\"),\n  lwd=2, \n  bty = \"n\", \n  cex = 1.0, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.02,0.02))\n\n\n\n# dev.off()"
  },
  {
    "objectID": "blog/posts/logistic_indirect_ve/index.html",
    "href": "blog/posts/logistic_indirect_ve/index.html",
    "title": "Logistic function in R",
    "section": "",
    "text": "Logistic function\nThe logistic function, represented as: \\[\nf(x) = \\frac{L}{1+e^{-k(x-x_0)}}\n\\] , where \\(x_{0}, L\\), and \\(k\\) represent tht value of the function’s midpoint, the supremum of the values of the function, and the logistic growth rate or steepness of the curve. For values of \\(x\\) in the domain of real numbers from \\(-\\infty\\) to \\(+\\infty\\), the S-shaped curve approaching \\(L\\) as \\(x\\) approaches \\(+\\infty\\) and approaching zero as \\(x\\) approaches \\(-\\infty\\).\nThis function was originally devised as a model of population growth and is a versatile tool used in various fields, including epidemiology. In epidemiological contexts, it serves as a mathematical model to describe the growth or decline of infectious diseases within a population over time. See this study, for example, in which the logistic equation was used to decribe the growth of COVID-19 in a population.\nThe standard logistic function, where \\(x_{0}=0, L=1, k=1\\), is sometimes called sigmoid or expit.\nIn the logistic function, the initial stage of growth is approximately exponential (geometric); then, as saturation begins, the growth slows to linear (arithmetic), and at maturity, growth stops. This function may be also used to describe other phenomena and was recently used to model the shape of indirect vaccine efficacy (IVE) in response to the vaccine coverage levels. We are going to explore the use of the logistic function to model the IVE.\n\n\nIndirect effect effectiveness\nIndirect vaccine effectiveness (IVE) is a crucial metric in assessing the impact of vaccination on both vaccinated and unvaccinated individuals within a population. It refers to the protection unvaccinated individuals receive due to the presence of vaccinated individuals in the community.\nTo delve into the dynamics of IVE, we examine data from clinical trials of oral cholera vaccine conducted in Bangladesh and India. These studies provide insights into how varying levels of vaccination coverage affect the incidence rates among unvaccinated individuals residing in the same cluster.\nIn particular, we use the IVE analyzed based on clinical trials of oral cholera vaccines in Bangladesh and in India. In Table 1 below, we analyze the relationship between “Level of vaccine coverage” and the incidence rates among placebo recipients. The presence of IVE is evident as the risk of illness among placebo recipients diminishes with increasing vaccine coverage levels.\n\n\n\nTable 1\n\n\n\nvc_cut &lt;- c(0, 28, 35, 40, 50, 65)\nvc_matlab &lt;- vc_cut[1:5] + diff(vc_cut)/2\n# the data coming from the first year of vaccination\nve_first_year &lt;- 0.65\neff_vc_matlab &lt;- ve_first_year * vc_matlab \nir &lt;- c(7.01, 5.87, 4.72, 4.65, 1.47)\n# lowest vaccine coverage group has no indirect effect\nive_matlab &lt;- (ir[1]-ir)/ir[1]*100\n# Ali (2013) Kolkata, India\n# Data from 3-year follow-up. Total protective efficacy remained high (66%)\n# therefore, effective vaccine coverage still calculated as we did for\n# Matlab, Bangladesh data\n# Out of 107,347 eligible residents, 66,990 received 2 doses\nvc_cut &lt;- c(0, 25, 28, 31, 34, 100*66990/107347)\nvc_kolkata &lt;- vc_cut[1:5] + diff(vc_cut)/2\nve_first_year &lt;- 0.65 # the data coming from the first year of vaccination\neff_vc_kolkata &lt;- ve_first_year * vc_kolkata\nir &lt;- c(5.54, 5.64, 2.48, 2.25, 1.93)\nive_kolkata &lt;- (ir[1]-ir)/ir[1]*100\n\nive_dat &lt;- data.frame(eff_vacc_cov = c(eff_vc_kolkata, eff_vc_matlab), \n                      indirect_vacc_eff = c(ive_kolkata, ive_matlab))\nive_dat &lt;- ive_dat / 100\nive_dat$type &lt;- \"data\"\nive_dat$location &lt;- c(rep(\"Kolkata\", length(eff_vc_kolkata)), rep(\"Matlab\",length(eff_vc_matlab)))\n\nHere, we assumed that the incidence rate among in the unvaccinated population, (i.e., no one was vaccinated) would be the same as the incidence rate observed in the lowest level of vaccine coverage rate, which would be 14% for Matlab and 12.5% for Kolkata. Because of this assumption, we may be underestimating the IVE. While it may be possible to reduce effective vaccine coverage rates by taking the difference between the effective vaccine coverage rates presented here and the lowest vaccine coverage rates (i.e., 14% or 12.5%), I wanted to be rather conservative than overestimating the impact.\nNow we develop the logistic equation and fit the equation to the data. Since we know that the IVE has to be between \\(0\\) and \\(1\\), we set \\(L\\) at 1.\n\nfm &lt;- indirect_vacc_eff ~ 1 / (1 + exp(-k*(eff_vacc_cov-x0)))\nfit &lt;- nls(formula=fm, data=ive_dat, start=list(k=12, x0=0.27))\npredict_logis &lt;- function(x) {\n  1 / (1 + exp(-coef(fit)[1]*(x-coef(fit)[2])))\n}\nnewx &lt;- seq(0,1,by=0.01)\nnewy &lt;- predict_logis(newx)\nd &lt;- rbind(ive_dat, data.frame(eff_vacc_cov=newx, \n                               indirect_vacc_eff=newy,\n                               type=\"Model1\", location=NA))\n\nR has a self-starting logistic function, which can be helpful in case nls function is not successful for finding the parameter values with the given staring values.\n\nfit_SSlogis &lt;- nls(indirect_vacc_eff ~ SSlogis(eff_vacc_cov, Asym, xmid, scal), \n                   ive_dat)\nAsym &lt;- coef(fit_SSlogis)[\"Asym\"]\nxmid &lt;- coef(fit_SSlogis)[\"xmid\"]\nscal &lt;- coef(fit_SSlogis)[\"scal\"]\n## SSlogis has a different convention for the terms\npred_SSlogis &lt;- function(x){\n  Asym / (1 + exp((xmid - x)/scal))\n}\nnewx &lt;- seq(0,1,by=0.01)\nnewy &lt;- pred_SSlogis(newx)\nd &lt;- rbind(d, data.frame(eff_vacc_cov=newx, \n                               indirect_vacc_eff=newy,\n                               type=\"SSmodel\", location=NA))\n\nFinally, we can force the logistic function to converge to 1 when the effective vaccine coverage reaches 1 and to 0 when the effective vaccine coverage is 0.\n\ndat &lt;- data.frame(x=ive_dat$eff_vacc_cov, y=ive_dat$indirect_vacc_eff)\n# fm &lt;- indirect_vacc_eff ~ 1 / (1 + exp(-k*(eff_vacc_cov-x0)))\nfm2 &lt;- y ~ 1 / (1+((1-x)*x0/(x*(1-x0)))^k)\nfit2 &lt;- nls(formula=fm2, data=dat, start=list(k=1, x0=0.1))\npredict_logis2 &lt;- function(x) {\n  k &lt;- coef(fit2)[[\"k\"]]\n  x0 &lt;- coef(fit2)[[\"x0\"]]\n  1 / (1+((1-x)*x0/(x*(1-x0)))^k)\n}\n\nnewx &lt;- seq(0,1,by=0.01)\nnewy &lt;- predict_logis2(newx)\nd &lt;- rbind(d, data.frame(eff_vacc_cov=newx, \n                               indirect_vacc_eff=newy,\n                               type=\"Model2\", location=NA))\n\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot() +\n  geom_point(data=subset(d, type==\"data\"),\n  aes(eff_vacc_cov, indirect_vacc_eff, color=location))+\n  geom_line(data=subset(d, type==\"Model1\"), \n            aes(eff_vacc_cov, indirect_vacc_eff), color=\"steelblue\") +\n  geom_line(data=subset(d, type==\"Model2\"), \n            aes(eff_vacc_cov, indirect_vacc_eff), color=\"firebrick\") +\n  geom_line(data=subset(d, type==\"SSmodel\"), \n            aes(eff_vacc_cov, indirect_vacc_eff), color=\"forestgreen\")+\n  labs(y=\"Indirect vaccine efficacy\", x=\"Effective vaccine coverage\", color=\"\")\n\n\n\n # ggsave(\"ive_vacc_cov.png\")"
  },
  {
    "objectID": "blog/posts/modeling-philosophy/index.html",
    "href": "blog/posts/modeling-philosophy/index.html",
    "title": "Modeling philosophy",
    "section": "",
    "text": "감염병 전파를 이해하는 데에는 수리 모형 (model)이 아주 중요한 역할을 한다. 그런데 현실과는 모형을 통해서 어떻게 현실에 대해 배울 수 있을까? 스탠포드 철학 백과사전에 이러한 내용을 담고 있는 부분이 있어 여기에 정리를 해본다. 아직 만족한 만한 해답을 찾지는 못했다.\n철학자들은 모형을 만들고 (building), 조작하고 (manipulation), 적용하고 (application), 평가함 (evaluation)으로써 현실에 대해 추론 (reasoning) 을 할 수 있다고 기술하고 있는 것 같다. 이런 과정을 “surrogative reasoning” 혹은 “model-based reasoning” 이라는 용어로 표현하기도 했다. 추론 과정은 모형 자체를 이해하는 과정과 그 모형에 대한 이해를 현실에 적용하는 과정 두 가지로 나누어 볼 수 있다. 모형을 만드는 과정은 모형의 여러 부분들이 어떻게 서로 맞아 들어가는 지 알게 되는 과정이다. 모형을 조작하는 과정은 모형을 모수(parameter)를 변화시켜가며 시물레이션을 통해 그 결과를 확인하는 과정이 될 것이다. 모형을 만들고 조작함으로써 알게된 지식을 어떻게 그 대상인 (target system)인 현실의 언어로 변역할 수 있을까? 모형이 현실의 일부를 나타내도록 (represent) 만들어 졌다면 즉 다시 말해 우리가 알고자 하는 현실 (예를 들면 백신 접종 시에 감염자수의 감소 정도)에 상응하는 부분이 모형에 구현되어 있다면 모형을 통해서 얻은 지식이 현실에 적용될 수 있을 것 같다. 그런데 모형에 구현된 부분이 현실을 잘 반영하는지를 어떻게 알 수 있을까?"
  },
  {
    "objectID": "blog/posts/nb-regression-optim/index.html",
    "href": "blog/posts/nb-regression-optim/index.html",
    "title": "Regression using optim",
    "section": "",
    "text": "Data\nI will use the cars data with give the speed of cars and the distances taken to stop.\n\nd &lt;- datasets::cars\nm &lt;- lm(dist ~ speed, data=d)\nsummary(m)\n\n\nCall:\nlm(formula = dist ~ speed, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\n\n\nPlot\nPlot estimates with confidence and prediction intervals\n\npred &lt;- predict(m, interval=\"prediction\", level=0.95) # prediction interval\nconf &lt;- predict(m, interval=\"confidence\", level=0.95) # confidence interval\n\nmdat &lt;- m$model\nmdat$pred_estimate &lt;- pred[,1]\nmdat$pred_lb &lt;- pred[,2]\nmdat$pred_ub &lt;- pred[,3]\nmdat$conf_estimate &lt;- conf[,1]\nmdat$conf_lb &lt;- conf[,2]\nmdat$conf_ub &lt;- conf[,3]\n\nmdat$residuals &lt;- residuals(m)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\npltcar &lt;- mdat %&gt;% \n  ggplot(aes(speed, dist))+\n  # geom_point(aes(size = abs(m$residuals)))+\n  geom_point(aes(color=\"Data\"), size = 1)+\n  geom_line(aes(y=pred_estimate, color=\"Model\"))+\n  geom_ribbon(aes(ymax=pred_ub, ymin=pred_lb, fill=\"Model\"), alpha=0.2)+\n  # geom_line(aes(y=conf_estimate), color=\"steelblue\")+\n  geom_ribbon(aes(ymax=conf_ub, ymin=conf_lb, fill=\"Model\"), alpha=0.5)+\n  scale_color_manual(\"\", values=c(\"Data\"=\"firebrick\"))+\n  scale_fill_manual(\"\", values=c(\"Model\"=\"steelblue\"))+\n  labs(x=\"Speed\", y=\"Distance\", title=\"Speed and Stopping Distances of Cars\")+\n  theme(legend.position=\"bottom\")\n\n# ggsave(\"plot_car.png\", pltcar)\n\npltcar\n\n\n\n\n\n\noptim function\nNow let’s take an alternative approach to write down the likelihood function and maximize it using the optim function\n\n# define our likelihood function we like to optimize\nnegloglik &lt;- function(par, y, X){\n  sigma &lt;- par[1]\n  beta &lt;- par[-length(par)]\n  mu &lt;- X %*% beta \n  - sum(dnorm(y, mean=mu, sd=sigma, log=TRUE), na.rm=T)\n}\n\nX = model.matrix(m)\ninit = c(coef(m), sigma=summary(m)$sigma)\n# check\n# negloglik(par=init, y=d$y, X=X)\nfit &lt;- optim(par=init, \n             fn=negloglik, \n             y=d$dist, \n             X=X, \n             control=list(reltol=1e-6))\n\nLet’s compare the results.\n\nfit$par\n\n(Intercept)       speed       sigma \n -17.579095    3.932409   15.379587 \n\ncoef(m)\n\n(Intercept)       speed \n -17.579095    3.932409"
  },
  {
    "objectID": "blog/posts/nimble/index.html",
    "href": "blog/posts/nimble/index.html",
    "title": "SEIR model using the Nimble pacakge",
    "section": "",
    "text": "감염병 수리 모형을 개발하는 데 있어 가장 근본적인 질문 중 하나는 주어진 관찰값 (시계열)하에서 어떤 모형을 선택하고 그 모수의 값을 어떻게 결정하는가이다. 모형을 선택하는 과정은 따로 다루기로 하고 여기서는 일반적으로 사용되는 감염병 수리 모형 (i.e., SIR)을 사용할 때 모수를 추정하는 과정에 대해서 이야기해보자. 최대 가능도 (maximum likelihood) 방법에 대해서는 전에 언급하였다. 모수를 추정하는 여러 방법 중에 마르코프 연쇄 몬테카를로 (Markov Chain Monte Carlo; MCMC) 방법이 적절한 모수의 값을 찾아내고 그 값의 불확실성 (uncertainty)를 나타내는 데 가장 널리 쓰이는 방법 중의 하나이다. MCMC 알고리듬을 직접 작성해서 사용한는 것도 원리를 이해하는 데에는 도움이 되지만 이미 다양한 통계 패키지에서 MCMC가 사용되고 있으므로 기존 패키지를 사용하는 것도 합리적인 방법이 될 수 있다. 회귀 분석 등 통계모형의 경우BUGS (Bayesian Inference Using Gibbs Sampling) 혹은 JAGS (Just Another Gibbs Sampler), Stan, 그리고 NIMBLE 등에 구현된 MCMC를 사용하는 것이 많이 보편화 되어 있다.\n이 중 NIMBLE 은 R 패키지 nimble을 이용해서 사용할 수 있고 패키지에서 제공하는 함수 기능을 이용해서 감염병 수리 모형을 구현하고 MCMC 까지 할 수 있다. syntax 또한 R과 유사해서 R를 사용하는 사람에게는 Stan 보다 더 접근이 용이한 것 같다. 아래에는 nimble 함수 기능을 이용하여 Euler 방법에 기반한 SIR 모형을 구현한 예이다.\n\nlibrary(nimble)\nsir_incidence &lt;- nimbleFunction(\n  run = function(beta = double(0)) {\n    tend &lt;- 100 # 100 days of simulation\n    dt &lt;- 0.1 # time step of 0.1 day\n    \n    # initial condition\n    St &lt;- 999 \n    It &lt;- 1\n    Rt &lt;- 0\n    CIt &lt;- 0\n    \n    # create vectors for the state variables\n    S &lt;- rep(0, tend)\n    I &lt;- rep(0, tend)\n    R &lt;- rep(0, tend) \n    CI &lt;- rep(0, tend) # cumulative incidence\n    \n    # first elements of the vectors are initial conditions\n    S[1] &lt;- St\n    I[1] &lt;- It\n    R[1] &lt;- Rt\n    CI[1] &lt;- CIt\n  \n    gamma &lt;- 0.2 # 1/gamma = duration of infectiousness\n    \n    for (i in 2:tend) { # each day\n      for (j in 1:ceiling(1/dt)) { # time steps per day\n        Nt &lt;- St + It + Rt                     # total population size\n        rate_StoI &lt;- St * beta * It / Nt * dt  # transition rate from S to I\n        rate_ItoR &lt;- gamma * It * dt           # transition rate from I to R\n  \n        dS &lt;- - rate_StoI            # rate of change for S\n        dI &lt;- rate_StoI - rate_ItoR  # rate of change for I\n        dR &lt;- rate_ItoR              # rate of change for R\n        dCI &lt;- rate_StoI             # rate of change for cumulative incidence\n      \n        St &lt;- St + dS                # update the St\n        It &lt;- It + dI                # update the It \n        Rt &lt;- Rt + dR                # update the Rt\n        CIt &lt;- CIt + dCI             # update the CIt\n      }\n      S[i] &lt;- St                     # put St in the vector \n      I[i] &lt;- It                     # put It in the vector \n      R[i] &lt;- Rt                     # put Rt in the vector \n      CI[i] &lt;- CIt                   # put CIt in the vector \n   }\n   # daily incidence from cumulative incidence\n   inc &lt;- CI[2:tend] - CI[1:(tend-1)] \n   return(inc) \n   returnType(double(1)) # return type\n }\n)\n\n모수 추정을 위해서 푸아송 분포를 이용하여 거짓 관찰값 (Y) 을 만들어보자.\n\n# create observation\nbeta &lt;- 0.4 # true beta\nX &lt;- sir_incidence(beta) # true daily incidence\nY &lt;- rpois(length(X), lambda=X) # Poisson-distributed observation\n\n아래와 같이 prior distribution, likelihood, 그리고 posterior predictive check위해서 ypred 도 함께 구현한다.\n\n# BUGS style code\ncode &lt;- nimbleCode({\n  beta ~ T(dnorm(0, sd = 2), 0, 2)   # prior for beta truncated at 0 and 2\n  mu[1:N] &lt;- sir_incidence(beta)     # daily incidence from the model\n  for (i in 1:N) {\n    y[i] ~ dpois(mu[i])              # likelihood\n    ypred[i] ~ dpois(mu[i])          # posterior predictive value\n  }\n})\n\n아래와 같이 초기 조건을 설정하고 모형을 구성한다. 빠른 실행을 위해서 컴파일 한다.\n\n# constants, data, and initial values\nconstants &lt;- list(N = length(Y))      # number of observation\ndata &lt;- list(y = Y)                   # observation\ninits &lt;- list(beta = 0.1)             # starting point for beta  \n\n# create the model object\nsir_model &lt;- nimbleModel(code = code,\n                         constants = constants,\n                         data = data,\n                         inits = inits,\n                         check = FALSE)\n\nsirMCMC &lt;- buildMCMC(sir_model, monitors=c('beta','ypred'))\nCsir &lt;- compileNimble(sir_model)\nCsirMCMC &lt;- compileNimble(sirMCMC, project=Csir)\n\n# thining interval was chosen based on previous analyses of ACF\nsamples &lt;- runMCMC(CsirMCMC, niter=5000, thin=10, nburnin=1000)\n# saveRDS(samples, \"samples_nimble_20231125e.rds\")\n\n\nsamples &lt;- readRDS(\"samples_nimble_20231125e.rds\")\nplot(samples[,1], type=\"l\", ylab=expression(beta), xlab=\"Iterations\")\n\n\n\n\n\\(\\beta\\) 의 posterior distribution과 거짓 자료를 만들기 위해 사용했던 \\(\\beta\\)값 (빨간색)을 비교해보자.\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nsamples |&gt; \n  as.data.frame() |&gt; \n  ggplot()+\n  geom_histogram(aes(x=beta, fill=\"posterior\"))+\n  geom_vline(aes(xintercept=0.4, color=\"true\"), linewidth=1.2)+\n  labs(x=expression(beta), y=\"frequency\")+\n  scale_fill_manual(\"\", values=c(\"posterior\"=\"grey\"))+\n  scale_color_manual(\"\", values=c(\"true\"=\"firebrick\"))+\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nPosterior predictive check\n\n# posterior predictive check\nnsamp &lt;- nrow(samples)\ndf &lt;- data.frame(time=rep(1:99, nsamp+1), \n                 name=c(rep(1:nsamp, each=99),rep(\"data\",99)))\ndf$value &lt;- c(c(t(samples[,2:100])), Y)\n\n\nlibrary(dplyr)\nggplot(df)+\n  geom_line(data=filter(df, name!=\"data\"), aes(x=time, y=value, group=name,\n            color=\"posterior predictive values\"))+\n  geom_point(data=filter(df, name==\"data\"), aes(x=time, y=value, color=\"data\"),\n             size=1.2) +\n  labs(x='Time (day)', y='Daily infected')+\n  scale_color_manual(\"\",values=c(\"posterior predictive values\"=\"grey\",\n                                 \"data\"=\"firebrick\"))+\n  theme(legend.position = \"bottom\")\n\n\n\n# ggsave(\"nimble_ppc_incidence.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "blog/posts/odin/index.html",
    "href": "blog/posts/odin/index.html",
    "title": "odin package",
    "section": "",
    "text": "이번 Vaccine Impact Modeling Consoritum (VIMC) 연례회의에서 odin이라는 패키지에 대해 알게 되었다. deSolve의 업그레이드 버전이라고 보면 될까? R 코드를 C언어로 컴파일하기 때문에 최종 모형의 구동속도가 빠르다. 따라서 모형을 여러번 돌려야 하는 경우 (예를 들어 MCMC) 에 유리하다. pomp 보다도 훨씬 더 빠르다고 했는데 정확한 비교 수치는 잘 기억이 안남. 종종 C++로 모형을 만들었는데 odin 패키지를 사용하면 훨씬 쉬워질 것 같다. 좀 더 살펴보아야 할 텐데 일단 잊지 않기 위해 간단히 SIR 모형만 만들어 보았다.\n\nDeterministic SIR model\n\npath_sir_model &lt;- \"C:/Users/jonghoon.kim/Documents/myblog/blog/posts/odin/sir.R\"\nwriteLines(readLines(path_sir_model))\n\n## Core equations for transitions between compartments:\nupdate(S) &lt;- S - beta * S * I / N\nupdate(I) &lt;- I + beta * S * I / N - gamma * I\nupdate(R) &lt;- R + gamma * I\n\n## Total population size (odin will recompute this at each timestep:\n## automatically)\nN &lt;- S + I + R\n\n## Initial states:\ninitial(S) &lt;- S_ini # will be user-defined\ninitial(I) &lt;- I_ini # will be user-defined\ninitial(R) &lt;- 0\n\n## User defined parameters - default in parentheses:\nS_ini &lt;- user(1000)\nI_ini &lt;- user(1)\nbeta &lt;- user(0.2)\ngamma &lt;- user(0.1)\n\n\nRun the model and plot the results\n\nlibrary(odin)\nsir_generator &lt;- odin::odin(path_sir_model)\n\n\nx &lt;- sir_generator$new()\n# see what the object is like\n# x\nsir_col &lt;- c(\"#8c8cd9\", \"#cc0044\", \"#999966\")\nx_res &lt;- x$run(0:200)\n\npar(mar = c(4.1, 5.1, 0.5, 0.5), las = 1)\nmatplot(x_res[, 1], x_res[, -1], xlab = \"Time\", ylab = \"Number of individuals\",\n        type = \"l\", col = sir_col, lty = 1)\nlegend(\"topright\", lwd = 1, col = sir_col, legend = c(\"S\", \"I\", \"R\"), bty = \"n\")\n\n\n\n\n\n\nStochastic SIR model\n\npath_sir_stoch_model &lt;- \"C:/Users/jonghoon.kim/Documents/myblog/blog/posts/odin/sir_stoch.R\"\nwriteLines(readLines(path_sir_stoch_model))\n\n## Core equations for transitions between compartments:\nupdate(S) &lt;- S - n_SI\nupdate(I) &lt;- I + n_SI - n_IR\nupdate(R) &lt;- R + n_IR\n\n## Individual probabilities of transition:\np_SI &lt;- 1 - exp(-beta * I / N) # S to I\np_IR &lt;- 1 - exp(-gamma) # I to R\n\n## Draws from binomial distributions for numbers changing between\n## compartments:\nn_SI &lt;- rbinom(S, p_SI)\nn_IR &lt;- rbinom(I, p_IR)\n\n## Total population size\nN &lt;- S + I + R\n\n## Initial states:\ninitial(S) &lt;- S_ini\ninitial(I) &lt;- I_ini\ninitial(R) &lt;- 0\n\n## User defined parameters - default in parentheses:\nS_ini &lt;- user(1000)\nI_ini &lt;- user(1)\nbeta &lt;- user(0.2)\ngamma &lt;- user(0.1)\n\n\nRun the model and plot the results\n\nsir_generator &lt;- odin::odin(path_sir_stoch_model)\n\n\nset.seed(42)\nx &lt;- sir_generator$new()\nx_res &lt;- x$run(0:200)\npar(mar = c(4.1, 5.1, 0.5, 0.5), las = 1)\nmatplot(x_res[, 1], x_res[, -1], xlab = \"Time\", ylab = \"Number of individuals\",\n        type = \"l\", col = sir_col, lty = 1)\nlegend(\"topright\", lwd = 1, col = sir_col, legend = c(\"S\", \"I\", \"R\"), bty = \"n\")"
  },
  {
    "objectID": "blog/posts/parallel/index.html",
    "href": "blog/posts/parallel/index.html",
    "title": "Parallel simulation in R",
    "section": "",
    "text": "I find that parallel, doParallel and foreach packages provide the easiest approach for parallel computing in R. The doParallel vignette provides a great overview. library(doParallel) command automatically loads required packages.\n\nlibrary(doParallel)\nncores &lt;-  detectCores()\ncl &lt;- makeCluster(getOption(\"cl.cores\", ncores/2))\nregisterDoParallel(cl)\nnruns &lt;- ncores\n\nx &lt;- 2\nres &lt;- foreach(i=1:4, .combine=c) %dopar% { c(i, x^2) }\n\nstopCluster(cl)\nres\n\n[1] 1 4 2 4 3 4 4 4\n\n\nWe can export packages.\n\ncl &lt;- makeCluster(getOption(\"cl.cores\", ncores/2))\nregisterDoParallel(cl)\n\nres &lt;- \n  foreach(i=1:5, .packages=c(\"data.table\"), .combine=cbind) %dopar% {\n    frollmean(rnorm(10), 7)\n  }\n\nstopCluster(cl)\nres\n\n         result.1   result.2   result.3  result.4     result.5\n [1,]          NA         NA         NA        NA           NA\n [2,]          NA         NA         NA        NA           NA\n [3,]          NA         NA         NA        NA           NA\n [4,]          NA         NA         NA        NA           NA\n [5,]          NA         NA         NA        NA           NA\n [6,]          NA         NA         NA        NA           NA\n [7,]  0.10610803 -0.2996241 0.03278228 0.4701361 -0.004181703\n [8,]  0.05807023 -0.3114088 0.20836936 0.3251219  0.400871629\n [9,]  0.14870568 -0.3538062 0.35330620 0.3301056  0.293785357\n[10,] -0.01413357 -0.4768548 0.19839694 0.2946405 -0.029407561\n\n\nWe may want to set the same seed for each worker.\n\ncl &lt;- makeCluster(getOption(\"cl.cores\", ncores/2))\nregisterDoParallel(cl)\n\nres &lt;- \n  foreach(i=1:5, .packages=c(\"data.table\"), .combine=cbind) %dopar% {\n    set.seed(12)\n    frollmean(rnorm(10), 7)\n}\nstopCluster(cl)\nres\n\n        result.1   result.2   result.3   result.4   result.5\n [1,]         NA         NA         NA         NA         NA\n [2,]         NA         NA         NA         NA         NA\n [3,]         NA         NA         NA         NA         NA\n [4,]         NA         NA         NA         NA         NA\n [5,]         NA         NA         NA         NA         NA\n [6,]         NA         NA         NA         NA         NA\n [7,] -0.6236335 -0.6236335 -0.6236335 -0.6236335 -0.6236335\n [8,] -0.5018746 -0.5018746 -0.5018746 -0.5018746 -0.5018746\n [9,] -0.7423937 -0.7423937 -0.7423937 -0.7423937 -0.7423937\n[10,] -0.5445709 -0.5445709 -0.5445709 -0.5445709 -0.5445709\n\n\nDifferent foreach sessions with the same random seed are not reproducible.\n\ncl &lt;- makeCluster(getOption(\"cl.cores\", ncores/2))\nregisterDoParallel(cl)\n\nset.seed(12)\na1 &lt;- foreach(i=1:2, combine=cbind) %dopar% { rnorm(5) }\nb1 &lt;- foreach(i=1:2, combine=cbind) %dopar% { rnorm(5) }\nset.seed(12)\na2 &lt;- foreach(i=1:2, combine=cbind) %dopar% { rnorm(5) }\nb2 &lt;- foreach(i=1:2, combine=cbind) %dopar% { rnorm(5) }\nidentical(a1, a2) && identical(b1, b2)\n\n[1] FALSE\n\nstopCluster(cl)\n\nReproducibility across different foreach sessions are possible using doRNG package. Examples below were adapted from the stackoverflow post.\nYou use %dorng% instead of %dopar% in the doRNG approach\n\nlibrary(doRNG)\n\ncl &lt;- makeCluster(getOption(\"cl.cores\", ncores/2))\nregisterDoParallel(cl)\n\nset.seed(12)\na1 &lt;- foreach(i=1:2, combine=cbind) %dorng% { rnorm(5) }\nb1 &lt;- foreach(i=1:2, combine=cbind) %dorng% { rnorm(5) }\n\nset.seed(12)\na2 &lt;- foreach(i=1:2, combine=cbind) %dorng% { rnorm(5) }\nb2 &lt;- foreach(i=1:2, combine=cbind) %dorng% { rnorm(5) }\nidentical(a1, a2) && identical(b1, b2)\n\n[1] TRUE\n\nstopCluster(cl)\n\nWe may want to examine parallel workers.\n\ncl &lt;- makeCluster(getOption(\"cl.cores\", ncores/2))\nregisterDoParallel(cl)\ngetDoParWorkers()\n\n[1] 10\n\ngetDoParName()\n\n[1] \"doParallelSNOW\"\n\ngetDoParVersion()\n\n[1] \"1.0.17\"\n\nstopCluster(cl)"
  },
  {
    "objectID": "blog/posts/particle_filter_COVID19/index.html",
    "href": "blog/posts/particle_filter_COVID19/index.html",
    "title": "Estimating the instantaneous reproduction number using the particle filter",
    "section": "",
    "text": "A simple particle filter in R\n파티클 필터 (particle filter) 를 이용하여 잠재 변수 (latent variable)를 추정하는 과정을 지난 글에서 다루었다. 관찰값들이 코로나 19 일별 감염자일때 감염병 수리 모형을 이용하여 일별 감염재생산지수 (\\((R_t\\)) 를 추정한다. 아래 글은 2020년 Kucharski et al. 논문에 사용되었던 방법을 차용하였다. 이해를 돕기 위해 모형을 단순화 하였고 가상의 데이타를 만들어 내는 과정을 더하였다. 우선 SEIR 모형을 이용해서 가상의 데이타 (일별 감염자 수)를 만든다. 누적 감염자 (cumulative incidence) 를 나타내는 CI라는 변수의 일별 차이를 계산하여 일별 감염자 수를 계산한다. 보통의 SEIR 모형에서는 \\(\\beta\\)가 상수로 취급 되지만 아래 모형에서는 일별 감염 재생산지수 \\(R_t = \\beta (t) \\times D\\) \\(D\\)는 감염 기간)가 방역 정책, 활동 변화 등 이유로 인해 시간에 따라 변화한다고 가정하기 때문에 시간에 따른 함수 \\(\\beta(t)\\)로 표현한다. 우리가 추정 하고자 하는 \\(R_t\\)를 미리 정의하고 이로 부터 \\(\\beta(t)\\) 를 계산하고 이를 SEIR 모형에 적용하여 가상의 데이타를 만든다.\n아래와 같은 방식으로 SEIR 모형을 만든다. 본래 미분식으로 정의하고 deSolve 패키지의 ode 함수 등을 이용하여 적분할 수 있으나 이 글에서는 간단하게 Euler 방법을 사용한다.\n\nSEIR_Euler &lt;- function (params = NULL,\n                        y = NULL,\n                        tbegin = 0,\n                        tend = 1,\n                        dt = 0.2) {\n  \n  M &lt;- matrix(NA, nrow=(tend-tbegin+1), ncol=length(y)) # output matrix\n  M[1,] &lt;- y # initial values for the first row\n  \n  S &lt;- y[1]; E &lt;- y[2]; I &lt;- y[3]; R &lt;- y[4]; CI &lt;- y[5]\n  N &lt;- S + E + I + R\n  epsilon &lt;- params[[\"epsilon\"]]\n  gamma &lt;- params[[\"gamma\"]]\n  Rt &lt;- params[[\"Rt\"]] # daily reproduction number\n  \n  for (t in seq(tbegin, tend, by=1)) { # for each day\n    for (i in seq(dt, 1, dt)) { # sub-intervals that can vary\n      # beta is already adjusted by N \n      # t is not an integer\n      beta &lt;- Rt[floor(t+1+dt)] * gamma # transmission rate\n      S_to_E &lt;- beta * I * dt\n      E_to_I &lt;- E * epsilon * dt\n      I_to_R &lt;- I * gamma * dt\n      \n      # update state variables\n      S &lt;- S - S_to_E\n      E &lt;- E + S_to_E - E_to_I\n      I &lt;- I + E_to_I - I_to_R\n      R &lt;- R + I_to_R\n      CI &lt;- CI + S_to_E\n    }\n    # output for each day\n    M[t+1, 1] &lt;- S \n    M[t+1, 2] &lt;- E\n    M[t+1, 3] &lt;- I\n    M[t+1, 4] &lt;- R\n    M[t+1, 5] &lt;- CI\n  }\n  return(M)\n}\n\n일별 감염자 수를 플롯해본다.\n\n# pre-defined Rt\nRt_true &lt;- c(rep(1.2, 15), 0.5*sin(0.1*pi*0:32) + 1.2, rep(0.9, 100))\nI0 &lt;- 100 # initially infected people\ny0 &lt;- c(S = 1e7-I0, E = 0, I = I0, R = 0, CI = 0) # initial values for state variables\nparams &lt;- list() # input parameters for the SEIR model\nparams$Rt &lt;- Rt_true\nparams$epsilon &lt;- 0.5 # 1/epsilon = latent period\nparams$gamma &lt;- 0.2 # 1/gamma = duration of infectiousness\ntend &lt;- 50 # simulation end time 50 days\n\nres1 &lt;- SEIR_Euler(params = params, y=y0, tend=50) # run the model\nres1 &lt;- as.data.frame(res1)\nres1$daily_infected &lt;- c(0, diff(res1$V5))\nres1$time &lt;- 0:tend\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(res1, aes(x = time, y = daily_infected)) +\n  geom_line(size = 1.2) +\n  labs(x = 'Time (day)', y = 'Daily infected')\n\n\n\n\n푸아송 분포를 이용하여 가상의 데이타를 만든다.\n\n# Create the data assunming observations are poisson random variable\nset.seed(42)\nfakedata &lt;- data.frame(daily_infected = rpois(nrow(res1), lambda = res1$daily_infected))\n\n일별 변화를 계산하는 SEIR 전파 모형, 행의 수는 파티클 수와 같다.\n\n# stochastic differential equation (with beta(t) moves according to a geometric Brownian motion) are modeled using the Euler-Maruyama method.\n# daily change is modeled using the subinterval dt\nSEIR_step &lt;- function (params = NULL,\n                       y = NULL,\n                       tbegin = 0,\n                       tend = 1,\n                       dt = 0.2,\n                       beta = NULL) {\n  # daily infection reset to zero to hold values from tbegin to tend\n  y[, c(\"CI\")] &lt;- 0\n  \n  S &lt;- y[, \"S\"]\n  E &lt;- y[, \"E\"]\n  I &lt;- y[, \"I\"]\n  R &lt;- y[, \"R\"]\n  daily_infected &lt;- y[, \"CI\"]\n  \n  N &lt;- S + E + I + R\n  epsilon &lt;- params[[\"epsilon\"]]\n  gamma &lt;- params[[\"gamma\"]]\n  \n  for (i in seq((tbegin + dt), tend, dt)) {\n    # beta is already assumed to be adjusted by N such that it can\n    # be translated to Rt by multiplying the duration of infectiousness\n    S_to_E &lt;- beta * I * dt\n    E_to_I &lt;- E * epsilon * dt\n    I_to_R &lt;- I * gamma * dt\n    # Process model for SEIR\n    S &lt;- S - S_to_E\n    E &lt;- E + S_to_E - E_to_I\n    I &lt;- I + E_to_I - I_to_R\n    R &lt;- R + I_to_R\n    daily_infected &lt;- daily_infected + S_to_E\n  }\n  y[, \"S\"] &lt;- S\n  y[, \"E\"] &lt;- E\n  y[, \"I\"] &lt;- I\n  y[, \"R\"] &lt;- R\n  y[, \"CI\"] &lt;- daily_infected\n  \n  return(y)\n}\n\n파티클 필터링 함수\n\npfilter &lt;- function (params, # parameters\n                     y, # initial values of state variables\n                     data, # input data set\n                     npart = 1000, # number of particles\n                     tend = NULL, # simulation stop time\n                     dt = 0.2) {\n  \n  # Assumptions - using daily growth rate\n  nstatevar &lt;- length(y) # number of state variables\n  if(is.null(tend)) {\n    tend = nrow(data)\n  }\n  # to store state variables\n  latent_var &lt;- array(0,\n                      dim = c(npart, tend, nstatevar),\n                      dimnames = list(NULL, NULL, names(y)))\n  # latent_var[, 1, ] &lt;- y\n  for (nm in names(y)) { # initial value\n    latent_var[, 1, nm] &lt;- y[[nm]]\n  }\n  ## parameters \n  gamma &lt;- params[[\"gamma\"]]\n  beta0 &lt;- params[[\"R0\"]] * gamma\n  beta_sd &lt;- params[[\"betavol\"]]\n  beta &lt;- matrix(rnorm(npart * tend, mean = 0, sd = beta_sd), nrow = tend)\n  beta[1,] &lt;- beta0 # this is updated at t=2\n  \n  wt &lt;- matrix(NA, nrow = npart, ncol = tend) # weight (likelihood)\n  wt[, 1] &lt;- 1 / npart  # initial weights\n  W &lt;- matrix(NA, nrow = npart, ncol = tend) # normalized weights\n  A &lt;- matrix(NA, nrow = npart, ncol = tend) # Resample according to the normalized weight\n  \n  for (t in 2:tend) {# begin particle loop\n    # beta changes according to a Geometric Brownian motion \n    beta[t, ] &lt;- beta[t-1, ] * exp(beta[t, ])\n    # run process model\n    latent_var[, t, ] &lt;- SEIR_step(params = params,\n                                   y = latent_var[, t-1, ],\n                                   tbegin = t-1,\n                                   tend = t,\n                                   dt = dt,\n                                   beta = beta[t,])\n    # calculate weights (likelihood)\n    # wt[, t] &lt;- assign_weights(var = latent_var, t = t, data = data)\n    \n    case_expected &lt;- latent_var[, t, \"CI\"]\n    case_data &lt;- round(unlist(data[t, \"daily_infected\"]))\n    expected_val &lt;- pmax(0, case_expected) # make sure that the value is not negative\n    log_lik &lt;- dpois(round(case_data), lambda = expected_val, log = T)\n    wt[, t] &lt;- exp(log_lik)\n    # normalize particle weights\n    W[, t] &lt;- wt[, t] / sum(wt[, t])\n    # resample particles by sampling parent particles according to weights\n    A[, t] &lt;- sample(1:npart, prob = W[1:npart, t], replace = T)\n    # Resample particles for corresponding variables\n    latent_var[, t,] &lt;- latent_var[A[, t], t,]\n    beta[t,] &lt;- beta[t, A[, t]] #- needed for random walk on beta\n  } # end particle loop\n  \n  # Marginal likelihoods\n  lik_values &lt;- rep(NA, tend)\n  for (t in 1:tend) {\n    lik_values[t] &lt;- log(sum(wt[1:npart, t])) # log-likelihoods\n  }\n  # averaged log likelihoods log(L/(npart^tend))\n  loglik &lt;- - tend * log(npart) + sum(lik_values)\n  \n  return (list(lik_marginal = lik_values,\n               lik_overall_average = loglik,\n               latent_var_filtered = latent_var,\n               beta_filtered = beta,\n               W = W, A = A))\n}\n\n일별 변화를 계산하는 SEIR 전파 모형, 행의 수는 파티클 수와 같다.\n\nparams$R0 &lt;- 2\nparams$betavol &lt;- 0.3\nsample &lt;- pfilter(params=params, # parameters\n                     y=y0, # initial values of state variables\n                     data=fakedata, # input data set\n                     npart = 1000, # number of particles\n                     tend = tend, # simulation stop time\n                     dt = 0.2) \nobserved &lt;- fakedata$daily_infected[2:nrow(fakedata)]\n\n\nPlot the results\n\n# draw incidence plot\ndaily_inc_summary &lt;- t(apply(sample$latent_var_filtered[,,5], 2, quantile,\n            probs=c(0.025, 0.5, 0.975)))\ndf &lt;- cbind(data.frame(time=1:(nrow(res1)-1), observed = observed), daily_inc_summary)\n\n\nggplot(df, aes(x=time)) +\n  geom_ribbon(aes(ymin=`2.5%`, ymax=`97.5%`), fill=\"steelblue\", alpha=0.8)+\n  geom_line(aes(y=`50%`), color=\"steelblue\")+\n  geom_point(aes(y=observed), color = \"darkred\")+\n  labs(x=\"Time\", y=\"Daily incidence\")\n\n\n\n# draw daily Rt plot\ndur &lt;- 1/params$gamma\ndaily_Rt_summary &lt;- t(apply(sample$beta_filtered * dur, 1, quantile,\n                            probs=c(0.025, 0.5, 0.975)))  \ndf &lt;- cbind(data.frame(time=1:(nrow(res1)-1), true_Rt = Rt_true[2:51]), daily_Rt_summary)\nggplot(df, aes(x=time)) +\n  geom_ribbon(aes(ymin=`2.5%`, ymax=`97.5%`), fill=\"darkgreen\", alpha=0.7)+\n  geom_line(aes(y=`50%`), color=\"darkgreen\")+\n  geom_point(aes(y=true_Rt), color = \"black\") + \n  labs(x=\"Time\", y=expression(italic(R[t])))\n\n\n\n# ggsave(\"particle_filter_covid.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "blog/posts/pop-monte-carlo/index.html",
    "href": "blog/posts/pop-monte-carlo/index.html",
    "title": "Population Monte Carlo 파퓰레이션 몬테카를로",
    "section": "",
    "text": "최근에 파티클필터링 (particle filtering; PF) 방법을 이용하여 \\(\\mathcal{R}_t\\) 추정하는 과정에 대한 논문을 썼다. 그런데, 항상 의문이었던 것은 PF를 조금만 변형하면 감염병 모형의 감염속도 \\(\\beta=\\mathcal{R}_0 \\gamma\\) 와 같은 time-invariant 파라미터를 추정할 수도 있지 않을까 하는 것이었다. Population Monte Carlo (PMC)가 바로 그 방법이었다.\n이번 포스트에서는 SIR 모형의 모수 \\(\\beta\\)를 PMC 방법으로 추정하여 보았다. 추정하는 PMC 알고리즘을 아래에 구현하였다. 전에 구현했던 particle filtering 와 유사하다. 즉 중요도 샘플링 (importance sampling)을 연속으로 구현하는 데 연속으로 샘플링 하기 위해 Markov Chain Monte Carlo 에서 사용하듯이 proposal 을 이용하여 다음 단계의 샘플을 만들고 중요도 샘플링을 이용하여 추정을 하는 것이다.\n\nlibrary(truncnorm) # draw or evaluate according to a truncated normal dist  \npmc &lt;- function (params = NULL,\n                 x0 = NULL, # initial values\n                 y = NULL, # observation\n                 npart = 1000, # number of particles \n                 niter = 10, # iterations\n                 tend = 100, # to control the number of daily y to be fitted\n                 dt = 0.1, # dt for the ODE integration\n                 prior_mean = 0.5,\n                 prior_sd = 2,\n                 prior_lb = 0,\n                 prior_ub = 2) {\n  \n  # makes it easy to use truncated normal distribution\n  nstate &lt;- length(x0) # number of state variables (i.e., S, I, R, CI)\n  \n  # initial betas are sampled according to the prior distribution\n  beta0 &lt;- rtruncnorm(npart, a=prior_lb, b=prior_ub, mean=prior_mean, sd=prior_sd)\n  beta &lt;- matrix(NA, ncol=npart, nrow=niter) # to store the samples for beta\n  beta[1,] &lt;- beta0 # the initial values for the first row\n  # proposal for the next iteration, which is then resampled according to the  weight\n  sd = sd(beta[1,]) # scale for the proposal is adapted according to the current sample \n  beta[2,] = rtruncnorm(npart, a=prior_lb, b=prior_ub, mean=beta[1,], sd=sd)\n  \n  lik &lt;- matrix(NA, ncol = npart, nrow = niter) # likelihood \n  proposal_prob &lt;- matrix(NA, ncol = npart, nrow = niter)\n  wt &lt;- matrix(NA, ncol = npart, nrow = niter) # weight \n  W &lt;- matrix(NA, ncol = npart, nrow = niter) # normalized weights\n  A &lt;- matrix(NA, ncol = npart, nrow = niter) # Resample according to the normalized weight\n  # initial value  \n  proposal_prob[1,] &lt;- 1\n  wt[1,] &lt;- 1 / npart  # initial weights\n  W[1,] &lt;- wt[1,]\n \n  for (i in 2:niter) {\n    # cat(\"i =\", i, \"\\n\")\n    # tend increases by 1 accounts for the initial values\n    X &lt;- array(0, dim = c(npart, tend+1, nstate),\n               dimnames = list(NULL, NULL, names(x0)))\n    for (nm in names(x0)) {# starting values for each particle\n      X[, 1, nm] &lt;- x0[[nm]]\n    }\n    # run process model (i.e., SIR model) \n    x_1_tend &lt;- \n      process_model(params = params,\n                   x = X,\n                   dt = dt,\n                   beta = beta[i,])\n    # calculate weights (likelihood)\n    lik[i,] &lt;- assign_weights(x = x_1_tend, y = y[1:tend])\n    # normalize particle weights\n    proposal_prob[i,] = dtruncnorm(beta[i,], beta[i-1,], a=prior_lb, b=prior_ub, sd=sd)\n    prior_prob = dtruncnorm(beta[i,], a=prior_lb, b=prior_ub, mean=prior_mean, sd=prior_sd)\n    wt[i,] &lt;- lik[i,] * prior_prob / proposal_prob[i,]\n    \n    W[i,] &lt;- wt[i,] / sum(wt[i,])\n    # resample particles by sampling parent particles according to normalized weights\n    A[i,] &lt;- sample(1:npart, prob=W[i,], replace=T)\n    beta[i,] &lt;- beta[i, A[i,]] # resampled beta according to the normalized weight\n    # sd for the proposal can be adapted in various other ways, but we use the sd of the current sample\n    sd = sd(beta[i,]) \n    # generate proposals for the next iteration\n    if (i &lt; niter) {\n      beta[i+1,] &lt;- rtruncnorm(npart, a=prior_lb, b=prior_ub, mean=beta[i,], sd=sd)\n    } \n  } # end iteration\n  return (list(theta=beta, lik=lik, W=W, A=A))\n}\n\n감염병 확산 과정을 나타내는 SIR 모형을 구현해보자. 파티클수에 따라 벡터형태로 SIR 모형을 구현하였다.\n\nprocess_model &lt;- function (params = NULL,\n                           x = NULL,\n                           dt = 0.1,\n                           beta = NULL) {\n  \n  S &lt;- x[, 1, \"S\"] # a vector of initial S across the particles\n  I &lt;- x[, 1, \"I\"] # a vector of initial I across the particles\n  R &lt;- x[, 1, \"R\"] # a vector of initial S across the particles\n  \n  len &lt;- length(x[1,,\"S\"]) # length of model predictions (same as the data points) + 1 accounting for the initial values\n         \n  N &lt;- S + I + R\n  gamma &lt;- params[[\"gamma\"]]\n  \n  for (j in 2:len) {\n    daily_infected &lt;- 0 # to track the daily infection\n    for (i in seq(dt, 1, dt)) { # steps per day\n      FOI &lt;- beta * I * S/N\n      S_to_I &lt;- FOI * dt\n      I_to_R &lt;- I * gamma * dt\n  \n      S &lt;- S - S_to_I\n      I &lt;- I + S_to_I - I_to_R\n      R &lt;- R + I_to_R\n      \n      daily_infected &lt;- daily_infected + S_to_I\n    }\n    \n    x[, j, \"S\"] &lt;- S\n    x[, j, \"I\"] &lt;- I\n    x[, j, \"R\"] &lt;- R\n    x[, j, \"Inc\"] &lt;- daily_infected\n  }\n  return(x[, 2:len, \"Inc\"])\n}\n\n모수 추정에 사용할 거짓 일별 감염자수를 만들어보자. 위에서 구현한 process_model에서 예측되는 일별 감염자 수를 평균으로 하는 푸아송 분포를 이용하여 만들었다.\n\nparm = list(gamma=0.3) #\nx0 = c(S=9990, I=10, R=0, Inc=0)#\ntend = 50 # the number of observations\n# tend + 1 to account for the initial values\nX &lt;- array(0, dim = c(1, tend+1, 4), \n               dimnames = list(NULL, NULL, names(x0)))\nfor (nm in names(x0)) {# starting values for each particle\n  X[, 1, nm] &lt;- x0[[nm]]\n}\ntruebeta &lt;- 0.6 # true beta\npred &lt;- process_model(params=parm, x=X, beta=truebeta)\ny &lt;- rpois(tend, lambda=round(pred))  # \n\npmc 함수에 사용된 또 다른 함수 assign_weights를 아래에 구현하였다.\n\nassign_weights &lt;- function (x, y) {\n  di &lt;- dim(x)\n  npart &lt;- di[1] # number of particles\n  nobs &lt;- di[2] # number of observations\n  loglik &lt;- rep(NA, npart)\n  for (i in 1:npart) {\n    mean_case &lt;- x[i,] # for the ith particle\n    expected_case &lt;- pmax(0, mean_case)\n    obs_case &lt;- round(y)\n    loglik[i] &lt;- sum(dpois(obs_case, lambda=expected_case, log=T), na.rm=T)\n  }\n  return (exp(loglik)) # convert to normal probability\n}\n\nPMC를 이용하여 모수 추정을 해보고 결과를 그림으로 나타내보자.\n\nset.seed(45)\n# gamma and x0 are set to the same as the model used to generate the data\nparm = list(gamma=0.3)\nx0 = c(S=9990, I=10, R=0, Inc=0)# initial condition\nniter = 50\n# out = pmc(params = parm, x0 = x0, y=y, npart=10000, niter=niter,\n#    tend = length(y), dt=0.1, prior_mean=0.5, prior_sd=0.1, prior_lb=0,\n#    prior_ub=2)\n# saveRDS(out, \"out_20230811.rds\")\n\nout &lt;- readRDS(\"out_20230811.rds\")\nhist(out$theta[niter,], xlab=expression(beta), main=\"\")\nabline(v=truebeta, col=2, lwd=2)"
  },
  {
    "objectID": "blog/posts/Probabily_outbreak/index.html",
    "href": "blog/posts/Probabily_outbreak/index.html",
    "title": "감염병의 대유행 가능성",
    "section": "",
    "text": "감염병 인류 라는 책을 재미있게 읽는 중이다. 136페이지에는 기초감염재생산지수와 대유행의 가능성에 대한 간단한 수식이 나온다. \\[\\text{대유행의 가능성} = 1 - \\frac{1}{R_0}\\]\n\\(R_0\\)는 기초감염재생산지수, 즉 한 사람의 감염자가 다른 모든 사람이 감수성자 (susceptible) 일때 감염시키는 평균 감염자수를 나타낸다. 위 수식은 제한적인 경우에만 적용된다. 감염병 유입 시 대유행의 가능성에 대해 좀 더 일반적으로 적용될 수 있는 방법에 대해 적어보려고 한다. 물론 아래 내용도 상당히 이상적인 상황에 대한 기술일 뿐이고 현실은 그 보다 훨씬 더 복잡할 것이다. 감염병 유입 시 대유행의 가능성에 대한 내용은 Niels G. Becker가 저술한 Modeling to Inform Infectious Disease Control의 제 2장에 자세하게 기술되어 있다.\n동일한 사람들로 이루어진 인구 집단에서 감염병이 퍼져나가는 현상을 생각해 보자. 한 사람이 평균적으로 \\(R_0\\) 명을 감염시킨다고 할 때 \\(R_0\\)은 평균일뿐 후속 감염자수는 어떤 확률 분포를 가진다고 생각해볼 수 있다. 한 명의 감염자가 총 \\(j\\) 명의 후속 감염자를 만들어 내는 확률을 \\(P(X=j) = p_j\\)라고 할 때 \\(R_0\\)는 아래와 같이 표현할 수 있다.\n\\[R_0 = \\sum_{k=0}^\\infty j p_j.\\]\n대유행의 가능성은 역으로 생각하는 것이 유리하다. 즉 대유행이 아닌 소규모의 감염으로 막을 내리는 확률, \\(\\theta\\), 을 구한 후 대유행의 확률은 \\(1-\\theta\\)로 구하는 것이다. 소규모의 감염이 일어나기 위해서는 유입된 초기 환자 (index patient)가 아무도 감염시키지 않거나 혹은 몇 명을 감염시켰다고 할지라도 후속 감염자들이 추가적으로 일으키는 감염이 소규모일때만 가능할 것이다. 즉 소규모 감염의 확률, \\(\\theta\\), 는 아래의 식을 만족한다.\n\\[\n\\theta =  p_0 + p_1 \\theta + p_2 \\theta^2 + ...\n\\tag{1}\\]\n위 식을 보면 \\(\\theta\\) 는 후속 감염자수가 어떠한 분포를 따르느냐에 따라 달라질 것이라 예상할 수 있다. 가장 흔히 쓰이는 분포 중의 하나인 푸아송 분포 (Poisson distribution)를 생각해보자. 화률 분포 함수는 아래와 같다.\n\\[\\text{Prob}(X=j) = \\frac{R^j e^R}{j!}\\text{ for } j=0,1,2,...\\]\n이 경우 Equation 1 의 우변은 아래와 같이 나타내어질 수 있다.\n\\[\\sum_{j=0}^\\infty p_j \\theta^{j} = e^{(1-\\theta)R_0}.\\]\n따라서 \\(\\theta\\)는 아래의 식을 계산하면 된다. 다만 해를 직접 구할 수는 없고 수치해석방법을 이용 해서 답을 구해야 한다.\n\\[\\theta = e^{(1-\\theta)R_0}.\\]\n위에서 언급한 \\(\\text{대유행의 가능성} = 1 - \\frac{1}{R_0}\\)는 후속 감염자수의 분포가 기하분포 (geometric distribution)를 따를때 성립한다. 기하분포는 아래와 같이 표현되 \\[\\text{Prob}(X=j) = (1-p)^j p\\]\n평균과 성공확률의 관게, \\(R = \\frac{1-p}{p}\\), 이용하여 다시 표현하면 아래와 같다.\n\\[\\text{Prob}(X=j) = (\\frac{R_0}{1+R_0})^j \\frac{1}{1+R_0}\\] 위에서와 동일하게 계산하면 아래와 같다. \\[\\sum_{j=0}^\\infty p_j \\theta^{j} = \\frac{1}{1 + (1-\\theta)R_0}\\] 따라서 아래의 식을 풀면 \\(\\theta\\)를 구할 수 있다.\n\\[\\theta = \\frac{1}{1 + (1-\\theta)R_0}.\\]\n\\[\\theta = \\frac{1}{R_0}.\\]\n따라서 감염병 인류 책에서 언급된 것처럼 \\[\\text{대유행의 가능성} = 1 - \\frac{1}{R_0}.\\] 마지막으로 후속 감염자수가 이항분포 (negative binomial distribution)을 따른다고 가정해보자. 최근 연구들에서 빈번하게 언급되고 가장 현실에 가까운 가정인 듯 하다.\n다양한 모수를 사용하여 표현할 수 있고 평균 \\(R_0\\)과 확산(dispersion; \\(k\\))를 이용하여 나타내면 아래와 같다. 이항 분포는 \\(k=1\\)일 때는 기하분포와 동일하고 \\(k=\\infty\\) 일때는 푸아송분포와 동일하다. 코로나 19의 경우 \\(k\\) 값이 약 0.55이다.\n\\[\\text{Prob}(X=j) = \\frac{\\Gamma(k+j)}{j!\\Gamma(k)}\\left(\\frac{k}{k+R_0}\\right)^k\\left(\\frac{R_0}{k+R_0}\\right)^j \\text{ for } k=0,1,2,...\\] The probability of a minor outbreak, 위와 동일한 방법으로 계산하면 우변은 아래와 같다.\n\\[\\sum_{j=0}^\\infty p_j \\theta^{j} = \\left(\\frac{k}{k + (1-\\theta)R_0}\\right)^k\\] 따라서 \\[\\theta = \\left(\\frac{k}{k + (1-\\theta)R_0}\\right)^k\\]를 계산하여 소규모감염의 확률을 계산하고 \\(1-\\theta\\)를 계산하여 대규모유행 확률을 계산하면 된다.\nR 시물레이션을 통해서 세방법이 어떻게 다른 결과를 나타내는지 살펴보자\n\nprob_outbreak_pois = function(theta, R0){\n  theta - exp(-R0 + R0*theta) \n}\nprob_outbreak_geo = function(R0){\n  1/R0 \n}\nprob_outbreak_nb = function(theta, R0, k){\n  theta - (k / (k + R0 - R0*theta))^k \n}\n\nRs &lt;- seq(1.1, 10, length.out=100)\ntheta1 &lt;- sapply(Rs, function(x) \n  min(rootSolve::multiroot(prob_outbreak_pois, c(0, 1), R0=x)$root))\ntheta2 &lt;- sapply(Rs, function(x) prob_outbreak_geo(x))\ntheta3 &lt;- sapply(Rs, function(x) \n   min(rootSolve::multiroot(prob_outbreak_nb, c(0, 1), R0=x, k=0.55)$root))\ndf &lt;- data.frame(R0=rep(Rs,3), \n                 dist=rep(c(\"Pois\",\"Geom\",\"NB(k=0.55)\"),each=100),\n                 prob_outbreak=c(1-theta1,1-theta2,1-theta3))\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, \n                                     axis_title_size=12))\n\nggplot(df) +\n  geom_line(aes(R0, prob_outbreak, color=dist))+\n  ggtitle(expression(\"Probability of a large outbreak vs.\" ~italic(R)[0])) +\n  labs(y=\"Probability of a large outbreak\", x=expression(italic(R)[0]), color=\"\")\n\n\n\n# ggsave(\"outbreak_prob.png\", units=\"in\", width=3.4*2, height=2.7*2)  \n\n각 분포에 대한 무한 수열의 합과 \\(\\theta\\)에 대한 해는 아래 Mathematica 명령어를 사용하여 확인할 수 있다.\n\nSum[PDF[PoissonDistribution[R],j]\\[Theta]^j, {j, 0, Infinity}]\nFindRoot[\\[Theta] - E^(R (-1 + \\[Theta])) == 0, {\\[Theta], 0.1}] /. \n R -&gt; 3\nSum[PDF[GeometricDistribution[1/(1+R)],j]\\[Theta]^j, {j, 0, Infinity}]\nFindRoot[\\[Theta] - 1/(R + \\[Theta] - R \\[Theta]) == 0, {\\[Theta], \n   0.5}] /. R -&gt; 3\n\nFullSimplify[Sum[PDF[NegativeBinomialDistribution[k,k/(R+k)],i]\\[Theta]^i, {i, 0, Infinity}],{ k&gt;0, R&gt;1, 0&lt;\\[Theta]&lt;1}ㅑ\nFindRoot[\\[Theta] - (k/(k + R - R \\[Theta]))^k == 0, {\\[Theta], 0.1}]"
  },
  {
    "objectID": "blog/posts/R0-sympy/index.html",
    "href": "blog/posts/R0-sympy/index.html",
    "title": "Basic reproduction number using SymPy",
    "section": "",
    "text": "감염병의 전파를 이해하는 데 있어 가장 기본적인 개념이 재감염지수, 특히 기초재감염지수 (\\(\\mathcal{R}_0\\)) 이다. 재감염지수는 한 명의 감염자로부터 생산되는 평균 후속 감염자의 수를 일컫는데 기초재감염지수는 코로나19의 경우 처럼 인구 집단에 면역력을 가진 사람이 없어 모든 사람이 감염될 수 있는 상태하 에서의 재감염지수를 말한다. 기초 재감염 지수는 다음과 같은 수식으로 표현할 수 있다.\n\\[ \\mathcal{R}_0 = \\beta c D \\]\n\\(\\beta\\) 는 한 명의 감염자가 타인을 접촉할 때 상대방을 감염시킬 수 있는 확률, \\(c\\) 는 단위 시간 당 접촉이 일어나는 횟수, \\(D\\) 는 감염 상태가 지속되는 시간을 나타낸다. \\(\\beta\\) 만으로 \\(\\beta c\\) 를 대신해 사용하는 경우도 흔하다. 그 경우 \\(\\beta\\) 는 단위 시간 당 후속 감염자의 수로 표현할 수 있을 것 같다. 미분방정식에 기반한 감염병 모형의 경우는 \\(\\mathcal{R}_0\\)를 어떻게 계산할까? 아래와 같이 SIR 모형을 정의해 보자.\n\\[\\begin{align}\n\\mathrm{d}S/\\mathrm{d}t &= -\\beta I S/N \\\\\n\\mathrm{d}I/\\mathrm{d}t &= \\beta I S/N - \\gamma I\\\\\n\\mathrm{d}R/\\mathrm{d}t &= \\gamma I\n\\end{align}\\]\n위의 정의에서 사용되었던 개념을 적용한다면 \\(\\mathcal{R}_0 = \\beta/\\gamma\\) 라고 할 수 있다. 이는 \\(\\mathcal{R}_0\\)가 감염병이 집단 내에서 유행을 일으킬 수 있는 역치조건임을 이용해도 동일한 결론에 이를 수 있다. (i.e., \\(\\mathrm{d}I/\\mathrm{d}t&gt;0\\))\n위와는 달리 Diekmann et al. 에 의해서 도입된 next generation 방법으로 좀 더 다양한 상황 하에서 \\(\\mathcal{R}_0\\)를 구할 수 있다. 이 방법에서는 \\(\\mathcal{R}_0\\)가 next generation operator의 spectral radius 가 된다. 위 논문 보다는 van den Driessche et al. 가 좀 더 이해하기 쉬운 것 같아 이 방법을 기준으로 살펴보겠다. 그리고 그 계산을 python의 SymPy 라이브러리를 이용해서 구현을 해보겠다. 먼저 간단한 우선 ( SEIR ) 모형의 경우부터 살펴보자.\nNext generation operator \\(G\\) 은 전파를 통해서 생산되는 새로운 감염이 발생하는 속도를 나타내는 행렬 \\(F\\)와 감염이 다른 상태로 변화되는 속도 (V)로 구성되며 다음과 같은 관계를 갖는다. \\(G=FV^{-1}\\). 그리고 \\(R_0\\)는 \\(G\\)의 spectral radius가 된다. 아래 파이썬 구현에서는 \\(\\beta, \\gamma\\) 를 b, g로 나타내었다.\n\nfrom sympy import *\nb, k, g, = symbols('b k g')\nF = Matrix([[0, b],[0, 0]])\nV = Matrix([[k, 0], [-k, g]])\nM = F*(V**-1)\neigval = M.eigenvals(simplify=true)\ninit_printing(use_unicode=True)\nlst = list(eigval.keys())\nlst[1]\n\nb\n─\ng\n\n\n위에서 언급한 SEIR 모형의 경우는 너무 간단하니 그 보다 조금 더 복잡한 그 예로 다음 연구를 살펴보자. Pitzer et al.의 연구인데 사용된 감염병 모형은 사람 간 직접 전파와 물을 통한 간접 전파 두 가지의 전파 메케니즘을 구현 하였고 (\\(\\lambda_p\\) 와 \\(\\lambda_w\\)) 최초 감염 \\(S_1 \\rightarrow I_1\\) 과 중복 감염\\(R \\rightarrow S_2 \\rightarrow I_2\\) 을 다르게 취급하였다. 부록 (supplementary material)을 보면 사용된 미분식을 볼 수 있다.\n\\[\\begin{align}\n\\mathrm{d}S_1/\\mathrm{d}t &= B + \\epsilon S_2 - (\\lambda_p+\\lambda_w+\\mu)S_1\\\\\n\\mathrm{d}I_1/\\mathrm{d}t &= (\\lambda_p+\\lambda_w)S_1 - (\\delta+\\mu) I_1 \\\\\n\\mathrm{d}R/\\mathrm{d}t &= \\delta(1-\\theta-\\alpha)(I_1+I_2) - (\\omega +\\mu)R \\\\\n\\mathrm{d}C/\\mathrm{d}t &= \\delta\\theta(I_1+I_2) - \\mu C \\\\\n\\mathrm{d}S_2/\\mathrm{d}t &= \\omega R -\\epsilon S_2 - (\\lambda_p+\\lambda_w+\\mu) S_2\\\\\n\\mathrm{d}I_2/\\mathrm{d}t &= (\\lambda_p+\\lambda_w) S_2 - (\\delta+\\mu) I_2 \\\\\n\\mathrm{d}W/\\mathrm{d}t &= \\gamma(I_1+rI_2+rC) - \\xi W\n\\end{align}\\]\n또한 아래와 같이 기초재감염지수도 계산 결과를 보여준다. SymPy를 통해 동일한 결과를 얻을 수 있는지 확인해보자.\n\\[\\begin{align}\nR_0 = \\frac{1}{\\mu+\\delta} \\left(\\beta_p +\\frac{\\gamma \\beta_w}{\\xi}\\right) \\left(1 +\\frac{\\delta\\theta r}{\\mu}\\right)\n\\end{align}\\]\n\np, r, w, N, d, m, t, g, x = symbols('p r w N d m t g x')\nF = Matrix([[p, r*p, r*p, w*N], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]])\nV = Matrix([[d+m, 0, 0, 0], [0, d+m, 0, 0], [-d*t, -d*t, m, 0], [-g, -r*g, -r*g, x]])\nM = F*(V**-1)\neigval = M.eigenvals(simplify=true)\ninit_printing(use_unicode=True)\neigval \n\n⎧      (N⋅g⋅w + p⋅x)⋅(d⋅r⋅t + m)   ⎫\n⎨0: 3, ─────────────────────────: 1⎬\n⎩             m⋅x⋅(d + m)          ⎭\n\nlst = list(eigval.keys())\nR0_eig = lst[1]\nR0 = (1/(d+m))*(p+N*g*w/x)*(1+(d*t*r)/m) # R0 from the Pitzer (2014)\nsimplify(R0-R0_eig) # 0 for the same expression (symbolic assessment)\n\n0\n\nR0.equals(R0_eig) # True for the same expression (numerical assessment)\n\nTrue\n\n\n부록에 보면 기초 재감염지수에 이르는 상세한 과정이 나와있는데 \\(V_{3,3}\\) 에 오류가 있음을 알아냈다. \\(\\delta +\\mu\\) 가 \\(\\mu\\) 로 바뀌어야 한다. 이유는 위 미분식에서 \\(C\\) 식을 보면 알 수 있는데 이는 만성 감염자를 나타내고 따라서 회복을 의미하는 \\(\\delta\\) 가 없어야 한다. 이는 기록하는 과정에서의 오류인 듯 하고 결과로 얻어진 \\(R_0\\) 는 우리가 계산한 결과와 동일하다. 다만, 계산 결과를 위 식에서와 같이 의미있는 구획으로 나누어서 표현하려면 SymPy 결과를 직접 수정하여야 한다."
  },
  {
    "objectID": "blog/posts/regression-toward-mean/index.html",
    "href": "blog/posts/regression-toward-mean/index.html",
    "title": "Regression toward the mean",
    "section": "",
    "text": "In his lecture Joseph Blitzstein talks about two basic statistical phenomena: Regression toward to the mean (RTTM) and survivor bias (or conditioning more broadly). The RTTM is today’s topic. The following was written mainly by GPT 4.\nRegression toward the mean is a statistical phenomenon where extreme data points are likely to be followed by less extreme ones when measured again. In simpler terms, it means that if an extreme observation is observed, the next observation is likely to be closer to the mean or average.\n\nWhy does it happen?\nIt’s primarily a matter of probability. Extreme values are, by definition, rare. So, when you take a second measurement, it’s simply more probable that the new value will be closer to the mean than the previous extreme value was.\n\n\nExamples:\nSports Performance: Imagine a basketball player who has an outstanding game, scoring well above their average number of points. If they’ve played at such an exceptional level, it’s likely that in the next game they will score closer to their average (not necessarily because their skill has decreased, but simply due to the natural variability in performance).\nStudent Test Scores: If a student who typically scores around the average on exams gets an exceptionally high score on one test, they might score closer to their average on a subsequent test. Conversely, if they score exceptionally low on one test, they might score higher the next time.\nInvestment Returns: If a particular stock has an exceptionally good year with returns way above the market average, it might have more modest (or even below-average) returns the following year.\nHeight of Parents and their Children: This is a classic example by Sir Francis Galton. He found that very tall parents tend to have children who are shorter than them (but still above average), and very short parents tend to have children who are taller than them (but still below average). The children’s heights tend to “regress” towards the mean height.\nMedical Treatments: If patients are selected for a clinical trial because they have exceptionally high blood pressure, some of them will probably show a reduction in blood pressure over time even without any treatment. This isn’t because of any therapeutic effect, but simply because their initial measurements were unusually high and subsequent measurements tend to be closer to the mean.\n\n\nMisunderstandings:\nThis concept is often misunderstood. For instance, if a student performs poorly on a test and then improves on the next one, it might be tempting to attribute this improvement to a particular intervention (like tutoring). While the intervention might have had an effect, it’s also possible that some of the improvement was due to regression toward the mean.\nIn research and experimental designs, this phenomenon needs to be taken into account, especially when making causal inferences from observed changes.\nToward the end of his talk, he mentions about the quote by Daniel Kahneman that very clearly explain the concept of RTTM.\nI had the most satisfying Eureka experience of my career while attempting to teach flight instructors that praise is more effective than punishment for promoting skill-learning. …. [A flight instructor objected:] “On many occasions I have praised flight cadets for clean execution of some aerobatic maneuver, and in general when they try it again, they do worse. On the other hand, I have often screamed at cadets for bad execution, and in general they do better the next time. So please don’t tell us that reinforcement works and punishment does not, because the opposite is the case.” …\nThis was a joyous moment, in which I understood an important truth about the world: because we tend to reward others when they do well and punish them when they do badly, and because there is regression to the mean, it is part of the human condition that we are statistically punished for rewarding others and rewarded for punishing them.\nFull details of Kahneman’s quote can be found from the following article"
  },
  {
    "objectID": "blog/posts/right-truncation/index.html",
    "href": "blog/posts/right-truncation/index.html",
    "title": "Estimating serial interval for a growing epidemic",
    "section": "",
    "text": "In this case, the above likelihood function may be modified as follows:\n\\[\\mathcal{L}(X;\\theta) = \\prod_{i=1}^{n} f_{\\theta}(B_i-A_i)\\] , where \\(A^L, A^R, B\\) present the times for lower end and upper bound on the potential dates of symptom onset of the infector, and the symptom onset time of the infectee, respectively.\n\\[f^t_{\\theta}(B_i-A_i) = \\frac{f_{\\theta}(B_i-A_i)}{F(T-A_i)}\\]\n\nset.seed(42)\nn &lt;- 1000\nshape_true &lt;- 2.2\nscale_true &lt;- 3.3\n\ndf &lt;- data.frame(A=sample(0:30, size=n, replace=TRUE))\nsi &lt;- rgamma(n, shape=shape_true, scale=scale_true)\ndf$B &lt;- df$A + si\nmax(df$B)\n\n[1] 57.03661\n\nsummary(df$B)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3967 13.9763 22.4103 22.4031 30.3811 57.0366 \n\nTmax &lt;- 35\nunder_Tmax &lt;- df$B &lt; Tmax\n\nnewdf &lt;- df[under_Tmax,]\n\nnll &lt;- function(parms, A, B) -sum(dgamma(B-A, shape=parms[[1]], scale=parms[[2]], log=TRUE))\nres1 = optim(par=c(1,2), fn=nll, A=newdf$A, B=newdf$B,\n             method = \"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\nres1\n\n$par\n[1] 2.366494 2.735134\n\n$value\n[1] 2389.103\n\n$counts\nfunction gradient \n     101       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\nnll_right_trunc &lt;- function(parms, A, B, Tmax) -sum(log(dgamma(B-A, shape=parms[[1]], scale=parms[[2]])/pgamma(Tmax-A, shape=parms[[1]], scale=parms[[2]])))\n\nres2 = optim(par=c(1,2), \n             fn=nll_right_trunc, \n             A=newdf$A, \n             B=newdf$B,\n             Tmax=Tmax,\n             method = \"Nelder-Mead\",\n             control = list(maxit=2e4, reltol=1e-15))\nres2\n\n$par\n[1] 2.230993 3.230118\n\n$value\n[1] 2317.955\n\n$counts\nfunction gradient \n     115       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\nn &lt;- 1e5\nx1 &lt;- rgamma(n, shape=res1$par[[1]], scale=res1$par[[2]])\nx2 &lt;- rgamma(n, shape=res2$par[[1]], scale=res2$par[[2]])\n\nsummary(x1)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.01066  3.36629  5.58653  6.47582  8.62659 44.32536 \n\nsummary(x2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0263  3.6574  6.1697  7.2078  9.6218 43.8174 \n\ndf = data.frame(model=rep(c(\"No truncation\", \"Right truncated\"), each=n), val=c(x1,x2))\n\nlibrary(ggplot2)\nextrafont::loadfonts()\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\ndf |&gt; ggplot(aes(x=val, fill=model))+\n  geom_density(alpha=0.2) +\n  labs(x=\"value\", y=\"density\")+\n  theme(legend.position = \"bottom\", \n        legend.title = element_blank())\n\n\n\n# ggsave(\"right_trunc.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "blog/posts/rootsolve_optimize/index.html",
    "href": "blog/posts/rootsolve_optimize/index.html",
    "title": "Final epidemic size: uniroot vs. optimize",
    "section": "",
    "text": "Final size of an epidemic\nMiller 2012 shows that the final size of an epidmic for a well-mixed population can be derived in the following way. We divide the population into susceptible, infected, and recovered fractions: \\(S(t), I(t), and R(t)\\) respectively. Assuming a large population, constant transmission and recovery rates, and mass action mixing, we have\n\\[\\dot{S}= -\\beta IS, ~\\dot{I}=\\beta IS -\\gamma I, ~\\dot{R}=\\gamma I\\] We can remove \\(\\dot{R}\\) since \\(S+I+R=1\\). From the equation, we can have the following relationship.\n\\[\\frac{dS}{dI}= -1 + \\frac{\\gamma}{\\beta S}\\] Solving this equation gives the following: \\[ I(t) = -S(t) + \\frac{\\gamma}{\\beta} \\text{ln} S(t) + C\\]\nWe can find \\(C=1\\) using the initial conditions (\\(I\\rightarrow 0, S\\rightarrow 0\\)). Then, using \\(I(\\infty)=0\\) gives the following relationship\n\\[S(\\infty) = 1 − \\text{exp}\\left[-R_0\\left(1-S(\\infty)\\right)\\right]\\] Using the \\(R(\\infty)=1-S(\\infty)\\), we can get the following equation for the final size of an epidemic, \\(R(\\infty)\\):\n\\[R(\\infty) = 1 − \\text{exp}\\left[-R_0R(\\infty)\\right]\\] Let’s use the above relationship to compute the final epidemic size nuerically\n\nfinal_size &lt;- function(R, R0){\n  R - 1 + exp(-R0*R)\n}\n# lower bound set at 0.1 to avoid R=0, which is also a solution\nuniroot(final_size, interval=c(0.1,1), R0=2)$root\n\n[1] 0.796811\n\ndf &lt;- data.frame(R0vec = c(1.1, seq(1.2, 4, by=0.1))) # as  \ndf$sizevec = sapply(df$R0vec, function(x) uniroot(final_size, interval=c(0.1,1), R0=x)$root)\n\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(df, aes(R0vec, sizevec)) +\n  geom_line(linetype=\"dashed\")+\n  geom_point()+\n  labs(x=parse(text=\"R[0]\"), y=\"Final epidemic size\")\n\n\n\n# ggsave(\"epidemicsize_R.png\", p, units=\"in\", width=3.4, height=2.7)\n\nInstead of uniroot, optimize function can be used to find the solution for the above equation. However, optimize gives the correct answer when the function was squared.\n\noptimize(final_size, interval=c(0.1,1), R0=2)\n\n$minimum\n[1] 0.3465758\n\n$objective\n[1] -0.1534264\n\nfinal_size_sq &lt;- function(R, R0){\n  (R - 1 + exp(-R0*R))^2\n}\noptimize(final_size_sq, interval=c(0.1,1), R0=2)\n\n$minimum\n[1] 0.7968155\n\n$objective\n[1] 3.933157e-12"
  },
  {
    "objectID": "blog/posts/seir_varying_number_compartments/index.html",
    "href": "blog/posts/seir_varying_number_compartments/index.html",
    "title": "SEIR model with varying number of compartments",
    "section": "",
    "text": "The \\(SEIR\\) model structures each stage as a distinct compartment, with transitions between compartments dictated by specific rate constants, except for the transition between \\(S\\) and \\(E\\) that involve force of infection which is also a function of \\(t\\). Waiting times in the \\(E\\) and \\(I\\) compartments are assumed to follow an exponential distribution. However, to better reflect reality, it might be necessary to consider alternative distributions. For instance, the waiting time in the \\(E\\) compartment, i.e., incubation period, could be represented by a lognormal or Weibull distribution. Within the framework of ordinary differential equations (ODEs), incorporating these distributions directly poses challenges. Nevertheless, by dividing a single compartment into multiple sub-compartments, waiting times follow the Erlang distribution, effectively capturing the essence of both lognormal and Weibull distributions.\nThe process of adding multiple sub-compartments is relatively straightforward. The challenge arises when one wishes to dynamically adjust the number of sub-compartments within a single model framework. This post discusses a method to achieve such adaptability in model construction. To enhance model flexibility, I extensively employ strings, specifically utilizing the eval(str2lang(\"some text\")) function.\n\ndiffeqr package\ndiffeqr package is used use Julia’s DifferentialEquations library\n\nlibrary(diffeqr)\nde &lt;- diffeqr::diffeq_setup()\n\nFirst, let’s construct the classic SEIR model for comparison. This model is equivalent to a variant with variable compartments for the \\(I\\) stage when considering only a single compartment.\n\nseir &lt;- function(u, p, t){\n  \n  S &lt;- u[1]; \n  E &lt;- u[2];\n  # the number of compartments to model the duration of infectiousness\n  I &lt;- u[3];\n  R &lt;- u[4]; \n  C &lt;- u[5];\n\n  # population size\n  pop &lt;- S+E+I+R\n  \n  epsilon &lt;- p[1] # 1/latent period\n  gamma &lt;- p[2] # 1/duration of infectiousness\n  beta &lt;- p[3] # transmission rate\n  omega &lt;- p[4] # 1/omega = duration of natural immunity\n  # force of infection\n  foi &lt;- beta*I/pop \n  \n  muEI &lt;- epsilon\n  muIR &lt;- gamma\n  muRS &lt;- omega\n  \n  # differential equations\n  dS &lt;- - foi*S + muRS*R \n  dE &lt;- foi*S - muEI*E\n  dI &lt;- muEI*E - muIR*I\n  dR &lt;- muIR*I - muRS*R\n  \n  dC &lt;- muEI*E\n\n  return(c(dS,dE,dI,dR,dC))\n}\n\n# simulation\nR0 &lt;- 2 # basic reproduction number\nepsilon &lt;- 1/4 # 1/epsilon = incubation period\ngamma &lt;- 1/7 # 1/gamma = duration of infectiousness\nbeta &lt;- R0*gamma # instantaneous transmission rate\nomega &lt;- 1/(4*365) # natural immunity waning rate\n# parameters\nparams &lt;- c(epsilon=epsilon, gamma=gamma, beta=beta, omega=omega)\nu0 &lt;- c(0.99, 0, 0.01, 0, 0)\ntend &lt;- 100 #  \ntspan &lt;- c(0.0, tend)\n  \nprob &lt;- de$ODEProblem(seir, u0, tspan, params)\nsol &lt;- de$solve(prob, de$Tsit5(), saveat=1)\nmat &lt;- sapply(sol$u, identity)\nudf &lt;- as.data.frame(t(mat))\nout &lt;- cbind(data.frame(t=sol$t), udf)\nnames(out) &lt;- c(\"t\",\"S\",\"E\",\"I\",\"R\",\"C\") \n\n\n\n\\(SEIR\\) model with str2lang\nThe model with variable compartments for the \\(I\\) stage. A part of the model is written using strings using paste, which are later evaluated using eval(str2lang(\"some string\")).\n\nseir_nstg &lt;- function(u, p, t){\n  \n  S &lt;- u[1]; \n  E &lt;- u[2];\n  # the number of compartments to model the duration of infectiousness\n  nstg &lt;- p[1] \n  for (i in 1:nstg) {\n    eval(str2lang(paste0(\"I\",i,\" &lt;- \", \"u[2+\",i,\"]\")));\n  } \n  R &lt;- u[2 + nstg + 1]; \n  C &lt;- u[2 + nstg + 2];\n\n  # population size\n  istr_sum &lt;- paste(sapply(1:nstg, function(x) paste0(\"I\",x)), collapse=\"+\")\n  eval(str2lang(paste0(\"pop &lt;- S+E+R+\", istr_sum)))\n  \n  epsilon &lt;- p[2] # 1/latent period\n  gamma &lt;- p[3] # 1/duration of infectiousness\n  beta &lt;- p[4] # transmission rate\n  omega &lt;- p[5] # 1/omega = duration of natural immunity\n  # force of infection\n  eval(str2lang(paste0(\"foi &lt;- beta*(\", istr_sum, \")/pop\"))) \n  \n  muEI &lt;- epsilon\n  muIR &lt;- gamma\n  muRS &lt;- omega\n  \n  # differential equations\n  dS &lt;- - foi*S + muRS*R \n  dE &lt;- foi*S - muEI*E\n  dI1 &lt;- muEI*E - nstg*muIR*I1\n\n  #dI2, ...\n  if (nstg &gt;= 2) {\n    for (i in 2:nstg) {\n      eval(str2lang(paste0(\"dI\",i,\" &lt;- \", \n                           nstg, \"*muIR*(I\", i-1, \"-I\", i, \")\")))\n    }\n  }\n  # dR\n  eval(str2lang(paste0(\"dR &lt;- muIR*I\", nstg, \"-muRS*R\")))\n  \n  dC &lt;- muEI*E\n\n  distr &lt;- paste(sapply(1:nstg, function(x) paste0(\"dI\", x)), collapse=\",\")\n  return(eval(str2lang(paste0(\"c(dS,dE,\", distr, \",dR,dC)\"))))\n}\n\nSimulation of the variable-compartment model\n\nrun_seir_nstg &lt;- function(nstg) {\n  R0 &lt;- 2 # basic reproduction number\n  epsilon &lt;- 1/4 # 1/epsilon = incubation period\n  gamma &lt;- 1/7 # 1/gamma = duration of infectiousness\n  beta &lt;- R0*gamma # instantaneous transmission rate\n  omega &lt;- 1/(4*365) # natural immunity waning rate\n  # parameters\n  params &lt;- c(nstg=nstg, epsilon=epsilon, gamma=gamma, beta=beta, omega=omega)\n  # initial distribution of the population across the states\n  I0s &lt;- paste(sapply(1:nstg, function(x) paste0(\"I\", x,\"=0\")), collapse=\",\")\n  eval(str2lang(paste0(\"u0 &lt;- c(S=0,E=0,\", I0s,\",R=0,C=0)\")))\n  u0[c(\"S\",\"E\",\"I1\",\"R\")] &lt;- \n    c(0.99, 0, 0.01, 0) # all I in the first compartment of the nstg I compartments\n  tend &lt;- 100 #  \n  tspan &lt;- c(0.0, tend)\n  \n  prob &lt;- de$ODEProblem(seir_nstg, u0, tspan, params)\n  sol &lt;- de$solve(prob, de$Tsit5(), saveat=1)\n  \n  mat &lt;- sapply(sol$u, identity)\n  udf &lt;- as.data.frame(t(mat))\n  out &lt;- cbind(data.frame(t=sol$t), udf)\n  In &lt;- paste(sapply(1:nstg, function(x) paste0('\"I', x, '\"')), collapse=\",\")\n  names(out) &lt;- eval(str2lang(paste0('c(\"t\",\"S\",\"E\",', In, ',\"R\",\"C\")'))) \n  \n  return(out)\n}\n\nnstg &lt;- 1\nIn &lt;- paste(sapply(1:nstg, function(x) paste0('\"I', x, '\"')), collapse=\",\")\nout1 &lt;- run_seir_nstg(nstg=nstg)\nIt1 &lt;- out1[,\"I1\"]\n\nnstg &lt;- 2\nIn &lt;- paste(sapply(1:nstg, function(x) paste0('\"I', x, '\"')), collapse=\",\")\nout2 &lt;- run_seir_nstg(nstg=nstg)\neval(str2lang(paste0('It2 &lt;- rowSums(out2[,c(', In, ')])')))\nnstg &lt;- 5\nIn &lt;- paste(sapply(1:nstg, function(x) paste0('\"I', x, '\"')), collapse=\",\")\nout5 &lt;- run_seir_nstg(nstg=nstg)\neval(str2lang(paste0('It5 &lt;- rowSums(out5[,c(', In, ')])')))\n\nPlot the simulation results.\n\n# png(filename=\"seir_multi_comp.png\")\n\nplot(1:nrow(out), out$I, type=\"l\", ylim=c(0,0.2),\n     ylab=\"fraction infectious\",\n     xlab=\"day\", col=\"black\")\n\nlines(1:nrow(out), It1, col=\"steelblue\")\nlines(1:nrow(out), It2, col=\"firebrick\")\nlines(1:nrow(out), It5, col=\"magenta\")\n\nlegend(\"topright\", \n  legend=c(\"SEIR\",\"1 compartment\",\"2 compartments\", \"5 compartments\"), \n  col=c(\"black\",\"steelblue\", \"firebrick\",\"magenta\"), \n  lty= 1,\n  bty = \"n\", \n  cex = 1.0, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.02,0.02))\n\n\n\n# dev.off()"
  },
  {
    "objectID": "blog/posts/SIR-sympy/index.html",
    "href": "blog/posts/SIR-sympy/index.html",
    "title": "SIR model using SymPy",
    "section": "",
    "text": "I attempted to replicate some of the simple analytical results presented in the book, Mathematical Epidemiology by Brauer et al.\n\\[\n\\begin{align}\n\\mathrm{d}S/\\mathrm{d}t &= -\\beta I S \\\\\n\\mathrm{d}I/\\mathrm{d}t &= \\beta I S - \\gamma I\\\\\n\\end{align}\n\\] The first part is simply to compute \\(dI/dS\\).\n\nfrom sympy import *\n\nR_0, b, g, dIdt, dSdt, S, I = symbols('R_0 b g dIdt dSdt S I')\n\ndSdt = - b*S*I\ndIdt = + b*S*I - g*I\ndSdI = dIdt / dSdt #-(I*S*b - I*g)/(I*S*b)\n\n# b &lt;- R0*g\nsimplify(-(I*S*R_0*g - I*g)/(I*S*R_0*g)) \n\n-1 + 1/(R_0*S)\n\n\nThe second part is integrate the equation, \\(\\text{d}I/\\text{d}S\\)\n\nS, I = symbols(\"S I\", cls=Function)\nb, g, t, R_0, S0, I0 = symbols(\"b g t R_0 S0 I0\")\n\neq = Eq(I(t).diff(t), - S(t).diff(t) + (1/R_0)*(1/S(t))*S(t).diff(t))\n\nintegrate(eq, t)\n\nEq(I(t), -S(t) + log(S(t))/R_0)"
  },
  {
    "objectID": "blog/posts/stan_sir_transformed_parameters/index.html",
    "href": "blog/posts/stan_sir_transformed_parameters/index.html",
    "title": "SIR model in Stan: Euler method",
    "section": "",
    "text": "SIR model in Stan\nI developed a SIR model and solved it an Euler method and generated a fake data as a sequence of noisy observation of daily incidence. I could have used the ODE solving routine available in Stan as in my previous post. However, an SIR model solved via the Euler method can be extended mor easily (e.g., stochatic model). I also suspect it would be easier to combine with the other statistical modeling techinques (e.g., hierarchical model), which I am going to post later.\n\nSimulate the data\nGenerate \\(y_{1:N}\\) as a sequence of noisy observations of a daily incidence. Almost the same Stan model is used twice: once to create fake data and the second time to estimate parameters via HMC.\n\n\nStan model to create fake data\n\nstan_code_data &lt;- \"\ndata {\n  int&lt;lower=0&gt; N; // length of the data\n  int&lt;lower=0&gt; iter_per_day;\n  real dt;\n  real&lt;lower=0&gt; S0;\n  real&lt;lower=0&gt; I0;\n  real&lt;lower=0&gt; R0;\n  real&lt;lower=0&gt; CI0;\n  real&lt;lower=0&gt; phi;\n  real&lt;lower=0&gt; gamma;\n  real&lt;lower=0&gt; beta;\n}\n\nparameters {\n\n}\n\n\ntransformed parameters {\n  vector&lt;lower=0&gt;[N] daily_inf;\n  vector&lt;lower=0&gt;[N+1] S;\n  vector&lt;lower=0&gt;[N+1] I;\n  vector&lt;lower=0&gt;[N+1] R;\n  vector&lt;lower=0&gt;[N+1] CI;\n  \n  real&lt;lower=0&gt; st; // susceptible at time t\n  real&lt;lower=0&gt; it;\n  real&lt;lower=0&gt; rt;\n  real&lt;lower=0&gt; cit;  // cumulative infections at time t\n  real&lt;lower=0&gt; n; // total population size\n  real&lt;lower=0&gt; n_si; // number moving from S to I\n  real&lt;lower=0&gt; n_ir; // number moving from I to R\n  \n  S[1] = S0;\n  I[1] = I0;\n  R[1] = R0;\n  CI[1] = CI0;\n    \n  for (i in 2:(N+1)) {\n    st = S[i-1];\n    it = I[i-1];\n    rt = R[i-1];\n    cit = CI[i-1];\n    for (j in 1:iter_per_day) {\n      n = st + it + rt;\n      n_si = dt * beta * st * it / n;\n      n_ir = dt * gamma * it;\n      st = st - n_si;\n      it = it + n_si - n_ir;\n      rt = rt + n_ir;\n      cit = cit + n_si;\n    }\n    S[i] = st;\n    I[i] = it;\n    R[i] = rt;\n    CI[i] = cit;\n  }\n\n  for (i in 2:(N+1)) {\n    daily_inf[i-1] = CI[i] - CI[i-1];\n  }\n}\n\nmodel {\n\n}\n\ngenerated quantities {\n  array[N] int y_sim;\n  for (i in 1:N) {\n    y_sim[i] = neg_binomial_2_rng(daily_inf[i] + 1e-6, phi);\n    //y_sim[i] = poisson_rng(daily_inf[i] + 1e-6);\n  }\n}\n\"\n\n\n\n\n\n\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\nmod_data &lt;- stan_model(model_code=stan_code_data, verbose=TRUE)\n\nN=31L \nS0=999\nI0=1\ndt=0.1\n\ndata = list(N=N,\n  S0=S0, I0=I0, R0=0, CI0=0, iter_per_day=round(1/dt),\n  phi=5000, beta=0.3, gamma=0.2, dt=dt)\n\nset.seed(42)\n\nfit = sampling(mod_data, data=data,\n                iter = 200,\n                chains = 1,\n                cores = 1, \n                algorithm = \"Fixed_param\")\n\ndf = as.data.frame(fit)\ny_sim = df[, grepl(\"^y_sim.*\", names(df))]\nplot(1:N, as.numeric(y_sim[1,]))\n\n# saveRDS(y_sim, \"stan_sir_daily_inc_NB.rds\")\n\n\nStan model to estimate parameters\nNote that there are two parameters in the parameters block\n\nstan_code_est &lt;- \"\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; iter_per_day;\n  real dt;\n  int&lt;lower=0&gt; y[N];\n  real&lt;lower=0&gt; S0;\n  real&lt;lower=0&gt; I0;\n  real&lt;lower=0&gt; R0;\n  real&lt;lower=0&gt; CI0;\n  real&lt;lower=0&gt; phi;\n  real&lt;lower=0&gt; r;\n}\n\nparameters {\n  real&lt;lower=0&gt; gamma;\n  real&lt;lower=0&gt; beta;\n}\n\ntransformed parameters {\n  vector&lt;lower=0&gt;[N] daily_inf;\n  vector&lt;lower=0&gt;[N+1] S;\n  vector&lt;lower=0&gt;[N+1] I;\n  vector&lt;lower=0&gt;[N+1] R;\n  vector&lt;lower=0&gt;[N+1] CI;\n  \n  real&lt;lower=0&gt; st; // susceptible at time t\n  real&lt;lower=0&gt; it;\n  real&lt;lower=0&gt; rt;\n  real&lt;lower=0&gt; cit;  // cumulative infections at time t\n  real&lt;lower=0&gt; n; // total population size\n  real&lt;lower=0&gt; n_si; // number moving from S to I\n  real&lt;lower=0&gt; n_ir; // number moving from I to R\n  \n  S[1] = S0;\n  I[1] = I0;\n  R[1] = R0;\n  CI[1] = CI0;\n    \n  for (i in 2:(N+1)) {\n    st = S[i-1];\n    it = I[i-1];\n    rt = R[i-1];\n    cit = CI[i-1];\n    for (j in 1:iter_per_day) {\n      n = st + it + rt;\n      n_si = dt * beta * st * it / n;\n      n_ir = dt * gamma * it;\n      st = st - n_si;\n      it = it + n_si - n_ir;\n      rt = rt + n_ir;\n      cit = cit + n_si;\n    }\n    S[i] = st;\n    I[i] = it;\n    R[i] = rt;\n    CI[i] = cit;\n  }\n\n  for (i in 2:(N+1)) {\n    daily_inf[i-1] = CI[i] - CI[i-1];\n  }\n}\n\n\nmodel {\n  beta ~ exponential(r);\n  gamma ~ exponential(r);\n\n  y ~ neg_binomial_2(daily_inf + 1e-6, phi);\n\n}\"\n\n\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nmod_est = stan_model(model_code=stan_code_est, verbose=TRUE)\n\ny_sim = readRDS(\"stan_sir_daily_inc_NB.rds\")\n\ny = as.integer(y_sim[1,])\ndata = list(N=length(y), y=y, S0=S0, I0=I0, R0=0, CI0=0, iter_per_day=round(1/dt), phi=50, r=0.1, dt=dt)\n\nset.seed(42)\n\nfit = sampling(mod_est, data=data,\n                iter = 2000,\n                chains = 4,\n                cores = min(parallel::detectCores(), 4))\n\n# saveRDS(fit, \"stan_sir_daily_inc_NB_fit.rds\")\n\nTrace plot\n\nlibrary(rstan)\nfit = readRDS(\"stan_sir_daily_inc_NB_fit.rds\")\ntraceplot(fit, c(\"beta\",\"gamma\"))\n\n\n\n\n\nPlot the results\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\ndf = as.data.frame(fit)\nd &lt;- df[, c(\"beta\",\"gamma\")]\ndlong &lt;- tidyr::pivot_longer(d, cols=c(\"beta\",\"gamma\"),\n                             names_to=\"param\")        \n# dlong$param &lt;- as.factor(dlong$param)\nlibrary(dplyr)\nggplot(dlong)+ \n  geom_histogram(aes(x=value))+\n  facet_wrap(~param, nrow=1, scales = \"free_x\")+\n  geom_vline(data=filter(dlong, param ==\"beta\"), aes(xintercept=0.3), color=\"firebrick\", linewidth=1.2) +\n  geom_vline(data=filter(dlong, param ==\"gamma\"), aes(xintercept=0.2), color=\"firebrick\", linewidth=1.2)\n\n\n\n# ggsave(\"sir_euler_stan_param.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "blog/posts/survivor-bias/index.html",
    "href": "blog/posts/survivor-bias/index.html",
    "title": "Survivor bias",
    "section": "",
    "text": "In his lecture titled “The Soul of statistics” Joseph Blitzstein talks about a survivor bias (or conditioning more broadly) Dr. Derek Muller also talks about various examples of Korean houses in Bukchon Hanok Village on his YouTube\nSurvivor bias is today’s topic and the following was written mostly by GPT 4.\nSurvivor Bias: What Remains Tells Only Half the Story\nImagine walking through a forest and noticing the tallest trees. You marvel at their height and strength, thinking that this is the natural order of things. But what about the saplings and smaller trees that didn’t survive? This is the essence of survivor bias.\nWhat is Survivor Bias?\nSurvivor bias, or survivorship bias, is a logical error of focusing on the people or things that made it past some selection process and overlooking those that did not, typically because of their lack of visibility. This can lead to false conclusions in numerous different ways.\nA Classic Example: WWII Airplanes\nDuring World War II, military officials examined planes returning from combat missions to determine where they were most frequently hit by enemy fire. The plan was simple: reinforce these areas to improve the aircraft’s survival rate. The bullet holes were predominantly in the wings, body, and tail. So, it might seem logical to reinforce these parts.\nHowever, a statistician named Abraham Wald pointed out a flaw in this reasoning. The planes they were inspecting had survived. The real question was: where were the bullet holes on the planes that didn’t return? Wald hypothesized that the missing airplanes had been hit in the engine, a critical area absent of damage in the returning planes. By only looking at the survivors, the military had almost made a grave error.\nWhy Does It Matter?\nSurvivor bias can skew our understanding and lead to incorrect conclusions in various fields:\n\nBusiness: When studying successful companies, we might conclude that their practices are best. But what about companies that followed the same practices and failed?\nMedicine: If we only focus on patients who return for follow-up after treatment, we might miss side effects or outcomes in those who didn’t return.\nCulture: Celebrating only the top artists or authors might make us think that a particular style or theme is the key to success, overlooking other potential talents.\n\nOvercoming Survivor Bias\nAwareness is the first step. Whenever you’re examining successes, ask yourself: “What am I not seeing?” Seek out the failures, the unseen, the unreturned. By considering the whole picture, not just the apparent survivors, you get a clearer, more accurate view of reality.\nIn conclusion, while it’s natural to focus on winners and success stories, it’s crucial to remember the unseen and unspoken failures. They often hold the most valuable lessons."
  },
  {
    "objectID": "blog/posts/universal_differential_eqn_julia/index.html",
    "href": "blog/posts/universal_differential_eqn_julia/index.html",
    "title": "Universal differential equation using Julia",
    "section": "",
    "text": "Universal differential equation (UDE)\nThe UDE refers to an approach to embed the machine learning into differential equations. The resulting UDE has some parts of the equation replaced by universal approximators i.e., neural network (NN). The UDE model approach allows us to approximate a wide, if not infinite, variety of functional relationships. As an example, I will test how well the UDE model approach will approximate a sub-exponential growth model, which is challenging to fit if we use an exponential growth model.\nI am using Julia for the UDE approach as it appeared that the Julia is the most advanced in this regard.\n\n# SciML (Scientific Machine Learning) Tools\nusing OrdinaryDiffEq, SciMLSensitivity\nusing Optimization, OptimizationOptimisers, OptimizationOptimJL\n\n# Standard Libraries\nusing LinearAlgebra, Statistics\n\n# External Libraries\nusing ComponentArrays, Lux, Zygote, Plots, StableRNGs\ngr()\n\nPlots.GRBackend()\n\n\n# Set a random seed for reproducible behaviour\nrng = StableRNG(1111)\n\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000008af)\n\n\n\n\nData generation\nThe SIR model with a sub-exponential growth is used.\n\nfunction sir_subexp!(du, u, p, t)\n    α, β, γ = p \n    du[1] = - β*u[1]*u[2]^α\n    du[2] = + β*u[1]*u[2]^α - γ*u[2]\n    du[3] = + γ*u[2]\nend\n\nsir_subexp! (generic function with 1 method)\n\n\n# Define the experimental parameter\ntspan = (0.0, 20.0);\n# u0 = 5.0f0 * rand(rng, 2)\nu0 = [0.99, 0.01, 0.0];\np_ = [0.8, 0.4, 0.2];\nprob = ODEProblem(sir_subexp!, u0, tspan, p_);\nsolution = solve(prob, Tsit5(), abstol = 1e-12, reltol = 1e-12, saveat = 1.0);\n\n# Add noise in terms of the mean\nX = Array(solution);\nt = solution.t;\n\nxbar = mean(X, dims=2);   \nnoise_magnitude = 5e-2;\nXn = X .+ (noise_magnitude * xbar) .* randn(rng, eltype(X), size(X));\n\nplot(solution, alpha = 0.75, color = :black, label = [\"True Data\" nothing]);\nscatter!(t, transpose(Xn), color = :red, label = [\"Noisy Data\" nothing])\n\n\n\n\n\n\nUDE model\n\n# Let's define our Universal Differential eqution\nrbf(x) = exp.(-(x .^ 2));\n\n# Multilayer FeedForward\nconst U = Lux.Chain(Lux.Dense(3, 5, rbf), Lux.Dense(5, 5, rbf), \nLux.Dense(5, 5, rbf), Lux.Dense(5, 1))\n\nChain(\n    layer_1 = Dense(3 =&gt; 5, rbf),       # 20 parameters\n    layer_2 = Dense(5 =&gt; 5, rbf),       # 30 parameters\n    layer_3 = Dense(5 =&gt; 5, rbf),       # 30 parameters\n    layer_4 = Dense(5 =&gt; 1),            # 6 parameters\n)         # Total: 86 parameters,\n          #        plus 0 states.\n\n# Get the initial parameters and state variables of the model\np, st = Lux.setup(rng, U)\n\n((layer_1 = (weight = Float32[-0.11597705 -0.5499123 0.10071843; -0.20088743 0.5602648 0.2718303; … ; -0.22440201 -0.57859105 0.7904316; -0.4619576 -0.62989676 0.18545352], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_2 = (weight = Float32[-0.043933477 -0.21508422 … 0.55779475 0.5849693; 0.0011237671 0.006483868 … 0.27549765 -0.2874395; … ; 0.5079049 -0.36002874 … 0.41297784 -0.5777891; -0.5179172 -0.60432595 … -0.18625909 0.06577149], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_3 = (weight = Float32[0.21934992 0.20916325 … -0.357856 -0.27426103; 0.59777355 -0.04514681 … 0.22668682 0.73459923; … ; 0.36797842 0.13955377 … 0.28912562 0.20840885; -0.33154675 0.035615936 … 0.011346816 -0.13401343], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_4 = (weight = Float32[-0.49593353 -0.68478346 … -0.4632702 -0.1476636], bias = Float32[0.0;;])), (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()))\n\nconst _st = st\n\n(layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple())\n\n\n# Define the hybrid model\nfunction ude_dynamics!(du, u, p, t, p_true)\n    û = U(u, p, _st)[1] # Network prediction\n    du[1] = dS = - û[1]\n    du[2] = dI = + û[1] - p_true[3]*u[2] \n    du[3] = dR = + p_true[3]*u[2] \nend\n\nude_dynamics! (generic function with 1 method)\n\n\n# Closure with the known parameter\nnn_dynamics!(du, u, p, t) = ude_dynamics!(du, u, p, t, p_)\n\nnn_dynamics! (generic function with 1 method)\n\n# Define the problem\nprob_nn = ODEProblem(nn_dynamics!, Xn[:, 1], tspan, p)\n\nODEProblem with uType Vector{Float64} and tType Float64. In-place: true\ntimespan: (0.0, 20.0)\nu0: 3-element Vector{Float64}:\n 1.0239505612968622\n 0.0034985090690380412\n 0.00031492340046744696\n\n\n# I don't understand the details of the algorithm\n# sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))\n# I just adopted what's provided in the web page: \n# https://docs.sciml.ai/Overview/stable/showcase/missing_physics/\n    \nfunction predict(θ, X = Xn[:, 1], T = t)\n    _prob = remake(prob_nn, u0 = X, tspan = (T[1], T[end]), p = θ)\n    Array(solve(_prob, Tsit5(), saveat = T,\n                abstol = 1e-6, reltol = 1e-6,\n                sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))))\nend\n\npredict (generic function with 3 methods)\n\n\nfunction loss(θ)\n    Xhat = predict(θ)\n    mean(abs2, Xn .- Xhat)\nend\n\nloss (generic function with 1 method)\n\n\nlosses = Float64[];\n\ncallback = function (p, l)\n    push!(losses, l)\n    if length(losses) % 50 == 0\n        println(\"Current loss after $(length(losses)) iterations: $(losses[end])\")\n    end\n    return false\nend\n\n#125 (generic function with 1 method)\n\n\nadtype = Optimization.AutoZygote();\noptf = Optimization.OptimizationFunction((x, p) -&gt; loss(x), adtype);\noptprob = Optimization.OptimizationProblem(optf, ComponentVector{Float64}(p));\n\nmxiter = 2000\n\n2000\n\nres1 = Optimization.solve(optprob, ADAM(), callback = callback, maxiters = mxiter);\nprintln(\"Training loss after $(length(losses)) iterations: $(losses[end])\")\n\nTraining loss after 2001 iterations: 0.0015564208691683874\n\n\n# You can optimize further by using LBFGS\noptprob2 = Optimization.OptimizationProblem(optf, res1.u)\n\nOptimizationProblem. In-place: true\nu0: ComponentVector{Float64}(layer_1 = (weight = [-0.05170182389621984 -0.6044817578839353 0.030221881763510722; -0.32123561819724095 0.6510350091471692 0.44802414729182577; … ; -0.29710021515609997 -0.5198718000241755 0.8494439396600794; -0.5240838499417148 -0.5805885464625195 0.2207764494848388], bias = [0.07034258756689579; -0.1248750018606046; … ; -0.10460911230943246; -0.09231330051875483;;]), layer_2 = (weight = [-0.16444552836729504 -0.39149348409521373 … 0.4595876886836963 0.48596336484497255; 0.06110486638492416 0.09809356179876788 … 0.33257093321228715 -0.23929486432898225; … ; 0.44222617028480327 -0.07489082170130384 … 0.38613240122653225 -0.6098094519525386; -0.4255467375078114 -0.39126782596548754 … -0.07050700982314294 0.183500531584539], bias = [-0.10242371127901419; 0.05522672750843921; … ; -0.06460784045609771; 0.08515934187149939;;]), layer_3 = (weight = [0.30753421402862036 0.2947114335480522 … -0.26966507778144233 -0.19068344291102465; 0.6803575575897453 0.0350253144116066 … 0.3095389929942777 0.8122600471146871; … ; 0.4338542747333688 0.20165221719158863 … 0.35259992082403724 0.2701023387838381; -0.2070809282199827 0.1890743367589869 … 0.1659803847569096 -0.0012012794338499672], bias = [0.08539245136971987; 0.08006728472152563; … ; 0.06194248786256519; 0.15306636356417658;;]), layer_4 = (weight = [-0.42084525587118515 -0.6144846254382511 … -0.40340199958868356 -0.06553529479212013], bias = [0.09174711461704013;;]))\n\nres2 = Optimization.solve(optprob2, Optim.LBFGS(), callback = callback, maxiters = 1000);\nprintln(\"Final training loss after $(length(losses)) iterations: $(losses[end])\")\n\nFinal training loss after 3002 iterations: 0.00037095070511105146\n\n\n# Rename the best candidate\np_trained = res2.u\n\nComponentVector{Float64}(layer_1 = (weight = [0.18588559475980856 -0.678836961079842 0.003164511566198509; -0.2117593049546201 0.7659777764210745 0.5001880949174726; … ; -0.12793809908258996 -0.5806165465015546 0.7359889411458274; -0.5987978703086538 -0.8542736922318693 0.22071269547796915], bias = [0.20283880857556238; 0.1442364184846448; … ; -0.10951065483152145; -0.43136285105428973;;]), layer_2 = (weight = [-0.7722740902742339 -1.1539959612185595 … -0.2557276161227372 0.04069177034071063; 0.11613000554442242 -0.06879626497651166 … 0.49503733065196687 -0.20521706786070337; … ; 0.49449245622560034 0.17333857621023685 … 0.3021120793343474 -0.7328812254592773; -0.2225923998099525 -0.1546676318277597 … 0.38305691443718165 0.48987085385328843], bias = [-0.7211379554471269; 0.18024471356943225; … ; -0.06082478805131034; 0.35625527871182344;;]), layer_3 = (weight = [0.06379174538587522 0.13743725900849454 … -0.37275930238632443 -0.4224270345491679; 1.0145108036936996 0.5041281988482341 … 0.7904247027437314 0.9963675316648448; … ; 0.42914689744537293 0.2244122549609806 … 0.38639622079002267 0.24704090971935938; -0.2838037009222233 0.13375316701238169 … 0.12509351575197344 -0.08653639834262729], bias = [-0.1263271345517243; 0.44502442954219273; … ; 0.07338220202701212; 0.0804054739454783;;]), layer_4 = (weight = [-0.5832026087107504 -0.34311500443215825 … -0.3606462665796045 -0.2121086546094451], bias = [0.3482487547293952;;]))\n\n\n# Plot the losses\npl_losses = plot(1:mxiter, losses[1:mxiter], yaxis = :log10, xaxis = :log10,\n                 xlabel = \"Iterations\", ylabel = \"Loss\", label = \"ADAM\", color = :blue)\n\n\n\nplot!((mxiter+1):length(losses), losses[(mxiter+1):end], yaxis = :log10, xaxis = :log10,\n      xlabel = \"Iterations\", ylabel = \"Loss\", label = \"BFGS\", color = :red)\n\n\n\n\n\n## Analysis of the trained network\n# Plot the data and the approximation\nts = first(solution.t):(mean(diff(solution.t)) / 2):last(solution.t)\n\n0.0:0.5:20.0\n\nXhat = predict(p_trained, Xn[:, 1], ts)\n\n3×41 Matrix{Float64}:\n 1.02395      1.00815     0.992781   …  0.150512  0.141294  0.131538\n 0.00349851   0.0181957   0.0310887     0.16279   0.156074  0.15051\n 0.000314923  0.00141772  0.0038941     0.714463  0.730397  0.745716\n\n# Trained on noisy data vs real solution\npl_trajectory = plot(ts, transpose(Xhat), xlabel = \"t\", \n                     ylabel = \"S(t), I(t), R(t)\", color = :red,\n                     label = [\"UDE Approximation\" nothing])\n\n\n\nscatter!(solution.t, transpose(Xn), color = :black, label = [\"Measurements\" nothing])\n\n\n\n\n\n# Ideal unknown interactions of the predictor\n# Ybar = [-p_[2] * (Xhat[1, :] .* Xhat[2, :])'; p_[3] * (Xhat[1, :] .* Xhat[2, :])']\n# Ybar = [p_[2] .* Xhat[1,:] .* (Xhat[2,:].^p_[1])]\nYbar = transpose([p_[2] * Xhat[1,i] * (Xhat[2,i].^p_[1]) for i ∈ 1:41, j ∈ 1:1])\n\n1×41 transpose(::Matrix{Float64}) with eltype Float64:\n 0.00444064  0.0163516  0.024717  …  0.0140907  0.0127893  0.0115655\n\n# Neural network guess\nYhat = U(Xhat, p_trained, st)[1]\n\n1×41 Matrix{Float64}:\n 0.032401  0.0309925  0.0306477  0.031214  …  0.0180941  0.0188726  0.02026\n\n\npl_reconstruction = plot(ts, transpose(Yhat), xlabel = \"t\", \n                         ylabel = \"U(S,I,R)\", color = :red,\n                         label = [\"UDE Approximation\" nothing]);\n\nplot!(ts, transpose(Ybar), color = :black, label = [\"True Interaction\" nothing])\n\n\n\n\n\n# Plot the error\npl_reconstruction_error = plot(ts, norm.(eachcol(Ybar - Yhat)), yaxis = :log, xlabel = \"t\",\n                               ylabel = \"L2-Error\", label = nothing, color = :red);\npl_missing = plot(pl_reconstruction, pl_reconstruction_error, layout = (2, 1));\n\npl_overall = plot(pl_trajectory, pl_missing)"
  },
  {
    "objectID": "blog/posts/vaccine_effectiveness_waning/index.html",
    "href": "blog/posts/vaccine_effectiveness_waning/index.html",
    "title": "Waning of vaccine effectiveness",
    "section": "",
    "text": "The protection derived from vaccination often wanes over time and require the second or the third doses (so-called booster doses). For example, the study showed the efficacy of cholera vaccines over five years. The vaccine efficacy (VE) over the period seems to indicate that the VE wanes over time. In this post, we will develop a \\(SEIR\\) model to illustrate the waning of vaccine-derived immunity.\n\n\\(SEIR\\) model with vaccination\n\nseir_vacc_trial_jl &lt;- function(u, p, t){\n  # vaccine recipients lose partial immunity over time.\n  # this is implemented in a way that individuals in the V state\n  # decreases and transitions to the S state\n  S &lt;- u[1]; E &lt;- u[2]; I &lt;- u[3]; R &lt;- u[4]; C &lt;- u[5]\n \n  # vaccinated cohort\n  V &lt;- u[6]; # vaccinated and partially protected\n  VS &lt;- u[7]; # vaccinated but immunity waned and fully susceptible\n  VE &lt;- u[8]; VI &lt;- u[9]; VR &lt;- u[10]; VC &lt;- u[11]\n  \n  pop &lt;- S + E + I + R\n  popV &lt;- V + VS + VE + VI + VR\n\n  epsilon &lt;- p[1] # 1/latent period\n  gamma &lt;- p[2] # 1/duration of infectiousness\n  beta &lt;- p[3] # transmission rate\n  mu &lt;- p[4] # death rate is applied and population size decreases over time\n  omega &lt;- p[5] # 1 / duration of natural immunity\n  omega_v &lt;- p[6] # 1 / duration of partial vaccine-derived immunity\n  ve &lt;- p[7] # vaccine efficacy\n  # vaccinated and unvaccinated population mix randomly\n  foi &lt;- beta * (I+VI) / (pop + popV) # force of infection\n  \n  muEI &lt;- epsilon\n  muIR &lt;- gamma\n  muRS &lt;- omega\n  muVS &lt;- omega_v\n  \n  # differential equations\n  dS &lt;- - foi*S + muRS*R - mu*S\n  dE &lt;- foi*S - muEI*E - mu*E\n  dI &lt;- muEI*E - muIR*I - mu*I\n  dR &lt;- muIR*I - muRS*R - mu*R\n  dC &lt;- muEI*E\n  \n  dV &lt;- - foi*(1-ve)*V - muVS*V - mu*V\n  dVS &lt;- - foi*VS + muVS*V + muRS*VR - mu*VS \n  dVE &lt;- foi*((1-ve)*V + VS) - muEI*VE - mu*VE\n  dVI &lt;- muEI*VE - muIR*VI - mu*VI\n  dVR &lt;- muIR*VI - muRS*VR - mu*VR\n  dVC &lt;- muEI*VE\n  \n  return(c(dS,dE,dI,dR,dC,dV,dVS,dVE,dVI,dVR,dVC))\n}\n\ndiffeqr package is used use Julia’s DifferentialEquations library\n\nlibrary(diffeqr)\nde &lt;- diffeqr::diffeq_setup()\n\nWe assume that cholera transmission is in the steady state in the setting where the vaccine efficacy trial is examined. Algebraic solutions to the steady states (shown below) are used as initial conditions to generate initial conditions.\n\nSs &lt;- \"((gamma + mu) * (epsilon + mu)) / (beta * epsilon)\"\nEs &lt;- \"- ((gamma + mu) * (mu + omega) * ((gamma + mu) * (epsilon + mu) - beta * epsilon)) / (beta * epsilon * (gamma * (epsilon + mu + omega) + (epsilon + mu) * (mu + omega)))\"\n\nIs &lt;- \"((mu + omega) * (beta * epsilon - (gamma + mu) * (epsilon + mu))) / (beta * omega * (gamma + epsilon + mu) + beta * (gamma + mu) * (epsilon + mu))\"\nRs &lt;- \"(beta * gamma * epsilon - gamma * (gamma + mu) * (epsilon + mu)) / (beta * omega * (gamma + epsilon + mu) + beta * (gamma + mu) * (epsilon + mu))\"\n\n\n\nSimulation of a clinical trial\nThe basic idea is to use the steady states as initial conditions and therefore, cholera transmission is sustained as new susceptibles are supplied through birth and waning of vaccine-derived and natural immunity. In this setting, a clinical trial is implemented by moving some portion of the population to the vaccinated cohort. Note that the proportion of the vaccinated population, \\(f\\), is important in the subsequent dynamics. For example, we know that if \\(f&gt; 1- \\frac{1}{R_0}\\), cholera infections will die out. In this case, we can not measure the relative risk of cholera among vaccinated and unvaccinated population over a period based on the cumulative incidence. Even if \\(f &lt; 1- \\frac{1}{R_0}\\), moving some portion of the population into vaccinated states will perturb the steady states that were determined based on and a series of outbreaks may follow before the system reaches new steady states eventually.\nSince our objective is to vaccine efficacy in this dynamic population based on cumulative incidence among vaccinated and unvaccinated people, and we used the steady states without the vaccinated population, we can set the proportion of the vaccinated population small that the system more or less maintains the states without vaccination. Wh\n\nN &lt;- 1e5 # unvaccinated population\nf &lt;- 1e-4 # a very small proportion of the population is vaccinated\nR0 &lt;- 2\nepsilon &lt;- 1/1.4\ngamma &lt;- 1/2\nbeta &lt;- R0*gamma\n# mu &lt;- 1/(65*365) # natural death rate\nmu &lt;- 0 # natural death rate\nomega &lt;- 1/(40*365) # natural immunity waning rate\nomega_v &lt;- 1/(10*365) # vaccine-derived immunity waning rate\nve &lt;- 0.7 # vaccine efficacy (instantaneous hazard ratio)\n# vaccine efficacy based on CI will differ and we will eventually estimate\n# this parameter along with omega_v\nparams &lt;- c(epsilon=epsilon, gamma=gamma, beta=beta, mu=mu, \n            omega=omega, omega_v=omega_v, ve=ve)\n# steady states for given parameters\nstates0 &lt;- list(S=Ss, E=Es, I=Is, R=Rs)\nsteadys0 &lt;- sapply(states0, function(x) eval(parse(text = x)))\n# initial distribution of the population across the states\nu0 &lt;- c(steadys0*N,C=0,V=0,VS=0,VE=0,VI=0,VR=0,VC=0)\nu0[c(\"V\",\"VE\",\"VI\",\"VR\")] &lt;- as.numeric(steadys0*N*f/(1-f))\n\ntend &lt;- 20.0*365 # measure vaccin\ntspan &lt;- c(0.0, tend)\n\nprob &lt;- de$ODEProblem(seir_vacc_trial_jl, u0, tspan, params)\nsol &lt;- de$solve(prob, de$Tsit5(), saveat=1)\n\nmat &lt;- sapply(sol$u, identity)\nudf &lt;- as.data.frame(t(mat))\nout &lt;- cbind(data.frame(t=sol$t), udf)\nnames(out) &lt;- c(\"t\",\"S\",\"E\",\"I\",\"R\",\"C\",\"V\",\"VS\",\"VE\",\"VI\",\"VR\",\"VC\")\n# cumulative incidence\nplot(1:nrow(out)/365, out[,\"C\"], type=\"l\", ylab=\"cumulative incidence\", xlab=\"year\", col=\"black\")\nlines(1:nrow(out)/365,out[,\"VC\"], col=\"firebrick\")\nlegend(\"topleft\", \n  legend=c(\"No vaccine\", \"Vaccine\"), \n  col=c(\"black\", \"firebrick\"), \n  lty= 1,\n  bty = \"n\", \n  cex = 1.0, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.02,0.02))\n\n\n\n\nMeasure vaccine impact\n\nnovacc &lt;- c(\"S\",\"E\",\"I\",\"R\")\nvacc &lt;- c(\"V\",\"VS\",\"VE\",\"VI\",\"VR\")\ndve &lt;- rep(NA, nrow(out)) # direct vaccine effectiveness measured daily\nfor(i in 1:nrow(out)){\n  dve[i] &lt;- 1 - (out[i,\"VC\"]/sum(out[i,vacc]))/(out[i,\"C\"]/sum(out[i,novacc]))\n}\nplot(1:length(dve)/365,dve, type=\"l\", ylab=\"direct VE\", xlab=\"year\")\n\n\n\n\n\n\nEstimating VE and \\(\\omega_V\\)\nLet’s suppose we want to the following dataset showing the vaccine effec\n\nve_obs &lt;- c(76,72,68,63,58,52,46,39,32,24,15)/100\n\nCreate a function to measure vaccine efficacy from the model based on cumulative incidence\n\nmeasure_vacc_eff &lt;- function(p, times, N=1e5, f=1e-4){\n  params[\"ve\"] &lt;- p[1]\n  params[\"omega_v\"] &lt;- p[2]\n  u0 &lt;- c(steadys0*N,C=0,V=N,VS=0,VE=0,VI=0,VR=0,VC=0)\n  u0[c(\"V\",\"VE\",\"VI\",\"VR\")] &lt;- as.numeric(steadys0*N*f/(1-f))\n  prob &lt;- de$ODEProblem(seir_vacc_trial_jl, u0, c(0.0, max(times)+1), params)\n  sol &lt;- de$solve(prob, de$Tsit5(), saveat=1)\n  mat &lt;- sapply(sol$u, identity)\n  udf &lt;- as.data.frame(t(mat))\n  out &lt;- cbind(data.frame(t=sol$t), udf)\n  names(out) &lt;- c(\"t\",\"S\",\"E\",\"I\",\"R\",\"C\",\n                  \"V\",\"VS\",\"VE\",\"VI\",\"VR\",\"VC\")\n  novacc &lt;- c(\"S\",\"E\",\"I\",\"R\")\n  vacc &lt;- c(\"V\",\"VS\",\"VE\",\"VI\",\"VR\")\n  \n  ve_sim &lt;- rep(NA, length(times))\n  for (i in 1:length(times)) {\n    ve_sim[i] &lt;- 1 - \n      ((out[times[i],\"VC\"])/sum(out[times[i],vacc])) / \n      ((out[times[i],\"C\"])/sum(out[times[i],novacc]))\n  }\n  return(ve_sim)\n}\n\nDefine sum of squared difference to evaluate the difference between the model and the observation\n\n# the initial VE is not measured from the model\ntimes = seq(6,by=6,length.out=10)*30 # times to measure VE\nssq &lt;- function(p){\n  ve_sim &lt;- measure_vacc_eff(p, times=times)\n  sum((ve_obs - c(p[1],ve_sim))^2)  \n}\n# check for some predetermined ve and omega\n# ssq(p=c(0.8,1/3650)) must be smaller than ssq(p=c(0.4,1/365))\nssq(p=c(0.8,1/3650))\n\n[1] 0.5765074\n\nssq(p=c(0.4,1/365))\n\n[1] 1.260319\n\n\nUse the nlminb to identify the parameter values that minimize the ssq.\n\nfit &lt;- nlminb(c(0.2, 1/(2*365)), objective=ssq, \n              lower=c(0.1,1/(1000*365)),\n              upper=c(0.99,0.99))\nve_obs\n\n [1] 0.76 0.72 0.68 0.63 0.58 0.52 0.46 0.39 0.32 0.24 0.15\n\nround(c(fit$par[1], measure_vacc_eff(p=fit$par, times)), digits=2)\n\n [1] 0.83 0.73 0.65 0.58 0.52 0.46 0.42 0.38 0.34 0.31 0.29\n\n\n\n# png(\"vacc_eff_waning.png\")\nplot(0:10/2, ve_obs, col=\"firebrick\", \n     ylim=c(0,1), xlab=\"year\", ylab=\"direct VE\",\n     type=\"p\", pch=0)\n\nlines(0:10/2,c(fit$par[1], measure_vacc_eff(p=fit$par, times)),\n      col=\"black\")\nlegend(\"topright\", \n  legend=c(\"Data\", \"Model\"), \n  col=c(\"firebrick\", \"black\"), \n  lty= c(0,1),\n  pch=c(0,NA),\n  bty = \"n\", \n  cex = 1.0, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.02,0.02))\n\n\n\n# dev.off()"
  },
  {
    "objectID": "blog/posts/vacc_waning_nls_erlang/index.html",
    "href": "blog/posts/vacc_waning_nls_erlang/index.html",
    "title": "Modeling the waning of the vaccine-derived immunity in the ODE model",
    "section": "",
    "text": "The use of ordinary differential equation (ODE) models to simulate disease spread and evaluate vaccine impact is growing. Waning of vaccine-derived immunity often follows an exponential distribution in these models. However, as with the case with incubation period incubation period that we examined in the previous post, exponential distribution may not accurately depict the waning of vaccine-derived immunity.\nIn this post, I explore how to model the waning of vaccine efficacy over time using an ODE framework, utilizing the data from a study by Lee et al. Specifically, I examine how various cumulative distributions can be applied to acurately represent the diminishing percentage of of the vaccine protection over time, through the use of R code examples.\n\n# data from Table S4, Lee et al. (2020) Lancet Glob Health\ndat = data.frame(month = rep(seq(0,60,by=6),4),\n                 age = c(rep(\"Adult\", 11*2), rep(\"Kid\", 11*2)),\n                 type = rep(\"data\", 11*4),\n                 regimen = rep(c(rep(\"two doses\", 11), rep(\"one dose\", 11)),2),\n                 val = c(c(76,72,68,63,58,52,46,39,32,24,15)/100,\n                 c(76,72,68,0,0,0,0,0,0,0,0)/100,\n                 c(36,34,32,30,27,24,22,18,15,11,7)/100,\n                 c(36,34,32,0,0,0,0,0,0,0,0)/100))\n\nlibrary(dplyr)\n\nds = vector(\"list\", 4)\nds[[1]] = filter(dat, age==\"Adult\", type==\"data\", regimen==\"two doses\")\nds[[2]] = filter(dat, age==\"Kid\", type==\"data\", regimen==\"two doses\")\nds[[3]] = filter(dat, age==\"Adult\", type==\"data\", regimen==\"one dose\")\nds[[4]] = filter(dat, age==\"Kid\", type==\"data\", regimen==\"one dose\")\n\nfits_gamma_shape_2 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_2[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=2, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_3 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_3[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=3, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_4 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_4[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=4, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_5 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_5[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=5, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_8 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_8[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=8, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_exp &lt;- list()\n\nfor (i in 1:4) {\n fits_exp[[i]] &lt;- nls(val ~ \n        val[1]*pexp(month, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(c(rep(\"Adult\", 101), rep(\"Kid\", 101)), 2),\n                 type = rep(\"model\", 101*4),\n                 regimen = c(rep(\"two doses\", 101*2), rep(\"one dose\", 101*2)),\n                 val = c(predict(fits_gamma_shape_2[[1]], newdata = newd),\n                          predict(fits_gamma_shape_2[[2]], newdata = newd),\n                          predict(fits_gamma_shape_2[[3]], newdata = newd),\n                          predict(fits_gamma_shape_2[[4]], newdata = newd)))\n                 \nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(dat) +\n  geom_point(aes(month, val, color=regimen))+\n  geom_line(data=pred, aes(month, val, color=regimen)) +\n  facet_wrap(~age) +\n  ggtitle(\"Gamma decay with shape=2\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")\n\n\n\n\n\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(c(rep(\"Adult\", 101), rep(\"Kid\", 101)), 2),\n                 type = rep(\"model\", 101*4),\n                 regimen = c(rep(\"two doses\", 101*2), rep(\"one dose\", 101*2)),\n                 val = c(predict(fits_gamma_shape_3[[1]], newdata = newd),\n                          predict(fits_gamma_shape_3[[2]], newdata = newd),\n                          predict(fits_gamma_shape_3[[3]], newdata = newd),\n                          predict(fits_gamma_shape_3[[4]], newdata = newd)))\n                 \nggplot(dat) +\n  geom_point(aes(month, val, color=regimen))+\n  geom_line(data=pred, aes(month, val, color=regimen)) +\n  facet_wrap(~age) +\n  ggtitle(\"Gamma decay with shape=3\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")\n\n\n\n\n\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(c(rep(\"Adult\", 101), rep(\"Kid\", 101)), 2),\n                 type = rep(\"model\", 101*4),\n                 regimen = c(rep(\"two doses\", 101*2), rep(\"one dose\", 101*2)),\n                 val = c(predict(fits_exp[[1]], newdata = newd),\n                          predict(fits_exp[[2]], newdata = newd),\n                          predict(fits_exp[[3]], newdata = newd),\n                          predict(fits_exp[[4]], newdata = newd)))\n                 \nggplot(dat) +\n  geom_point(aes(month, val, color=regimen))+\n  geom_line(data=pred, aes(month, val, color=regimen)) +\n  facet_wrap(~age)+\n  ggtitle(\"Exponential decay\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")\n\n\n\n\nGiven that the data for adults following a two-dose regimen seem to be the most reliable, we evaluated the model’s residual errors in comparison to this data. The analysis of residual errors indicates that a gamma distribution with a shape parameter of 3 presents the optimal fit.\n\nsummary(fits_exp[[1]])\n\n\nFormula: val ~ val[1] * pexp(month, rate = exp(lograte), lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -4.09710    0.09047  -45.29 6.63e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0643 on 10 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 6.629e-06\n\nsummary(fits_gamma_shape_2[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 2, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -3.18619    0.02946  -108.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03005 on 10 degrees of freedom\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 6.246e-06\n\nsummary(fits_gamma_shape_3[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 3, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -2.72304    0.02375  -114.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02893 on 10 degrees of freedom\n\nNumber of iterations to convergence: 4 \nAchieved convergence tolerance: 1.595e-06\n\nsummary(fits_gamma_shape_4[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 4, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -2.41214    0.02884  -83.64 1.46e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0394 on 10 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 7.874e-07\n\nsummary(fits_gamma_shape_5[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 5, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -2.17801    0.03374  -64.56 1.94e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05009 on 10 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 9.615e-06\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(\"Adult\", 101*4),\n                 type = c(rep(\"Exponential\", 101),\n                          rep(\"Gamma w/ shape=2\", 101),\n                          rep(\"Gamma w/ shape=3\", 101),\n                          rep(\"Gamma w/ shape=4\", 101)),\n                 regimen = rep(\"two doses\", 101*4),\n                 val = c(predict(fits_exp[[1]], newdata = newd),\n                          predict(fits_gamma_shape_2[[1]], newdata = newd),\n                          predict(fits_gamma_shape_3[[1]], newdata = newd),\n                          predict(fits_gamma_shape_4[[1]], newdata = newd)))\n\nggplot() +\n  geom_point(data=filter(dat, age==\"Adult\", regimen==\"two doses\"), \n             aes(month, val, color=\"data\"), shape=19, size=3, alpha=0.3)+\n  geom_line(data=pred, aes(month, val, color=type)) +\n  ggtitle(\"Two-dose vaccine efficacy for adults\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")+\n  scale_colour_manual(\n    values=c(\"data\" = \"black\",  \"Exponential\" = \"firebrick\",\n             \"Gamma w/ shape=2\" = \"green4\",\"Gamma w/ shape=3\" = \"#6A3D9A\",\n              \"Gamma w/ shape=4\" = \"#FF7F00\"),\n    guide=guide_legend(override.aes = list(\n      linetype=c(\"data\"=\"blank\",\"Exponential\" = \"solid\",\n                          \"Gamma w/ shape=2\" = \"solid\",\n                          \"Gamma w/ shape=3\" = \"solid\",\n                          \"Gamma w/ shape=4\" = \"solid\"),\n      shape=c('data'=19,\"Exponential\" = NA,\n                          \"Gamma w/ shape=2\" = NA,\n                          \"Gamma w/ shape=3\" = NA,\n                          \"Gamma w/ shape=4\" = NA))))+ \n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "blog/posts/vacc_waning_nls_erlang/index.html#modeling-the-waning-of-the-vaccine-derived-immunity-in-the-ode-model",
    "href": "blog/posts/vacc_waning_nls_erlang/index.html#modeling-the-waning-of-the-vaccine-derived-immunity-in-the-ode-model",
    "title": "Modeling the waning of the vaccine-derived immunity in the ODE model",
    "section": "",
    "text": "The use of ordinary differential equation (ODE) models to simulate disease spread and evaluate vaccine impact is growing. Waning of vaccine-derived immunity often follows an exponential distribution in these models. However, as with the case with incubation period incubation period that we examined in the previous post, exponential distribution may not accurately depict the waning of vaccine-derived immunity.\nIn this post, I explore how to model the waning of vaccine efficacy over time using an ODE framework, utilizing the data from a study by Lee et al. Specifically, I examine how various cumulative distributions can be applied to acurately represent the diminishing percentage of of the vaccine protection over time, through the use of R code examples.\n\n# data from Table S4, Lee et al. (2020) Lancet Glob Health\ndat = data.frame(month = rep(seq(0,60,by=6),4),\n                 age = c(rep(\"Adult\", 11*2), rep(\"Kid\", 11*2)),\n                 type = rep(\"data\", 11*4),\n                 regimen = rep(c(rep(\"two doses\", 11), rep(\"one dose\", 11)),2),\n                 val = c(c(76,72,68,63,58,52,46,39,32,24,15)/100,\n                 c(76,72,68,0,0,0,0,0,0,0,0)/100,\n                 c(36,34,32,30,27,24,22,18,15,11,7)/100,\n                 c(36,34,32,0,0,0,0,0,0,0,0)/100))\n\nlibrary(dplyr)\n\nds = vector(\"list\", 4)\nds[[1]] = filter(dat, age==\"Adult\", type==\"data\", regimen==\"two doses\")\nds[[2]] = filter(dat, age==\"Kid\", type==\"data\", regimen==\"two doses\")\nds[[3]] = filter(dat, age==\"Adult\", type==\"data\", regimen==\"one dose\")\nds[[4]] = filter(dat, age==\"Kid\", type==\"data\", regimen==\"one dose\")\n\nfits_gamma_shape_2 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_2[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=2, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_3 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_3[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=3, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_4 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_4[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=4, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_5 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_5[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=5, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_8 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_8[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=8, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_exp &lt;- list()\n\nfor (i in 1:4) {\n fits_exp[[i]] &lt;- nls(val ~ \n        val[1]*pexp(month, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(c(rep(\"Adult\", 101), rep(\"Kid\", 101)), 2),\n                 type = rep(\"model\", 101*4),\n                 regimen = c(rep(\"two doses\", 101*2), rep(\"one dose\", 101*2)),\n                 val = c(predict(fits_gamma_shape_2[[1]], newdata = newd),\n                          predict(fits_gamma_shape_2[[2]], newdata = newd),\n                          predict(fits_gamma_shape_2[[3]], newdata = newd),\n                          predict(fits_gamma_shape_2[[4]], newdata = newd)))\n                 \nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(dat) +\n  geom_point(aes(month, val, color=regimen))+\n  geom_line(data=pred, aes(month, val, color=regimen)) +\n  facet_wrap(~age) +\n  ggtitle(\"Gamma decay with shape=2\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")\n\n\n\n\n\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(c(rep(\"Adult\", 101), rep(\"Kid\", 101)), 2),\n                 type = rep(\"model\", 101*4),\n                 regimen = c(rep(\"two doses\", 101*2), rep(\"one dose\", 101*2)),\n                 val = c(predict(fits_gamma_shape_3[[1]], newdata = newd),\n                          predict(fits_gamma_shape_3[[2]], newdata = newd),\n                          predict(fits_gamma_shape_3[[3]], newdata = newd),\n                          predict(fits_gamma_shape_3[[4]], newdata = newd)))\n                 \nggplot(dat) +\n  geom_point(aes(month, val, color=regimen))+\n  geom_line(data=pred, aes(month, val, color=regimen)) +\n  facet_wrap(~age) +\n  ggtitle(\"Gamma decay with shape=3\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")\n\n\n\n\n\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(c(rep(\"Adult\", 101), rep(\"Kid\", 101)), 2),\n                 type = rep(\"model\", 101*4),\n                 regimen = c(rep(\"two doses\", 101*2), rep(\"one dose\", 101*2)),\n                 val = c(predict(fits_exp[[1]], newdata = newd),\n                          predict(fits_exp[[2]], newdata = newd),\n                          predict(fits_exp[[3]], newdata = newd),\n                          predict(fits_exp[[4]], newdata = newd)))\n                 \nggplot(dat) +\n  geom_point(aes(month, val, color=regimen))+\n  geom_line(data=pred, aes(month, val, color=regimen)) +\n  facet_wrap(~age)+\n  ggtitle(\"Exponential decay\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")\n\n\n\n\nGiven that the data for adults following a two-dose regimen seem to be the most reliable, we evaluated the model’s residual errors in comparison to this data. The analysis of residual errors indicates that a gamma distribution with a shape parameter of 3 presents the optimal fit.\n\nsummary(fits_exp[[1]])\n\n\nFormula: val ~ val[1] * pexp(month, rate = exp(lograte), lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -4.09710    0.09047  -45.29 6.63e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0643 on 10 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 6.629e-06\n\nsummary(fits_gamma_shape_2[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 2, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -3.18619    0.02946  -108.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03005 on 10 degrees of freedom\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 6.246e-06\n\nsummary(fits_gamma_shape_3[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 3, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -2.72304    0.02375  -114.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02893 on 10 degrees of freedom\n\nNumber of iterations to convergence: 4 \nAchieved convergence tolerance: 1.595e-06\n\nsummary(fits_gamma_shape_4[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 4, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -2.41214    0.02884  -83.64 1.46e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0394 on 10 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 7.874e-07\n\nsummary(fits_gamma_shape_5[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 5, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -2.17801    0.03374  -64.56 1.94e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05009 on 10 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 9.615e-06\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(\"Adult\", 101*4),\n                 type = c(rep(\"Exponential\", 101),\n                          rep(\"Gamma w/ shape=2\", 101),\n                          rep(\"Gamma w/ shape=3\", 101),\n                          rep(\"Gamma w/ shape=4\", 101)),\n                 regimen = rep(\"two doses\", 101*4),\n                 val = c(predict(fits_exp[[1]], newdata = newd),\n                          predict(fits_gamma_shape_2[[1]], newdata = newd),\n                          predict(fits_gamma_shape_3[[1]], newdata = newd),\n                          predict(fits_gamma_shape_4[[1]], newdata = newd)))\n\nggplot() +\n  geom_point(data=filter(dat, age==\"Adult\", regimen==\"two doses\"), \n             aes(month, val, color=\"data\"), shape=19, size=3, alpha=0.3)+\n  geom_line(data=pred, aes(month, val, color=type)) +\n  ggtitle(\"Two-dose vaccine efficacy for adults\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")+\n  scale_colour_manual(\n    values=c(\"data\" = \"black\",  \"Exponential\" = \"firebrick\",\n             \"Gamma w/ shape=2\" = \"green4\",\"Gamma w/ shape=3\" = \"#6A3D9A\",\n              \"Gamma w/ shape=4\" = \"#FF7F00\"),\n    guide=guide_legend(override.aes = list(\n      linetype=c(\"data\"=\"blank\",\"Exponential\" = \"solid\",\n                          \"Gamma w/ shape=2\" = \"solid\",\n                          \"Gamma w/ shape=3\" = \"solid\",\n                          \"Gamma w/ shape=4\" = \"solid\"),\n      shape=c('data'=19,\"Exponential\" = NA,\n                          \"Gamma w/ shape=2\" = NA,\n                          \"Gamma w/ shape=3\" = NA,\n                          \"Gamma w/ shape=4\" = NA))))+ \n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Disease modeling for public health",
    "section": "",
    "text": "Blog\n  \n  \n     Publications\n  \n  \n    \n     GitHub\n  \n  \n    \n     Linkedin\n  \n  \n    \n     Twitter\n  \n  \n    \n     Email"
  },
  {
    "objectID": "index.html#welcome-to-my-website",
    "href": "index.html#welcome-to-my-website",
    "title": "Disease modeling for public health",
    "section": "Welcome to my website!",
    "text": "Welcome to my website!\nHi! My name is Jong-Hoon Kim. I am a theoretical epidemiologist who uses mathematical, statistical, and machine-learning models to study infectious disease epidemiology. My work involves developing theories about infectious disease transmission, making predictions, and assessing the effectiveness of intervention strategies to improve public health. Currently, in 2023, I work at the International Vaccine Institute, which is located in Seoul, South Korea. My current research primarily focuses on typhoid fever, non-typhoidal Salmonella, cholera, and COVID-19."
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Disease modeling for public health",
    "section": "Research interests",
    "text": "Research interests\nMy research focuses on leveraging disease models to generate actionable insights for controlling infectious diseases with real-world impact. Specifically, I aim to identify critical details necessary for generating actionable inferences regarding vaccination strategies for cholera and typhoid fever, tailored to specific locations and times. Additionally, I am interested in developing models to understand how human behavior in a community can enhance or hinder the effectiveness of disease control methods, including vaccination campaigns. Using that understanding, I aim to improve the control of infectious diseases and address other public health challenges."
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Publications",
    "section": "",
    "text": "Published\n2024\nHyolim Kang, Megan Auzenbergs, Hannah Clapham, Clara Maure, Jong-Hoon Kim, Henrik Salje, Christopher G. Taylor, Ahyoung Lim, John W. Edmunds, Sushant Sahastrabuddhe, Oliver J. Brady, and Kaja Abbas. (2024) \"Chikungunya seroprevalence, force of infection, and prevalence of chronic disability after infection in endemic and epidemic settings: a systematic review, meta-analysis, and modelling study.\" Lancet Infect Dis\n        \n        Preprint\n     \n        \n        Github\n     \n        \n        Published\n    \nAnna-Maria Hartner, Xiang Li, Hannah Clapham, ..., Jong-Hoon Kim, ..., and Katy Gaythorpe. (2024) \"Estimating the health effects of COVID-19-related immunisation disruptions in 112 countries during 2020-30: a modelling study.\" Lancet Glob Health\n        \n        Preprint\n     \n        \n        Github\n     \n        \n        Published\n    \nJong-Hoon Kim, Jungsoon Choi, Chaelin Kim, Gi Deok Pak, Prerana Parajulee, and et al. (2024) \"Mapping the incidence rate of typhoid fever in sub-Saharan Africa.\" PLoS Negl Trop Dis\n        \n        Preprint\n     \n        \n        Github\n     \n        \n        Published\n    \nJong-Hoon Kim, Bieke Tack, Fabio Fiorino, Elena Pettini, Christian S. Marchello, and et al. (2024) \"Examining geospatial and temporal distribution of invasive non-typhoidal Salmonella disease occurrence in sub-Saharan Africa: a systematic review and modelling study.\" BMJ Open\n        \n        Preprint\n     \n        \n        Github\n     \n        \n        Published\n    \nDejene Hailu, Yeonji Jeon, Abel Gedefaw, Jong-Hoon Kim, and et al. (2024) \"Dissecting Water, Sanitation, and Hygiene (WaSH) to Assess Risk Factors for Cholera in Shashemene, Oromia Region, Ethiopia.\" Clin Infect Dis\n        \n        Preprint\n     \n        \n        Published\n    \nJong-Hoon Kim, Prerana Parajulee, Thuy Tien Nguyen, Shreeya Wasunkar, and et al. (2024) \"Occurrence of human infection with Salmonella Typhi in sub-Saharan Africa.\" Sci Data\n        \n        Preprint\n     \n        \n        Github\n     \n        \n        Published\n    \nManjari Shankar, Anna-Maria Hartner, Callum R, ..., Jong-Hoon Kim, ..., and Katy Gaythorpe. (2024) \"Mathematical modelling to inform outbreak response vaccination.\" BMC Infect Dis\n        \n        Preprint\n     \n        \n        Published\n    \nSubmitted\n2024\nDaeHyup Koh, Monica Duong, Nodar Kipshidze, Virginia E. Pitzer, and Jong-Hoon Kim. (2024) \"Time series data on typhoid fever incidence during outbreaks from 2000 to 2022.\" Sci Data\n        \n        Preprint\n    \nThuy Tien Nguyen, Chaelin Kim, Gerard Goucher, and Jong-Hoon Kim. (2024) \"Associations of Water Quality with Cholera in Case-Control Studies: A Systematic Review and Meta-Analysis.\"\n        \n        Preprint\n     \n        \n        Github\n    \nWoo-Sik Son, Min-Kyung Chae, ..., Jong-Hoon Kim, and Jonggul Lee. (2024) \"Social contact patterns in South Korea: an analysis of a survey conducted in 2023-2024.\""
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "101-1804\nkimfinale@gmail.com\n\n\n174 Solsaem-ro, Gangbuk-gu\nwww.jonghoonk.com\n\n\nSeoul, Korea\n+82-10-9482-2517\n\n\n\n\n\n\n2021-2023 (expected)\n\nMicroMaster, Statistics and Data Science; MITx (online learning initiative of the Massachusetts Institute of Technology)\n\n2006-2010\n\nPhD, Theoretical Epidemiology (Disease Transmission Modeling); University of Michigan, Ann Arbor\nThesis title: Dynamic partnerships and HIV transmissions by stage\n\n2007-2010\n\nMSc, Biophysics and molecular biology; Gwangju Institute of Science and Technology\n\n2007-2010\n\nBSc, Biochemistry; Chungnam National University\n\n\n\n\n\n2008-9\nRackham Fellowship ($20,000) University of Michigan, Ann Arbor 2004 Student Award ($3,000)\nBlue Cross Blue Shield Michigan Foundation 2003-2004 Study Abroad Scholarship ($60,000) Korea Science and Engineering Foundation\n\n\n\nYour Most Recent Work Experience:\nShort text containing the type of work done, results obtained, lessons learned and other remarks. Can also include lists and links:\n\nFirst item\nItem with link. Links will work both in the html and pdf versions.\n\nThat Other Job You Had\nAlso with a short description.\n\n\n\nResearch Scientist at the International Vaccine Institute (2018-present)\n\n\n\nI am a theoretical epidemiologist specializing in the modeling of infectious disease transmission. My research involves utilizing mathematical and statistical models as well as data science techniques to examine the dynamics of disease transmission and evaluate the effectiveness of intervention programs like vaccination. Over the course of my career, I have investigated a wide range of pathogens including HIV, poliovirus, cholera, typhoid fever, non-typhoidal Salmonella disease, and COVID-19. To continue expanding my expertise in the field, I actively pursue new knowledge in machine learning and data science. Additionally, I have shared my knowledge through numerous lectures and participated in advisory panels aimed at controlling the spread of COVID-19 in Korea.\n\n\n\n\nMy Cool Side Project\n\nFor items which don’t have a clear time ordering, a definition list can be used to have named items.\n\nThese items can also contain lists, but you need to mind the indentation levels in the markdown source.\nSecond item.\n\n\nOpen Source\n\nList open source contributions here, perhaps placing emphasis on the project names, for example the Linux Kernel, where you implemented multithreading over a long weekend, or node.js (with link) which was actually totally your idea…\n\nProgramming Languages\n\nfirst-lang: Here, we have an itemization, where we only want to add descriptions to the first few items, but still want to mention some others together at the end. A format that works well here is a description list where the first few items have their first word emphasized, and the last item contains the final few emphasized terms. Notice the reasonably nice page break in the pdf version, which wouldn’t happen if we generated the pdf via html.\n\n\nsecond-lang: Description of your experience with second-lang, perhaps again including a [link] ref, this time placing the url reference elsewhere in the document to reduce clutter (see source file).\n\n\nobscure-but-impressive-lang: We both know this one’s pushing it.\n\n\nBasic knowledge of C, x86 assembly, forth, Common Lisp\n\n\n\n\n\n\nHuman Languages:\n\nEnglish (native speaker)\n???\nThis is what a nested list looks like.\n\nRandom tidbit\nOther sort of impressive-sounding thing you did"
  },
  {
    "objectID": "cv/index.html#education",
    "href": "cv/index.html#education",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "2021-2023 (expected)\n\nMicroMaster, Statistics and Data Science; MITx (online learning initiative of the Massachusetts Institute of Technology)\n\n2006-2010\n\nPhD, Theoretical Epidemiology (Disease Transmission Modeling); University of Michigan, Ann Arbor\nThesis title: Dynamic partnerships and HIV transmissions by stage\n\n2007-2010\n\nMSc, Biophysics and molecular biology; Gwangju Institute of Science and Technology\n\n2007-2010\n\nBSc, Biochemistry; Chungnam National University"
  },
  {
    "objectID": "cv/index.html#honors-and-awards",
    "href": "cv/index.html#honors-and-awards",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "2008-9\nRackham Fellowship ($20,000) University of Michigan, Ann Arbor 2004 Student Award ($3,000)\nBlue Cross Blue Shield Michigan Foundation 2003-2004 Study Abroad Scholarship ($60,000) Korea Science and Engineering Foundation"
  },
  {
    "objectID": "cv/index.html#experience",
    "href": "cv/index.html#experience",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "Your Most Recent Work Experience:\nShort text containing the type of work done, results obtained, lessons learned and other remarks. Can also include lists and links:\n\nFirst item\nItem with link. Links will work both in the html and pdf versions.\n\nThat Other Job You Had\nAlso with a short description."
  },
  {
    "objectID": "cv/index.html#current-position",
    "href": "cv/index.html#current-position",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "Research Scientist at the International Vaccine Institute (2018-present)"
  },
  {
    "objectID": "cv/index.html#research-interests",
    "href": "cv/index.html#research-interests",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "I am a theoretical epidemiologist specializing in the modeling of infectious disease transmission. My research involves utilizing mathematical and statistical models as well as data science techniques to examine the dynamics of disease transmission and evaluate the effectiveness of intervention programs like vaccination. Over the course of my career, I have investigated a wide range of pathogens including HIV, poliovirus, cholera, typhoid fever, non-typhoidal Salmonella disease, and COVID-19. To continue expanding my expertise in the field, I actively pursue new knowledge in machine learning and data science. Additionally, I have shared my knowledge through numerous lectures and participated in advisory panels aimed at controlling the spread of COVID-19 in Korea."
  },
  {
    "objectID": "cv/index.html#technical-experience",
    "href": "cv/index.html#technical-experience",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "My Cool Side Project\n\nFor items which don’t have a clear time ordering, a definition list can be used to have named items.\n\nThese items can also contain lists, but you need to mind the indentation levels in the markdown source.\nSecond item.\n\n\nOpen Source\n\nList open source contributions here, perhaps placing emphasis on the project names, for example the Linux Kernel, where you implemented multithreading over a long weekend, or node.js (with link) which was actually totally your idea…\n\nProgramming Languages\n\nfirst-lang: Here, we have an itemization, where we only want to add descriptions to the first few items, but still want to mention some others together at the end. A format that works well here is a description list where the first few items have their first word emphasized, and the last item contains the final few emphasized terms. Notice the reasonably nice page break in the pdf version, which wouldn’t happen if we generated the pdf via html.\n\n\nsecond-lang: Description of your experience with second-lang, perhaps again including a [link] ref, this time placing the url reference elsewhere in the document to reduce clutter (see source file).\n\n\nobscure-but-impressive-lang: We both know this one’s pushing it.\n\n\nBasic knowledge of C, x86 assembly, forth, Common Lisp"
  },
  {
    "objectID": "cv/index.html#extra-section-call-it-whatever-you-want",
    "href": "cv/index.html#extra-section-call-it-whatever-you-want",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "Human Languages:\n\nEnglish (native speaker)\n???\nThis is what a nested list looks like.\n\nRandom tidbit\nOther sort of impressive-sounding thing you did"
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html",
    "href": "blog/posts/bayesian_workflow/index.html",
    "title": "Bayesian workflow: fake social contact data example",
    "section": "",
    "text": "This post is my attempt to follow along a Bayesian Workflow by Gelman et al and create examples for better understanding. While my ultimate goal is to study infectious disease transmission using a dynamic model following a Bayesian workflow, I use a simpler statistical model for practice in this post. I also consulted a blog post by Michael Betancourt to create this post.\nRegarding the definition of Bayesian workflow, Gelman et al. writes:\n\n“… Bayesian inference is just the formulation and computation of conditional probability or probability densities, \\(p(\\theta|y) \\propto p(\\theta) p(y|\\theta)\\). Bayesian workflow includes the three steps of model building, inference, and model checking/improvement, along with the comparison of different models, not just for the purpose of model choice or model averaging but more importantly to better understand these models …”"
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#bayesian-workflow",
    "href": "blog/posts/bayesian_workflow/index.html#bayesian-workflow",
    "title": "Bayesian workflow: fake social contact data example",
    "section": "",
    "text": "This post is my attempt to follow along a Bayesian Workflow by Gelman et al and create examples for better understanding. While my ultimate goal is to study infectious disease transmission using a dynamic model following a Bayesian workflow, I use a simpler statistical model for practice in this post. I also consulted a blog post by Michael Betancourt to create this post.\nRegarding the definition of Bayesian workflow, Gelman et al. writes:\n\n“… Bayesian inference is just the formulation and computation of conditional probability or probability densities, \\(p(\\theta|y) \\propto p(\\theta) p(y|\\theta)\\). Bayesian workflow includes the three steps of model building, inference, and model checking/improvement, along with the comparison of different models, not just for the purpose of model choice or model averaging but more importantly to better understand these models …”"
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#conceptual-analysis",
    "href": "blog/posts/bayesian_workflow/index.html#conceptual-analysis",
    "title": "Bayesian workflow: fake social contact data example",
    "section": "Conceptual analysis",
    "text": "Conceptual analysis\nThe study is examining the variations of the number of contacts in the Korean population, specifically, variations of contact frequencies by age and household size. A prior study and other numerous studies show that contacts differ by age. A significant fraction of contacts occur with household members and as such the household size is also important.\nAccording to the study by Shmueli, there are three conceptual modelling approaches: descriptive, predictive and explanatory. This study is best described as a descriptive study. The descriptive study does not directly aim at obtaining optimal predictive performance, but at capturing the data structure parsimoniously. For a descriptive model, interpretability, transportability and general usability are important criteria as described in the article.\nThe integer number of people that each person met during one day. We start with all the variables. Participants are classified by age and household size. The number of contacts are best captured with negative binomial distribution and mean umber of contacts is a function of age and household size.\n\\[\ny_i \\sim \\text{NB}(y_i|\\mu_i,\\phi)=\\frac{\\Gamma (y_i + \\theta)}{\\Gamma (\\theta) y_i !}) \\left(\\frac{\\mu_i}{\\mu_i+\\phi} \\right)^y \\left(\\frac{\\phi}{\\mu_i+\\phi} \\right)^\\phi\n\\tag{1}\\]\n\\[\\text{log}(\\mu_i)= \\alpha + \\beta_{\\text{age}_i} +\\beta_{\\text{hhsize}_i} \\tag{2}\\]\nWe take the fake-data simulation approach, which can help us understand our data model and priors, what can be learned from an experiment, and the validity of the applied inference methods."
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#generate-the-fake-data",
    "href": "blog/posts/bayesian_workflow/index.html#generate-the-fake-data",
    "title": "Bayesian workflow: fake social contact data example",
    "section": "Generate the fake data",
    "text": "Generate the fake data\nParameters of interest are the ratios of contacts per day (i.e., rate ratios or relative risks) by age and household size. Plugging Equation 2 into Equation 1 becomes our generative model.\n\n# parameter values to be estimated\nintercept_true &lt;- 3 # base number of contacts\nrr_age_true &lt;- c(1.5,0.6) # relative number of contacts by age (age0 = 1)\nrr_hhsize_true &lt;- c(2,2.6) # relative number of contacts by household size  (hhsize0 =1)  \nsize_true &lt;- 5 # shape parameter for the negative binomial distribution\n\nset.seed(42)\n# 9 categories with &gt;100 observations on average\nd &lt;- data.frame(age = sample(1:3, 1000, replace=T),\n                hhsize = sample(1:3, 1000, replace=T))\n\nd$b_age &lt;- ifelse(d$age == 1, 1, ifelse(d$age == 2, rr_age_true[1], rr_age_true[2])) # relative risk by age\nd$b_hhsize &lt;- ifelse(d$hhsize == 1, 1, ifelse(d$hhsize == 2, rr_hhsize_true[1], rr_hhsize_true[2])) # relative risk by household size\n\n# mean as a function of age and hhsize\na &lt;- intercept_true \nlog_mu &lt;- log(a) + log(d$b_age) + log(d$b_hhsize)\nd$mu &lt;- exp(log_mu)\nd$age &lt;- as.factor(d$age)\nd$hhsize &lt;- as.factor(d$hhsize)\nd$contacts &lt;- rnbinom(nrow(d), mu=d$mu, size=size_true) # size, aka, shape, dispersion\n\nHistogram, mean, and variance of the data\n\nlibrary(tidyverse)\nd |&gt; \n  ggplot(aes(x=contacts))+\n  geom_histogram(binwidth=1)\n\n\n\nmean(d$contacts)\n\n[1] 6.148\n\nvar(d$contacts)\n\n[1] 25.07918"
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#bayesian-model-fitting-using-the-brms-package.",
    "href": "blog/posts/bayesian_workflow/index.html#bayesian-model-fitting-using-the-brms-package.",
    "title": "Bayesian workflow: fake social contact data example",
    "section": "Bayesian model fitting using the brms package.",
    "text": "Bayesian model fitting using the brms package.\n\nPrior predictive model using the sample_prior = \"only\" argument\n\nlibrary(brms)\n\n\n# prior distribution\na_mean &lt;- 0\na_scale &lt;- 1.5 # chosen with an assumption that exp(2*a_scale) will likely cover plausible values \n\nb_mean &lt;- 0\nb_scale &lt;- 1.5\n\nshp_alpha &lt;- 0.8 # 0.4 (brms default) causes divergent transitions,\nshp_beta &lt;- 0.3\n\n\nmy_priors &lt;- \n  c(prior_string(paste0(\"normal(\", a_mean, \",\", a_scale,\")\"), \n                 class = \"Intercept\"),\n    prior_string(paste0(\"normal(\", b_mean, \",\", b_scale,\")\"),\n                 class = \"b\"),\n    prior_string(paste0(\"inv_gamma(\", shp_alpha, \",\", shp_beta,\")\"), \n                 class=\"shape\"))\n\nprior_pred &lt;- brm(contacts ~ 1 + age + hhsize,\n                 data = d,\n                 family = negbinomial(),\n                 prior = my_priors,\n                 iter = 1e4, sample_prior = \"only\", cores = 4)\n\nsaveRDS(prior_pred, \"prior_pred.rds\")\n\n\n\nPrior predictive checks\n\nprior_pred &lt;- readRDS(\"prior_pred.rds\")\npriorpc &lt;- pp_check(prior_pred, type=\"bars\", ndraws = 200)\n\npriorpc$data |&gt; \n  ggplot(aes(x=x))+\n  geom_col(aes(y=y_obs, fill=\"Data\"))+\n  geom_point(aes(y=m, color=\"Prior\"))+\n  geom_linerange(aes(ymax=h, ymin=l, color=\"Prior\"))+\n  scale_x_continuous(limits=c(-1,30))+\n  labs(x=\"number of contacts\", y= \"frequency\")+\n  scale_color_manual(\"\", values=c(\"Prior\"=\"forestgreen\"))+\n  scale_fill_manual(\"\", values=c(\"Data\"=\"steelblue\"))+\n  theme_bw()+\n  theme(legend.position=c(0.8,0.8))\n\n\n\n\n\n\nFit the model\nGelman et al. writes: ” … The first step in validating computation is to check that the model actually finishes the fitting process in an acceptable time frame and the convergence diagnostics are reasonable. In the context of HMC, this is primarily the absence of divergent transitions, Rb diagnostic near 1, and sufficient effective sample sizes for the central tendency, the tail quantiles, and the energy (Vehtari et al., 2020). …”\n\nfit &lt;- brm(contacts ~ 1 + age + hhsize, \n           data = d, \n           family = negbinomial(),\n           prior = my_priors,\n           iter = 1e4, cores = 4)\nsaveRDS(fit, \"fit.rds\")\n\nbrms package reports all the relevant information.\n\nfit &lt;- readRDS(\"fit.rds\")\nsummary(fit)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: contacts ~ 1 + age + hhsize \n   Data: d (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.10      0.05     1.01     1.20 1.00    18087    13497\nage2          0.36      0.05     0.27     0.45 1.00    22199    15458\nage3         -0.53      0.05    -0.63    -0.42 1.00    22085    15938\nhhsize2       0.74      0.05     0.63     0.84 1.00    20153    15715\nhhsize3       1.03      0.05     0.93     1.13 1.00    19397    15626\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     5.19      0.46     4.37     6.16 1.00    26405    14803\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nPlot posterior predictive values along with prior predictive values\n\nprior_pred &lt;- readRDS(\"prior_pred.rds\")\npriorpc &lt;- pp_check(prior_pred, type=\"bars\", ndraws = 200)\npostpc &lt;- pp_check(fit, type=\"bars\", ndraws = 200)\n\npostpc$data |&gt; \n  ggplot(aes(x=x))+\n  geom_col(aes(y=y_obs, fill=\"Data\"))+\n  geom_point(data=priorpc$data, aes(y=m, color=\"Prior\"))+\n  geom_linerange(data=priorpc$data,aes(ymax=h, ymin=l, color=\"Prior\"))+\n  geom_point(aes(y=m, color=\"Posterior\"))+\n  geom_linerange(aes(ymax=h, ymin=l, color=\"Posterior\"))+\n  scale_x_continuous(limits=c(-1,30))+\n  labs(x=\"number of contacts\", y= \"frequency\")+\n  scale_color_manual(\"\", values=c(\"Prior\"=\"forestgreen\",\n                               \"Posterior\"=\"firebrick\"))+\n  scale_fill_manual(\"\", values=c(\"Data\"=\"steelblue\"))+\n  theme_bw()+\n  theme(legend.position=c(0.8,0.8))\n\n\n\n\n\n\nPrior and posterior distributions of parameters\nTrace plots and histograms\n\nplot(fit, pars = \"^b_\")\n\n\n\n# plot(fit, pars = \".*age*.\")\n# plot(fit, pars = \".*hhsize*.\")\n\nCorrelations\n\npairs(fit, pars = \".*age*.\")\n\n\n\n\n\\(\\beta_{\\text{age}}\\) parameter\n\ndf_post &lt;- as.data.frame(fit)\nx &lt;- seq(-2, 2, 0.01)\ndf &lt;- data.frame(x=seq(-2, 2, 0.01))\ndf$prior_density &lt;-  dnorm(df$x, b_mean, b_scale)\n\nplot(exp(x), dnorm(x, b_mean, b_scale), type=\"l\", ylim=c(0, 20), \n     xlab=\"value\", ylab=\"density\", main=expression(beta[age]))\nlines(density(exp(df_post$b_age2)), col=2)\nlines(density(exp(df_post$b_age3)), col=3)\nabline(v=rr_age_true, col=\"purple\", lwd=2)\n\nlegend(\"topright\",\n       bty = \"n\",\n       lty=1,\n       col=c(1,2,3,\"purple\"),\n       legend=c(\"Prior\", \"age2\", \"age3\",\"True\"),\n       inset=0.02)\n\n\n\n# ggplot2 way\n# ggplot()+\n#   geom_line(data=df, aes(x=exp(x), y=prior_density, color=\"Prior\"))+\n#   geom_density(data=df_post, aes(x=exp(b_age2), color=\"Posterior b_age2\"))+\n#   geom_density(data=df_post, aes(x=exp(b_age3), color=\"Posterior b_age3\"))+\n#   scale_color_manual(values=c(\"Prior\"=\"forestgreen\", \n#                               \"Posterior b_age2\"=\"firebrick\",\n#                               \"Posterior b_age3\"=\"steelblue\"))+\n#   labs(color=\"\", x=\"value\", y=\"density\")+\n#   theme(legend.position=\"bottom\")\n\n\\(\\beta_{\\text{hhsize}}\\) parameter\n\nplot(exp(x), dnorm(x, b_mean, b_scale), type=\"l\", ylim=c(0,5), \n     xlab=\"value\", ylab=\"density\", main=expression(beta[hhsize]))\nlines(density(exp(df_post$b_hhsize2)), col=2)\nlines(density(exp(df_post$b_hhsize3)), col=3)\nabline(v=rr_hhsize_true, col=\"purple\", lwd=2)\n\nlegend(\"topright\",\n       bty = \"n\",\n       lty=1,\n       col=c(1,2,3,\"purple\"),\n       legend=c(\"Prior\", \"hhsize2\", \"hhsize3\", \"True\"),\n       inset=0.02)\n\n\n\n\nIntercept\n\nx &lt;- seq(-3, 3,0.01)\nplot(exp(x), dnorm(x, a_mean, a_scale), type=\"l\", ylim=c(0,4), \n     xlab=\"value\", ylab=\"density\", main=\"Intercept\")\nlines(density(exp(df_post$b_Intercept)), col=2)\nabline(v=intercept_true, col=\"purple\", lwd=2)\nlegend(\"topright\",\n       bty = \"n\",\n       lty=1,\n       col=c(1,2,\"purple\"),\n       legend=c(\"Prior\", \"Posterior\", \"True\"),\n       inset=0.02)\n\n\n\n\nShape parameter\n\nlibrary(extraDistr)\n\nx &lt;- seq(0,10,0.01)\nplot(x, dinvgamma(x, shp_alpha, shp_beta), type=\"l\", \n     ylim=c(0,2), xlab=\"value\", ylab=\"density\", main=\"Shape\")\nlines(density(df_post$shape), col=2)\nabline(v=size_true, col=\"purple\", lwd=2)\nlegend(\"topright\",\n       bty = \"n\",\n       lty=1,\n       col=c(1,2,\"purple\"),\n       legend=c(\"Prior\", \"Posterior\", \"True\"),\n       inset=0.02)"
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#plot-prior-and-posterior-distributions",
    "href": "blog/posts/bayesian_workflow/index.html#plot-prior-and-posterior-distributions",
    "title": "Bayesian workflow: fake social contact data example",
    "section": "Plot prior and posterior distributions",
    "text": "Plot prior and posterior distributions\nTrace plots and historgrams\n\nplot(fit, pars = \"^b_\")\n\n\n\n# plot(fit, pars = \".*age*.\")\n# plot(fit, pars = \".*hhsize*.\")\n\nCorrelations\n\npairs(fit, pars = \".*age*.\")\n\n\n\n\n\\(\\beta_{\\text{age}}\\) parameter\n\ndf_post &lt;- as.data.frame(fit)\nx &lt;- seq(-2, 2, 0.01)\ndf &lt;- data.frame(x=seq(-2, 2, 0.01))\ndf$prior_density &lt;-  dnorm(df$x, b_mean, b_scale)\n\nplot(exp(x), dnorm(x, b_mean, b_scale), type=\"l\", ylim=c(0, 20), \n     xlab=\"value\", ylab=\"density\", main=expression(beta[age]))\nlines(density(exp(df_post$b_age2)), col=2)\nlines(density(exp(df_post$b_age3)), col=3)\nabline(v=rr_age_true, col=\"purple\", lwd=2)\n\nlegend(\"topright\",\n       bty = \"n\",\n       lty=1,\n       col=c(1,2,3,\"purple\"),\n       legend=c(\"Prior\", \"age2\", \"age3\",\"True\"),\n       inset=0.02)\n\n\n\n# ggplot2 way\n# ggplot()+\n#   geom_line(data=df, aes(x=exp(x), y=prior_density, color=\"Prior\"))+\n#   geom_density(data=df_post, aes(x=exp(b_age2), color=\"Posterior b_age2\"))+\n#   geom_density(data=df_post, aes(x=exp(b_age3), color=\"Posterior b_age3\"))+\n#   scale_color_manual(values=c(\"Prior\"=\"forestgreen\", \n#                               \"Posterior b_age2\"=\"firebrick\",\n#                               \"Posterior b_age3\"=\"steelblue\"))+\n#   labs(color=\"\", x=\"value\", y=\"density\")+\n#   theme(legend.position=\"bottom\")\n\n\\(\\beta_{\\text{hhsize}}\\) parameter\n\nplot(exp(x), dnorm(x, b_mean, b_scale), type=\"l\", ylim=c(0,5), \n     xlab=\"value\", ylab=\"density\", main=expression(beta[hhsize]))\nlines(density(exp(df_post$b_hhsize2)), col=2)\nlines(density(exp(df_post$b_hhsize3)), col=3)\nabline(v=rr_hhsize_true, col=\"purple\", lwd=2)\n\nlegend(\"topright\",\n       bty = \"n\",\n       lty=1,\n       col=c(1,2,3,\"purple\"),\n       legend=c(\"Prior\", \"hhsize2\", \"hhsize3\", \"True\"),\n       inset=0.02)\n\n\n\n\nIntercept\n\nx &lt;- seq(-3, 3,0.01)\nplot(exp(x), dnorm(x, a_mean, a_scale), type=\"l\", ylim=c(0,4), \n     xlab=\"value\", ylab=\"density\", main=\"Intercept\")\nlines(density(exp(df_post$b_Intercept)), col=2)\nabline(v=intercept_true, col=\"purple\", lwd=2)\nlegend(\"topright\",\n       bty = \"n\",\n       lty=1,\n       col=c(1,2,\"purple\"),\n       legend=c(\"Prior\", \"Posterior\", \"True\"),\n       inset=0.02)\n\n\n\n\nShape parameter\n\nlibrary(extraDistr)\n\nx &lt;- seq(0,10,0.01)\nplot(x, dinvgamma(x, shp_alpha, shp_beta), type=\"l\", \n     ylim=c(0,2), xlab=\"value\", ylab=\"density\", main=\"Shape\")\nlines(density(df_post$shape), col=2)\nabline(v=size_true, col=\"purple\", lwd=2)\nlegend(\"topright\",\n       bty = \"n\",\n       lty=1,\n       col=c(1,2,\"purple\"),\n       legend=c(\"Prior\", \"Posterior\", \"True\"),\n       inset=0.02)"
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#building-the-model",
    "href": "blog/posts/bayesian_workflow/index.html#building-the-model",
    "title": "Bayesian workflow: fake social contact data example [in progress]",
    "section": "Building the model",
    "text": "Building the model\n\nBayesian modeling using the brms package.\nWe first try the model that includes age and household size\n\nlibrary(brms)\n\n\n\nPrior predictive model using the sample_prior = \"only\" argument\n\n# prior distribution\na_mean &lt;- 0\na_scale &lt;- 1.5 # chosen with an assumption that exp(2*a_scale) will likely cover plausible values \nb_mean &lt;- 0\nb_scale &lt;- 1.5\n\nshp_alpha &lt;- 0.8 # 0.4 (brms default) causes divergent transitions,\nshp_beta &lt;- 0.3\n\n\nmy_priors &lt;- \n  c(prior_string(paste0(\"normal(\", a_mean, \",\", a_scale,\")\"), \n                 class = \"Intercept\"),\n    prior_string(paste0(\"normal(\", b_mean, \",\", b_scale,\")\"),\n                 class = \"b\"),\n    prior_string(paste0(\"inv_gamma(\", shp_alpha, \",\", shp_beta,\")\"), \n                 class=\"shape\"))\n\nprior_pred &lt;- brm(contacts ~ 1 + age + hhsize,\n                 data = d,\n                 family = negbinomial(),\n                 prior = my_priors,\n                 iter = 1e4, sample_prior = \"only\", cores = 4)\n\nsaveRDS(prior_pred, \"prior_pred.rds\")\n\n\n\nPrior predictive checks\n\nprior_pred &lt;- readRDS(\"prior_pred.rds\")\npriorpc &lt;- pp_check(prior_pred, type=\"bars\", ndraws = 200)\n\npriorpc$data |&gt; \n  ggplot(aes(x=x))+\n  geom_col(aes(y=y_obs, fill=\"Data\"))+\n  geom_point(aes(y=m, color=\"Prior\"))+\n  geom_linerange(aes(ymax=h, ymin=l, color=\"Prior\"))+\n  scale_x_continuous(limits=c(-1,30))+\n  labs(x=\"number of contacts\", y= \"frequency\")+\n  scale_color_manual(\"\", values=c(\"Prior\"=\"darkgreen\"))+\n  scale_fill_manual(\"\", values=c(\"Data\"=\"steelblue\"))+\n  theme_bw()+\n  theme(legend.position=c(0.8,0.8))"
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#fit-the-model",
    "href": "blog/posts/bayesian_workflow/index.html#fit-the-model",
    "title": "Bayesian workflow: fake social contact data example [in progress]",
    "section": "Fit the model",
    "text": "Fit the model\nGelman et al. writes: ” … The first step in validating computation is to check that the model actually finishes the fitting process in an acceptable time frame and the convergence diagnostics are reasonable. In the context of HMC, this is primarily the absence of divergent transitions, Rb diagnostic near 1, and sufficient effective sample sizes for the central tendency, the tail quantiles, and the energy (Vehtari et al., 2020). …”\n\nfit1 &lt;- brm(contacts ~ 1 + age + hhsize, \n           data = d, \n           family = negbinomial(),\n           prior = my_priors,\n           iter = 1e4, cores = 4)\nsaveRDS(fit1, \"fit1.rds\")\n\nbrms package reports all the relevant information.\n\nfit1 &lt;- readRDS(\"fit1.rds\")\nsummary(fit1)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: contacts ~ 1 + age + hhsize \n   Data: d (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.13      0.05     1.04     1.22 1.00    17035    13697\nage2          0.40      0.04     0.32     0.49 1.00    20548    15508\nage3         -0.47      0.05    -0.57    -0.37 1.00    20229    15028\nhhsize2       0.78      0.05     0.68     0.88 1.00    17675    15906\nhhsize3       1.10      0.05     1.01     1.20 1.00    17685    16213\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     5.30      0.45     4.48     6.24 1.00    22342    14546\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nPlot posterior predictive values along with prior predictive values\n\nprior_pred &lt;- readRDS(\"prior_pred.rds\")\npriorpc &lt;- pp_check(prior_pred, type=\"bars\", ndraws = 200)\npostpc &lt;- pp_check(fit1, type=\"bars\", ndraws = 200)\n\npostpc$data |&gt; \n  ggplot(aes(x=x))+\n  geom_col(aes(y=y_obs, fill=\"Data\"))+\n  geom_point(data=priorpc$data, aes(y=m, color=\"Prior\"))+\n  geom_linerange(data=priorpc$data,aes(ymax=h, ymin=l, color=\"Prior\"))+\n  geom_point(aes(y=m, color=\"Posterior\"))+\n  geom_linerange(aes(ymax=h, ymin=l, color=\"Posterior\"))+\n  scale_x_continuous(limits=c(-1,30))+\n  labs(x=\"number of contacts\", y= \"frequency\")+\n  scale_color_manual(\"\", values=c(\"Prior\"=\"darkgreen\",\n                               \"Posterior\"=\"firebrick\"))+\n  scale_fill_manual(\"\", values=c(\"Data\"=\"steelblue\"))+\n  theme_bw()+\n  theme(legend.position=c(0.8,0.8))"
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#prior-and-posterior-distributions-of-parameters",
    "href": "blog/posts/bayesian_workflow/index.html#prior-and-posterior-distributions-of-parameters",
    "title": "Bayesian workflow: fake social contact data example",
    "section": "Prior and posterior distributions of parameters",
    "text": "Prior and posterior distributions of parameters\nTrace plots and histograms\n\nplot(fit1, pars = \"^b_\")\n\n\n\n# plot(fit, pars = \".*age*.\")\n# plot(fit, pars = \".*hhsize*.\")\n\nCorrelations\n\npairs(fit1, pars = \".*age*.\")\n\n\n\n\n\\(\\beta_{\\text{age}}\\) parameter\n\ndf_post &lt;- as.data.frame(fit1)\nx &lt;- seq(-2, 2, 0.01)\ndf &lt;- data.frame(x=seq(-2, 2, 0.01))\ndf$prior_density &lt;-  dnorm(df$x, b_mean, b_scale)\n\nplot(exp(x), dnorm(x, b_mean, b_scale), type=\"l\", ylim=c(0, 20), \n     xlab=\"value\", ylab=\"density\", main=expression(beta[age]))\nlines(density(exp(df_post$b_age2)), col=2)\nlines(density(exp(df_post$b_age3)), col=3)\nabline(v=rr_age_true, col=\"purple\", lwd=2)\n\nlegend(\"topright\",\n       bty = \"n\",\n       lty=1,\n       col=c(1,2,3,\"purple\"),\n       legend=c(\"Prior\", \"age2\", \"age3\",\"True\"),\n       inset=0.02)\n\n\n\n# ggplot2 way\n# ggplot()+\n#   geom_line(data=df, aes(x=exp(x), y=prior_density, color=\"Prior\"))+\n#   geom_density(data=df_post, aes(x=exp(b_age2), color=\"Posterior b_age2\"))+\n#   geom_density(data=df_post, aes(x=exp(b_age3), color=\"Posterior b_age3\"))+\n#   scale_color_manual(values=c(\"Prior\"=\"forestgreen\", \n#                               \"Posterior b_age2\"=\"firebrick\",\n#                               \"Posterior b_age3\"=\"steelblue\"))+\n#   labs(color=\"\", x=\"value\", y=\"density\")+\n#   theme(legend.position=\"bottom\")\n\n\\(\\beta_{\\text{hhsize}}\\) parameter\n\nplot(exp(x), dnorm(x, b_mean, b_scale), type=\"l\", ylim=c(0,5), \n     xlab=\"value\", ylab=\"density\", main=expression(beta[hhsize]))\nlines(density(exp(df_post$b_hhsize2)), col=2)\nlines(density(exp(df_post$b_hhsize3)), col=3)\nabline(v=rr_hhsize_true, col=\"purple\", lwd=2)\n\nlegend(\"topright\",\n       bty = \"n\",\n       lty=1,\n       col=c(1,2,3,\"purple\"),\n       legend=c(\"Prior\", \"hhsize2\", \"hhsize3\", \"True\"),\n       inset=0.02)\n\n\n\n\nIntercept\n\nx &lt;- seq(-3, 3,0.01)\nplot(exp(x), dnorm(x, a_mean, a_scale), type=\"l\", ylim=c(0,4), \n     xlab=\"value\", ylab=\"density\", main=\"Intercept\")\nlines(density(exp(df_post$b_Intercept)), col=2)\nabline(v=intercept_true, col=\"purple\", lwd=2)\nlegend(\"topright\",\n       bty = \"n\",\n       lty=1,\n       col=c(1,2,\"purple\"),\n       legend=c(\"Prior\", \"Posterior\", \"True\"),\n       inset=0.02)\n\n\n\n\nShape parameter\n\nlibrary(extraDistr)\n\nx &lt;- seq(0,10,0.01)\nplot(x, dinvgamma(x, shp_alpha, shp_beta), type=\"l\", \n     ylim=c(0,2), xlab=\"value\", ylab=\"density\", main=\"Shape\")\nlines(density(df_post$shape), col=2)\nabline(v=shape_true, col=\"purple\", lwd=2)\nlegend(\"topright\",\n       bty = \"n\",\n       lty=1,\n       col=c(1,2,\"purple\"),\n       legend=c(\"Prior\", \"Posterior\", \"True\"),\n       inset=0.02)"
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#model-evaluation",
    "href": "blog/posts/bayesian_workflow/index.html#model-evaluation",
    "title": "Bayesian workflow: fake social contact data example [in progress]",
    "section": "Model evaluation",
    "text": "Model evaluation\nLeave-one-out (LOO) cross validation as suggested in Vehtari et al. and Gelman et al..\nWe fit two additional models: fit2 and fit0. fit2 includes sex variable in addition to all variables included in fit1 and could be better or worse than fit1 as impact of sex is not big in the data set. fit0 is intercept-only model and thus is very likely to be worse than fit1 or fit2.\n\nfit2 &lt;- brm(contacts ~ 1 + age + hhsize + sex, \n           data = d, \n           family = negbinomial(),\n           prior = my_priors,\n           iter = 1e4, cores = 4)\nsaveRDS(fit2, \"fit2.rds\")\n\nfit0 &lt;- brm(contacts ~ 1, \n           data = d, \n           family = negbinomial(),\n           prior = set_prior(\"normal(0,5)\", class=\"Intercept\"),\n           iter = 1e4, cores = 4)\nsaveRDS(fit0, \"fit0.rds\")\n\nSummary of fit2 and fit2\n\nfit2 &lt;- readRDS(\"fit2.rds\") \nfit0 &lt;- readRDS(\"fit0.rds\")\nsummary(fit2)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: contacts ~ 1 + age + hhsize + sex \n   Data: d (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.12      0.05     1.02     1.22 1.00    21089    14681\nage2          0.40      0.04     0.32     0.49 1.00    23623    15288\nage3         -0.47      0.05    -0.57    -0.37 1.00    23780    16196\nhhsize2       0.78      0.05     0.68     0.88 1.00    21501    16727\nhhsize3       1.10      0.05     1.01     1.20 1.00    20906    17372\nsex2          0.01      0.04    -0.06     0.08 1.00    30179    14675\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     5.28      0.45     4.49     6.22 1.00    30118    15669\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nsummary(fit0)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: contacts ~ 1 \n   Data: d (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.92      0.03     1.87     1.97 1.00    15994    12293\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     2.02      0.12     1.79     2.26 1.00    15828    12635\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nShape parameter estimates\n\nprobs &lt;- c(0.025, 0.25, 0.5, 0.75, 0.975)\nshp1 &lt;- quantile(as.matrix(fit1)[,\"shape\"], probs=probs)\nshp2 &lt;- quantile(as.matrix(fit2)[,\"shape\"], probs=probs)\nshp0 &lt;- quantile(as.matrix(fit0)[,\"shape\"], probs=probs)\n\ndf &lt;- data.frame(Model = \"Model0\", t(shp0))\ndf &lt;- rbind(df, data.frame(Model = \"Model1\", t(shp1)),\n            data.frame(Model = \"Model2\", t(shp2)))\n# df$percentile &lt;- rep(as.character(100*probs),3)\n\nggplot(df, aes(x=Model))+\n  geom_linerange(aes(ymin=`X2.5.`, ymax=`X97.5.`))+\n  geom_linerange(aes(ymin=`X25.`, ymax=`X75.`), linewidth=1)+\n  geom_point(aes(x=Model, y=`X50.`))+\n  ggtitle(\"\")+\n  theme_light() + \n  labs(y=\"inference for the shape parameter\", x=\"\")\n\n\n\n\nCompare fit0, fit1 and fit2 suing leave-one out (LOO) cross-validation\n\nlibrary(loo)\nloo_fit0 &lt;- loo(fit0)\nloo_fit1 &lt;- loo(fit1)\nloo_fit2 &lt;- loo(fit2)\nsaveRDS(loo_fit0, \"loo_fit0.rds\")\nsaveRDS(loo_fit1, \"loo_fit1.rds\")\nsaveRDS(loo_fit2, \"loo_fit2.rds\")\n\n\nloo_fit0 &lt;- readRDS(\"loo_fit0.rds\")\nloo_fit1 &lt;- readRDS(\"loo_fit1.rds\")\nloo_fit2 &lt;- readRDS(\"loo_fit2.rds\")\nprint(loo_fit0)\n\n\nComputed from 20000 by 1000 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo  -2924.2 26.4\np_loo         2.1  0.2\nlooic      5848.4 52.9\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\nprint(loo_fit1)\n\n\nComputed from 20000 by 1000 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo  -2618.3 26.2\np_loo         5.9  0.3\nlooic      5236.6 52.5\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\nprint(loo_fit2)\n\n\nComputed from 20000 by 1000 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo  -2619.3 26.2\np_loo         6.9  0.4\nlooic      5238.5 52.4\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\nbrms::loo_compare(loo_fit1, loo_fit2, loo_fit0)\n\n     elpd_diff se_diff\nfit1    0.0       0.0 \nfit2   -1.0       0.3 \nfit0 -305.9      19.4 \n\n\n\nfit0 &lt;- add_criterion(fit0, \"waic\")\nfit1 &lt;- add_criterion(fit1, \"waic\")\nfit2 &lt;- add_criterion(fit2, \"waic\")\n\nbrms::loo_compare(fit0, fit1, fit2, criterion = \"waic\")\n\n     elpd_diff se_diff\nfit1    0.0       0.0 \nfit2   -1.0       0.3 \nfit0 -305.9      19.4 \n\n\nIt appears that fit1 is the most favored by the looic metric."
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#model-building",
    "href": "blog/posts/bayesian_workflow/index.html#model-building",
    "title": "Bayesian workflow: fake social contact data example",
    "section": "Model building",
    "text": "Model building\n\nConceptual analysis\nLet’s suppose that we measured how the number of contact a person makes varies.A prior study and other numerous studies showed that contact frequencies differ by age. And a significant fraction of contacts occur with household members and as such the contact frequencies will likely vary by household size. We sampled a group of people and followed them for some time (e.g., 1 week) and measured the number of contacts that a typical person makes in a day. We also collected variables such as age, sex, and household size.\nThere are three conceptual modelling approaches: descriptive, predictive and explanatory approaches as per the study by Shmueli. This study is best described as a descriptive study and does not directly aim at obtaining optimal predictive performance, but at capturing the data structure parsimoniously. For a descriptive model, interpretability, transportability and general usability are important criteria as described in the article.\nAssuming that the number of contacts per day per person, \\(y_i\\), follows a negative binomial distribution, we can mathematically express our concepts in the following equations:\n\\[\ny_i \\sim \\text{NB}(y_i|\\mu_i,\\phi)=\\frac{\\Gamma (y_i + \\theta)}{\\Gamma (\\theta) y_i !}) \\left(\\frac{\\mu_i}{\\mu_i+\\phi} \\right)^y \\left(\\frac{\\phi}{\\mu_i+\\phi} \\right)^\\phi\n\\tag{1}\\]\n\\[\\text{log}(\\mu_i)= \\alpha + \\beta_{\\text{age}_i}  + \\beta_{\\text{sex}_i} +\\beta_{\\text{hhsize}_i} \\tag{2}\\]\n\\(\\mu_i\\) and \\(\\phi\\) represent the mean number of contacts per day for a person \\(i\\) and the shape parameter of the negative binomial distribution, respectively. \\(\\Gamma (\\cdot)\\) denotes a gamma function.\n\n\nImplemetation\nLoad packages\n\n# library(posterior)\n# options(pillar.neg = FALSE, pillar.subtle=FALSE, pillar.sigfig=2)\nlibrary(brms)\nlibrary(tidyverse)\n# library(bayesplot)\n# theme_set(bayesplot::theme_default(base_family = \"sans\"))\nset1 &lt;- RColorBrewer::brewer.pal(7, \"Set1\")\n\nGenerate the fake data\nWe take the fake-data simulation approach, which can help us understand our data model and priors, what can be learned from an experiment, and the validity of the applied inference methods. Parameters of interest are the ratios of number of contacts per day (i.e., rate ratios or relative risks) by age, sex, and household size. Plugging Equation 2 into Equation 1 becomes our generative model.\n\n# parameter values to be estimated\nintercept_true &lt;- 3 # base number of contacts\nrr_age_true &lt;- c(1.5, 0.6) # relative number of contacts by age (age0 = 1)\nrr_hhsize_true &lt;- c(2, 2.6) # relative number of contacts by household size  (hhsize0 =1)  \nrr_sex_true &lt;- c(1.2) # males have more frequent contacts, which is not modeled in model 1\nshape_true &lt;- 5 # shape parameter for the negative binomial distribution\n\nset.seed(1220)\n# 9 categories with &gt;100 observations on average\nd &lt;- data.frame(age = sample(1:3, 1000, replace=T),\n                hhsize = sample(1:3, 1000, replace=T), \n                sex = sample(1:2, 1000, replace=T))\nd$b_age &lt;- \n  ifelse(d$age == 1, 1, \n         ifelse(d$age == 2, rr_age_true[1], rr_age_true[2])) # relative risk by age\nd$b_hhsize &lt;- \n  ifelse(d$hhsize == 1, 1, \n         ifelse(d$hhsize == 2, rr_hhsize_true[1], rr_hhsize_true[2])) # relative risk by household size\nd$b_sex &lt;- ifelse(d$sex == 1, 1, rr_sex_true) # relative risk by sex\n\n# mean as a function of age and hhsize\na &lt;- intercept_true \nlog_mu &lt;- log(a) + log(d$b_age) + log(d$b_hhsize) + log(d$b_sex)\nd$mu &lt;- exp(log_mu)\nd$age &lt;- as.factor(d$age)\nd$hhsize &lt;- as.factor(d$hhsize)\nd$sex &lt;- as.factor(d$sex)\nd$contacts &lt;- rnbinom(nrow(d), mu=d$mu, size=shape_true) # size, aka, shape, dispersion\n\nHistogram, mean, and variance of the data\n\nlibrary(tidyverse)\nd |&gt; \n  ggplot(aes(x=contacts))+\n  geom_histogram(binwidth=1, fill=set1[2])+\n  theme_light()+\n  labs(x=\"Number of contacts per day per person\", y=\"Frequency\")\n\n\n\nmean(d$contacts)\n\n[1] 6.252\n\nvar(d$contacts)\n\n[1] 29.13964\n\n\nThe number of contacts is count data and the variance-to-mean ratio is around 5. Thefore, the choice of negative binomial distribution looks reasonable.\n\n\nModel 1: age and household size\nFor this model, we assume that the dependent variable is mostly influenced by age and household size and do not include sex variable in the model.\nWe use the brms package to implement the Bayesian regression model. We first set the priors for the intercept, a_*, the age and household size, b_*, and the shape parameter of the negative binomial distribution, shp_*.\n\n# prior distributions\n# intercept, modeled as normal\na_mean &lt;- 0\na_scale &lt;- 1.5 # exp(2*a_scale) will likely cover plausible values \n# beta for the age and the household, modeled as normal\nb_mean &lt;- 0\nb_scale &lt;- 1.5\n# shape parameter of the negbin distr, modeled as inverse gamma\nshp_alpha &lt;- 0.8 # 0.4 (brms default) causes divergent transitions,\nshp_beta &lt;- 0.3\n\nmy_priors &lt;- \n  c(prior_string(paste0(\"normal(\", a_mean, \",\", a_scale,\")\"), \n                 class = \"Intercept\"),\n    prior_string(paste0(\"normal(\", b_mean, \",\", b_scale,\")\"),\n                 class = \"b\"),\n    prior_string(paste0(\"inv_gamma(\", shp_alpha, \",\", shp_beta,\")\"), \n                 class=\"shape\"))\n\n\n\nPrior predictive check\nWe use sample_prior = \"only\" argument to get the prior predictive distribution.\n\nprior_pred &lt;- brm(contacts ~ 1 + age + hhsize,\n                 data = d,\n                 family = negbinomial(),\n                 prior = my_priors,\n                 iter = 1e4, sample_prior = \"only\", cores = 4)\n\nsaveRDS(prior_pred, \"prior_pred.rds\")\n\n\nprior_pred &lt;- readRDS(\"prior_pred.rds\")\npriorpc &lt;- pp_check(prior_pred, type=\"bars\", ndraws = 200)\n\npriorpc$data |&gt; \n  ggplot(aes(x=x))+\n  geom_col(aes(y=y_obs, fill=\"Data\"))+\n  geom_point(aes(y=m, color=\"Prior\"))+\n  geom_linerange(aes(ymax=h, ymin=l, color=\"Prior\"))+\n  scale_x_continuous(limits=c(-1,30))+\n  labs(x=\"Number of contacts per day per person\", y=\"Frequency\")+\n  scale_color_manual(\"\", values=c(\"Prior\" = set1[3]))+\n  scale_fill_manual(\"\", values=c(\"Data\" = set1[2]))+\n  theme_bw()+\n  theme(legend.position=c(0.8,0.8))\n\n\n\n\n\n\nFit the model\nGelman et al. writes:\n\n” … The first step in validating computation is to check that the model actually finishes the fitting process in an acceptable time frame and the convergence diagnostics are reasonable. In the context of HMC, this is primarily the absence of divergent transitions, \\(\\hat{R}\\) diagnostic near 1, and sufficient effective sample sizes for the central tendency, the tail quantiles, and the energy (Vehtari et al., 2020). …”\n\n\nfit1 &lt;- brm(contacts ~ 1 + age + hhsize, \n           data = d, \n           family = negbinomial(),\n           prior = my_priors,\n           iter = 1e4, cores = 4)\nsaveRDS(fit1, \"fit1.rds\")\n\nbrms package reports all the relevant information to check the fitting.\n\nfit1 &lt;- readRDS(\"fit1.rds\")\nsummary(fit1)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: contacts ~ 1 + age + hhsize \n   Data: d (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.13      0.05     1.03     1.23 1.00    18152    14753\nage2          0.48      0.05     0.38     0.57 1.00    22583    16259\nage3         -0.47      0.05    -0.57    -0.37 1.00    21570    15970\nhhsize2       0.71      0.05     0.60     0.81 1.00    19003    15277\nhhsize3       0.96      0.05     0.85     1.06 1.00    18962    15515\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     4.47      0.36     3.81     5.22 1.00    25673    14986\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nPosterior predictive values along with prior predictive values\n\nprior_pred &lt;- readRDS(\"prior_pred.rds\")\npriorpc &lt;- pp_check(prior_pred, type=\"bars\", ndraws = 200)\npostpc &lt;- pp_check(fit1, type=\"bars\", ndraws = 200)\n\npostpc$data |&gt; \n  ggplot(aes(x=x))+\n  geom_col(aes(y=y_obs, fill=\"Data\"))+\n  geom_point(data=priorpc$data, aes(y=m, color=\"Prior\"))+\n  geom_linerange(data=priorpc$data,aes(ymax=h, ymin=l, color=\"Prior\"))+\n  geom_point(aes(y=m, color=\"Posterior\"))+\n  geom_linerange(aes(ymax=h, ymin=l, color=\"Posterior\"))+\n  scale_x_continuous(limits=c(-1,30))+\n  labs(x=\"Number of contacts per day per person\", y= \"Frequency\")+\n  scale_color_manual(\"\", values=c(\"Prior\"=set1[3],\n                               \"Posterior\"=set1[4]))+\n  scale_fill_manual(\"\", values=c(\"Data\"=set1[2]))+\n  theme_bw()+\n  theme(legend.position=c(0.8,0.8))"
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#model-1-age-and-household-size",
    "href": "blog/posts/bayesian_workflow/index.html#model-1-age-and-household-size",
    "title": "Bayesian workflow: fake social contact data example",
    "section": "Model 1: age and household size",
    "text": "Model 1: age and household size\nFor this model, we assume that the dependent variable is mostly influenced by age and household size and do not include sex variable in the model.\nWe use the brms package to implement the Bayesian regression model. We first set the priors for the intercept, a_*, the age and household size, b_*, and the shape parameter of the negative binomial distribution, shp_*.\n\n# prior distributions\n# intercept, modeled as normal\na_mean &lt;- 0\na_scale &lt;- 1.5 # exp(2*a_scale) will likely cover plausible values \n# beta for the age and the household, modeled as normal\nb_mean &lt;- 0\nb_scale &lt;- 1.5\n# shape parameter of the negbin distr, modeled as inverse gamma\nshp_alpha &lt;- 0.8 # 0.4 (brms default) causes divergent transitions,\nshp_beta &lt;- 0.3\n\nmy_priors &lt;- \n  c(prior_string(paste0(\"normal(\", a_mean, \",\", a_scale,\")\"), \n                 class = \"Intercept\"),\n    prior_string(paste0(\"normal(\", b_mean, \",\", b_scale,\")\"),\n                 class = \"b\"),\n    prior_string(paste0(\"inv_gamma(\", shp_alpha, \",\", shp_beta,\")\"), \n                 class=\"shape\"))\n\n\nPrior predictive check\nWe use sample_prior = \"only\" argument to get the prior predictive distribution.\n\nprior_pred &lt;- brm(contacts ~ 1 + age + hhsize,\n                 data = d,\n                 family = negbinomial(),\n                 prior = my_priors,\n                 iter = 1e4, sample_prior = \"only\", cores = 4)\n\nsaveRDS(prior_pred, \"prior_pred.rds\")\n\n\nprior_pred &lt;- readRDS(\"prior_pred.rds\")\npriorpc &lt;- pp_check(prior_pred, type=\"bars\", ndraws = 200)\n\npriorpc$data |&gt; \n  ggplot(aes(x=x))+\n  geom_col(aes(y=y_obs, fill=\"Data\"))+\n  geom_point(aes(y=m, color=\"Prior\"))+\n  geom_linerange(aes(ymax=h, ymin=l, color=\"Prior\"))+\n  scale_x_continuous(limits=c(-1,30))+\n  labs(x=\"Number of contacts per day per person\", y=\"Frequency\")+\n  scale_color_manual(\"\", values=c(\"Prior\" = set1[3]))+\n  scale_fill_manual(\"\", values=c(\"Data\" = set1[2]))+\n  theme_bw()+\n  theme(legend.position=c(0.8,0.8))\n\n\n\n\n\n\nFit the model\nGelman et al. writes: ” … The first step in validating computation is to check that the model actually finishes the fitting process in an acceptable time frame and the convergence diagnostics are reasonable. In the context of HMC, this is primarily the absence of divergent transitions, \\(\\hat{R}\\) diagnostic near 1, and sufficient effective sample sizes for the central tendency, the tail quantiles, and the energy (Vehtari et al., 2020). …”\n\nfit1 &lt;- brm(contacts ~ 1 + age + hhsize, \n           data = d, \n           family = negbinomial(),\n           prior = my_priors,\n           iter = 1e4, cores = 4)\nsaveRDS(fit1, \"fit1.rds\")\n\nbrms package reports all the relevant information to check the fitting.\n\nfit1 &lt;- readRDS(\"fit1.rds\")\nsummary(fit1)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: contacts ~ 1 + age + hhsize \n   Data: d (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.13      0.05     1.04     1.22 1.00    17035    13697\nage2          0.40      0.04     0.32     0.49 1.00    20548    15508\nage3         -0.47      0.05    -0.57    -0.37 1.00    20229    15028\nhhsize2       0.78      0.05     0.68     0.88 1.00    17675    15906\nhhsize3       1.10      0.05     1.01     1.20 1.00    17685    16213\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     5.30      0.45     4.48     6.24 1.00    22342    14546\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nPosterior predictive values along with prior predictive values\n\nprior_pred &lt;- readRDS(\"prior_pred.rds\")\npriorpc &lt;- pp_check(prior_pred, type=\"bars\", ndraws = 200)\npostpc &lt;- pp_check(fit1, type=\"bars\", ndraws = 200)\n\npostpc$data |&gt; \n  ggplot(aes(x=x))+\n  geom_col(aes(y=y_obs, fill=\"Data\"))+\n  geom_point(data=priorpc$data, aes(y=m, color=\"Prior\"))+\n  geom_linerange(data=priorpc$data,aes(ymax=h, ymin=l, color=\"Prior\"))+\n  geom_point(aes(y=m, color=\"Posterior\"))+\n  geom_linerange(aes(ymax=h, ymin=l, color=\"Posterior\"))+\n  scale_x_continuous(limits=c(-1,30))+\n  labs(x=\"Number of contacts per day per person\", y= \"Frequency\")+\n  scale_color_manual(\"\", values=c(\"Prior\"=set1[3],\n                               \"Posterior\"=set1[4]))+\n  scale_fill_manual(\"\", values=c(\"Data\"=set1[2]))+\n  theme_bw()+\n  theme(legend.position=c(0.8,0.8))"
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#understanding-and-comparing-multiple-models",
    "href": "blog/posts/bayesian_workflow/index.html#understanding-and-comparing-multiple-models",
    "title": "Bayesian workflow: fake social contact data example",
    "section": "Understanding and comparing multiple models",
    "text": "Understanding and comparing multiple models\nGelman et al. writes:\n“… The key aspect of Bayesian workflow, which takes it beyond Bayesian data analysis, is that we are fitting many models while working on a single problem. We are not talking here about model selection or model averaging but rather of the use of a series of fitted models to better understand each one. …”\nModel evaluation\nLeave-one-out (LOO) cross validation as suggested in Vehtari et al. and Gelman et al..\nWe fit two additional models: fit2 and fit0. fit2 includes sex variable in addition to all variables included in fit1 and could be better or worse than fit1 as impact of sex is not big in the data set. fit0 is intercept-only model and thus is very likely to be worse than fit1 or fit2."
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#model-2-include-sex-in-addition-to-age-and-household-size-as-covariates.",
    "href": "blog/posts/bayesian_workflow/index.html#model-2-include-sex-in-addition-to-age-and-household-size-as-covariates.",
    "title": "Bayesian workflow: fake social contact data example",
    "section": "Model 2: Include sex in addition to age and household size as covariates.",
    "text": "Model 2: Include sex in addition to age and household size as covariates.\n\nfit2 &lt;- brm(contacts ~ 1 + age + hhsize + sex, \n           data = d, \n           family = negbinomial(),\n           prior = my_priors,\n           iter = 1e4, cores = 4)\nsaveRDS(fit2, \"fit2.rds\")\n\nfit0 &lt;- brm(contacts ~ 1, \n           data = d, \n           family = negbinomial(),\n           prior = set_prior(\"normal(0,5)\", class=\"Intercept\"),\n           iter = 1e4, cores = 4)\nsaveRDS(fit0, \"fit0.rds\")\n\nSummary of fit2 and fit2\n\nfit2 &lt;- readRDS(\"fit2.rds\") \nfit0 &lt;- readRDS(\"fit0.rds\")\nsummary(fit2)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: contacts ~ 1 + age + hhsize + sex \n   Data: d (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.12      0.05     1.02     1.22 1.00    21089    14681\nage2          0.40      0.04     0.32     0.49 1.00    23623    15288\nage3         -0.47      0.05    -0.57    -0.37 1.00    23780    16196\nhhsize2       0.78      0.05     0.68     0.88 1.00    21501    16727\nhhsize3       1.10      0.05     1.01     1.20 1.00    20906    17372\nsex2          0.01      0.04    -0.06     0.08 1.00    30179    14675\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     5.28      0.45     4.49     6.22 1.00    30118    15669\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nsummary(fit0)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: contacts ~ 1 \n   Data: d (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.92      0.03     1.87     1.97 1.00    15994    12293\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     2.02      0.12     1.79     2.26 1.00    15828    12635\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nShape parameter estimates\n\nprobs &lt;- c(0.025, 0.25, 0.5, 0.75, 0.975)\nshp1 &lt;- quantile(as.matrix(fit1)[,\"shape\"], probs=probs)\nshp2 &lt;- quantile(as.matrix(fit2)[,\"shape\"], probs=probs)\nshp0 &lt;- quantile(as.matrix(fit0)[,\"shape\"], probs=probs)\n\ndf &lt;- data.frame(Model = \"Model 0\", t(shp0))\ndf &lt;- rbind(df, data.frame(Model = \"Model 1\", t(shp1)),\n            data.frame(Model = \"Model 2\", t(shp2)))\n# df$percentile &lt;- rep(as.character(100*probs),3)\n\nggplot(df, aes(x=Model))+\n  geom_linerange(aes(ymin=`X2.5.`, ymax=`X97.5.`))+\n  geom_linerange(aes(ymin=`X25.`, ymax=`X75.`), linewidth=1)+\n  geom_point(aes(x=Model, y=`X50.`))+\n  ggtitle(\"\")+\n  theme_light() + \n  labs(y=\"Inference for the shape parameter\", x=\"\")\n\n\n\n\nCompare fit0, fit1 and fit2 suing leave-one out (LOO) cross-validation\n\nlibrary(loo)\nloo_fit0 &lt;- loo(fit0)\nloo_fit1 &lt;- loo(fit1)\nloo_fit2 &lt;- loo(fit2)\nsaveRDS(loo_fit0, \"loo_fit0.rds\")\nsaveRDS(loo_fit1, \"loo_fit1.rds\")\nsaveRDS(loo_fit2, \"loo_fit2.rds\")\n\n\nloo_fit0 &lt;- readRDS(\"loo_fit0.rds\")\nloo_fit1 &lt;- readRDS(\"loo_fit1.rds\")\nloo_fit2 &lt;- readRDS(\"loo_fit2.rds\")\nprint(loo_fit0)\n\n\nComputed from 20000 by 1000 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo  -2924.2 26.4\np_loo         2.1  0.2\nlooic      5848.4 52.9\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\nprint(loo_fit1)\n\n\nComputed from 20000 by 1000 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo  -2618.3 26.2\np_loo         5.9  0.3\nlooic      5236.6 52.5\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\nprint(loo_fit2)\n\n\nComputed from 20000 by 1000 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo  -2619.3 26.2\np_loo         6.9  0.4\nlooic      5238.5 52.4\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\nbrms::loo_compare(loo_fit1, loo_fit2, loo_fit0)\n\n     elpd_diff se_diff\nfit1    0.0       0.0 \nfit2   -1.0       0.3 \nfit0 -305.9      19.4 \n\n\n\nfit0 &lt;- add_criterion(fit0, \"waic\")\nfit1 &lt;- add_criterion(fit1, \"waic\")\nfit2 &lt;- add_criterion(fit2, \"waic\")\n\nbrms::loo_compare(fit0, fit1, fit2, criterion = \"waic\")\n\n     elpd_diff se_diff\nfit1    0.0       0.0 \nfit2   -1.0       0.3 \nfit0 -305.9      19.4 \n\n\nIt appears that fit1 is the most favored by the looic metric."
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#inference",
    "href": "blog/posts/bayesian_workflow/index.html#inference",
    "title": "Bayesian workflow: fake social contact data example",
    "section": "Inference",
    "text": "Inference\nPrior and posterior distributions of parameters. Trace plots and histograms\n\nplot(fit1, pars = \"^b_\")\n\n\n\n# plot(fit, pars = \".*age*.\")\n# plot(fit, pars = \".*hhsize*.\")\n\nCorrelations\n\npairs(fit1, pars = \".*age*.\")\n\n\n\n\nThere is a positive correlation between b_age2 and b_age3. It appears that increase in one parameter creates a deviation from the specified distribution (negative binomial distribution) and the other parameter also increases for all predicted values conform to the specified distribution.\n\npairs(fit1, pars = \"^b_age*.|^In*.\")\n\n\n\n\nI also expected that the increase in those parameters (b_age2 and b_age3) lead to the decrease in the Intercept to maintain the mean. This is, however, only clear when examining the association among Intercept, b_hhsize2, and b_hhsize3.\n\npairs(fit1, pars = \".*hh*.\")\n\n\n\npairs(fit1, pars = \"^b_hh*.|^In*.\")\n\n\n\n\n\\(\\beta_{\\text{age}}\\) parameter\n\ndf_post &lt;- as.data.frame(fit1)\nx &lt;- seq(-2, 2, 0.01)\ndf &lt;- data.frame(x=seq(-2, 2, 0.01))\ndf$prior_density &lt;-  dnorm(df$x, b_mean, b_scale)\n\nplot(exp(x), dnorm(x, b_mean, b_scale), type=\"l\", ylim=c(0, 20), \n     xlab=\"Value\", ylab=\"Density\", main=expression(beta[age]))\nlines(density(exp(df_post$b_age2)), col=set1[2])\nlines(density(exp(df_post$b_age3)), col=set1[3])\nabline(v=rr_age_true, col=set1[1], lwd=1.2, lty=\"dotted\")\nlegend(\"topright\",\n       bty = \"n\",\n       lty=c(1,1,1,3),\n       col=c(1,set1[2],set1[3],set1[1]),\n       legend=c(\"Prior\",\"age2\",\"age3\",\"True\"),\n       inset=0.02)\n\n\n\n# ggplot2 way\n# ggplot()+\n#   geom_line(data=df, aes(x=exp(x), y=prior_density, color=\"Prior\"))+\n#   geom_density(data=df_post, aes(x=exp(b_age2), color=\"Posterior b_age2\"))+\n#   geom_density(data=df_post, aes(x=exp(b_age3), color=\"Posterior b_age3\"))+\n#   scale_color_manual(values=c(\"Prior\"=\"forestgreen\", \n#                               \"Posterior b_age2\"=\"firebrick\",\n#                               \"Posterior b_age3\"=\"steelblue\"))+\n#   labs(color=\"\", x=\"value\", y=\"density\")+\n#   theme(legend.position=\"bottom\")\n\nTrue \\(\\beta_{\\text{age}}\\) values were well retrieved.\n\\(\\beta_{\\text{hhsize}}\\) parameter\n\nplot(exp(x), dnorm(x, b_mean, b_scale), type=\"l\", ylim=c(0,5), \n     xlab=\"Value\", ylab=\"Density\", main=expression(beta[hhsize]))\nlines(density(exp(df_post$b_hhsize2)), col=set1[2])\nlines(density(exp(df_post$b_hhsize3)), col=set1[3])\nabline(v=rr_hhsize_true, col=set1[1], lwd=1.2, lty=\"dotted\")\n\nlegend(\"topright\",\n       bty = \"n\",\n       lty=c(1,1,1,3),\n       col=c(1, set1[2], set1[3], set1[1]),\n       legend=c(\"Prior\", \"hhsize2\", \"hhsize3\", \"True\"),\n       inset=0.02)\n\n\n\n\nIntercept\n\nx &lt;- seq(-3,3,0.01)\nplot(exp(x), dnorm(x, a_mean, a_scale), type=\"l\", ylim=c(0,4), \n     xlab=\"Value\", ylab=\"Density\", main=\"Intercept\")\nlines(density(exp(df_post$b_Intercept)), col=set1[2])\nabline(v=intercept_true, col=set1[1], lwd=1.2, lty=\"dotted\")\nlegend(\"topright\",\n       bty = \"n\",\n       lty=c(1,1,3),\n       col=c(1,set1[2],set1[1]),\n       legend=c(\"Prior\", \"Posterior\", \"True\"),\n       inset=0.02)\n\n\n\n\nShape parameter\n\nlibrary(extraDistr)\nx &lt;- seq(0,10,0.01)\nplot(x, dinvgamma(x, shp_alpha, shp_beta), type=\"l\", \n     ylim=c(0,2), xlab=\"Value\", ylab=\"Density\", main=\"Shape\")\nlines(density(df_post$shape), col=set1[2])\nabline(v=shape_true, col=set1[1], lwd=1.2, lty=\"dotted\")\nlegend(\"topright\",\n       bty = \"n\",\n       lty=c(1,1,3),\n       col=c(1,set1[2],set1[1]),\n       legend=c(\"Prior\", \"Posterior\", \"True\"),\n       inset=0.02)"
  },
  {
    "objectID": "blog/posts/bayesian_workflow/index.html#model-checking-improvement-and-comparing-multiple-models",
    "href": "blog/posts/bayesian_workflow/index.html#model-checking-improvement-and-comparing-multiple-models",
    "title": "Bayesian workflow: fake social contact data example",
    "section": "Model checking, improvement, and comparing multiple models",
    "text": "Model checking, improvement, and comparing multiple models\nGelman et al. writes:\n\n“… The key aspect of Bayesian workflow, which takes it beyond Bayesian data analysis, is that we are fitting many models while working on a single problem. We are not talking here about model selection or model averaging but rather of the use of a series of fitted models to better understand each one. …”\n\n\nModel evaluation\nWe use leave-one-out (LOO) cross validation as suggested in Vehtari et al. and Gelman et al..\nWe fit two additional models: fit2 and fit0. fit2 includes sex variable in addition to all variables included in fit1 and could be better or worse than fit1 as impact of sex is not big in the data set. fit0 is intercept-only model and thus is very likely to be worse than fit1 or fit2.\n\n\nModel 2: Include sex in addition to age and household size as covariates.\n\nfit2 &lt;- brm(contacts ~ 1 + age + hhsize + sex, \n           data = d, \n           family = negbinomial(),\n           prior = my_priors,\n           iter = 1e4, cores = 4)\nsaveRDS(fit2, \"fit2.rds\")\n\nfit0 &lt;- brm(contacts ~ 1, \n           data = d, \n           family = negbinomial(),\n           prior = set_prior(\"normal(0,5)\", class=\"Intercept\"),\n           iter = 1e4, cores = 4)\nsaveRDS(fit0, \"fit0.rds\")\n\nSummary of fit0 and fit2\n\nfit2 &lt;- readRDS(\"fit2.rds\") \nfit0 &lt;- readRDS(\"fit0.rds\")\nsummary(fit2)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: contacts ~ 1 + age + hhsize + sex \n   Data: d (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.05      0.05     0.94     1.16 1.00    21036    16258\nage2          0.49      0.05     0.40     0.59 1.00    23875    16265\nage3         -0.46      0.05    -0.57    -0.36 1.00    23032    15811\nhhsize2       0.70      0.05     0.60     0.80 1.00    22731    15830\nhhsize3       0.95      0.05     0.85     1.06 1.00    21280    16524\nsex2          0.15      0.04     0.07     0.23 1.00    29050    14273\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     4.57      0.37     3.89     5.35 1.00    30332    14155\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nsummary(fit0)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: contacts ~ 1 \n   Data: d (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.83      0.03     1.78     1.88 1.00    16643    12592\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     1.92      0.11     1.71     2.15 1.00    15701    13170\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nShape parameter estimates\n\nprobs &lt;- c(0.025, 0.25, 0.5, 0.75, 0.975)\nshp1 &lt;- quantile(as.matrix(fit1)[,\"shape\"], probs=probs)\nshp2 &lt;- quantile(as.matrix(fit2)[,\"shape\"], probs=probs)\nshp0 &lt;- quantile(as.matrix(fit0)[,\"shape\"], probs=probs)\n\ndf &lt;- data.frame(Model = \"Model 0\", t(shp0))\ndf &lt;- rbind(df, data.frame(Model = \"Model 1\", t(shp1)),\n            data.frame(Model = \"Model 2\", t(shp2)))\n# df$percentile &lt;- rep(as.character(100*probs),3)\n\nggplot(df, aes(x=Model))+\n  geom_linerange(aes(ymin=`X2.5.`, ymax=`X97.5.`))+\n  geom_linerange(aes(ymin=`X25.`, ymax=`X75.`), linewidth=1)+\n  geom_point(aes(x=Model, y=`X50.`))+\n  ggtitle(\"\")+\n  theme_light() + \n  labs(y=\"Inference for the shape parameter\", x=\"\")\n\n\n\n\nCompare fit0, fit1 and fit2 models\nUsing WAIC\n\nfit1 &lt;- add_criterion(fit1, \"waic\")\nfit0 &lt;- add_criterion(fit0, \"waic\")\nfit2 &lt;- add_criterion(fit2, \"waic\")\n\nwaic(fit0)\n\n\nComputed from 20000 by 1000 log-likelihood matrix\n\n          Estimate   SE\nelpd_waic  -2848.4 28.0\np_waic         2.1  0.2\nwaic        5696.7 56.0\n\nwaic(fit1)\n\n\nComputed from 20000 by 1000 log-likelihood matrix\n\n          Estimate   SE\nelpd_waic  -2575.3 26.8\np_waic         5.8  0.4\nwaic        5150.5 53.7\n\nwaic(fit2)\n\n\nComputed from 20000 by 1000 log-likelihood matrix\n\n          Estimate   SE\nelpd_waic  -2569.2 26.6\np_waic         6.6  0.4\nwaic        5138.4 53.1\n\nloo_compare(fit1, fit2, fit0, criterion = \"waic\")\n\n     elpd_diff se_diff\nfit2    0.0       0.0 \nfit1   -6.1       3.6 \nfit0 -279.2      19.9 \n\n\nUsing leave-one-out (LOO) cross-validation\n\nfit1 &lt;- add_criterion(fit1, \"loo\")\nfit0 &lt;- add_criterion(fit0, \"loo\")\nfit2 &lt;- add_criterion(fit2, \"loo\")\nloo_compare(fit1, fit2, fit0, criterion = \"loo\")\n\n     elpd_diff se_diff\nfit2    0.0       0.0 \nfit1   -6.1       3.6 \nfit0 -279.2      19.9 \n\n\nbrms::loo_compare uses the loo::loo_compare under the hood and and loo::loo_compare package produces the same results.\n\nlibrary(loo)\nloo_fit0 &lt;- loo(fit0)\nloo_fit1 &lt;- loo(fit1)\nloo_fit2 &lt;- loo(fit2)\n\nprint(loo_fit0)\n\n\nComputed from 20000 by 1000 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo  -2848.4 28.0\np_loo         2.1  0.2\nlooic      5696.7 56.0\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\nprint(loo_fit1)\n\n\nComputed from 20000 by 1000 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo  -2575.3 26.8\np_loo         5.8  0.4\nlooic      5150.5 53.7\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\nprint(loo_fit2)\n\n\nComputed from 20000 by 1000 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo  -2569.2 26.6\np_loo         6.6  0.4\nlooic      5138.4 53.1\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\nbrms::loo_compare(loo_fit1, loo_fit2, loo_fit0)\n\n     elpd_diff se_diff\nfit2    0.0       0.0 \nfit1   -6.1       3.6 \nfit0 -279.2      19.9 \n\n\nIt appears that fit2 is the most favored by LOO cross validation. This implies that adding sex variable still improves the model performance while its influence on the number of contacts relatively small compared to the age and household size.\n\nsummary(fit2)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: contacts ~ 1 + age + hhsize + sex \n   Data: d (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.05      0.05     0.94     1.16 1.00    21036    16258\nage2          0.49      0.05     0.40     0.59 1.00    23875    16265\nage3         -0.46      0.05    -0.57    -0.36 1.00    23032    15811\nhhsize2       0.70      0.05     0.60     0.80 1.00    22731    15830\nhhsize3       0.95      0.05     0.85     1.06 1.00    21280    16524\nsex2          0.15      0.04     0.07     0.23 1.00    29050    14273\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     4.57      0.37     3.89     5.35 1.00    30332    14155\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "blog/posts/behavior_change/index.html",
    "href": "blog/posts/behavior_change/index.html",
    "title": "Modeling human behavior: Infectious disease transmission modeling perspective",
    "section": "",
    "text": "감염병의 전파 양상을 연구하는 감염병 역학 (Epidemiology of Infectious Diseases ; EID)에서는 감염병의 발생 양샹을 세 가지 요소로 해석하고자 한다. 하나는 병을 일으키는 감염원 (infectious agent)이고, 다른 하나는 사람이 살아가고 있는 주변 환경 (environment)이며 마지막은 감염되는 대상, 즉 숙주 (host) 이다[Barreto et al.].\nSebastian Funk et al. 의 논문 Modelling the influence of human behaviour on the spread of infectious diseases: a review은 을 살펴보고자 한다.\n\n“… After all, individual self-initiated behaviour can change the fate of an outbreak, and its interaction with disease dynamics requires proper understanding if we are to fully comprehend what happens when a disease spreads through human populations Ferguson et al. Capturing human behaviour. …”"
  },
  {
    "objectID": "blog/posts/behavior_change/index.html#bayesian-workflow",
    "href": "blog/posts/behavior_change/index.html#bayesian-workflow",
    "title": "Bayesian workflow: fake social contact data example",
    "section": "",
    "text": "사람의 질병 발생 패턴을 연구하는 학문을 역학, 영어로는 epidemiology,이라고 한다. 패턴을 이해하는 데 필수적인 세가지 축은 감염원 (infectious agent), 환경 (environment), 숙주 (host) 이다.\n연구하는 세 가지 패턴은\n그 중에 감염병학 (epidemiology))사람 간 감염병 전파에 효과적으로 대처하기 위해서는 감염병 전파에 잘 이해하여야 한다. 이러한 감염병 역학 (epidemiology)을 감염병 전파를 잘 이해하기 위해서는 감염원 (infectious agent)에 대한 이해가 필수적인데 그에 못지 않게 중요할 수 있는 것이 감염병의 전파에 반응하여 시시각각 변할 수 있는 사람 (host). Sebastian Funk et al. 의 논문 Modelling the influence of human behaviour on the spread of infectious diseases: a review 을 살펴보고자 한다.\n논문에서 다음과 같이 적었다:\n\n“… After all, individual self-initiated behaviour can change the fate of an outbreak, and its interaction with disease dynamics requires proper understanding if we are to fully comprehend what happens when a disease spreads through human populations Ferguson et al. Capturing human behaviour. …”"
  },
  {
    "objectID": "blog/posts/behavior_change/index.html#model-building",
    "href": "blog/posts/behavior_change/index.html#model-building",
    "title": "Modeling human behavior: Infectious disease transmission modeling perspective",
    "section": "Model building",
    "text": "Model building\n\nConceptual analysis\nLet’s suppose that we measured how the number of contact a person makes varies.A prior study and other numerous studies showed that contact frequencies differ by age. And a significant fraction of contacts occur with household members and as such the contact frequencies will likely vary by household size. We sampled a group of people and followed them for some time (e.g., 1 week) and measured the number of contacts that a typical person makes in a day. We also collected variables such as age, sex, and household size.\nThere are three conceptual modelling approaches: descriptive, predictive and explanatory approaches as per the study by Shmueli. This study is best described as a descriptive study and does not directly aim at obtaining optimal predictive performance, but at capturing the data structure parsimoniously. For a descriptive model, interpretability, transportability and general usability are important criteria as described in the article.\nAssuming that the number of contacts per day per person, \\(y_i\\), follows a negative binomial distribution, we can mathematically express our concepts in the following equations:\n\\[\ny_i \\sim \\text{NB}(y_i|\\mu_i,\\phi)=\\frac{\\Gamma (y_i + \\theta)}{\\Gamma (\\theta) y_i !}) \\left(\\frac{\\mu_i}{\\mu_i+\\phi} \\right)^y \\left(\\frac{\\phi}{\\mu_i+\\phi} \\right)^\\phi\n\\tag{1}\\]\n\\[\\text{log}(\\mu_i)= \\alpha + \\beta_{\\text{age}_i}  + \\beta_{\\text{sex}_i} +\\beta_{\\text{hhsize}_i} \\tag{2}\\]\n\\(\\mu_i\\) and \\(\\phi\\) represent the mean number of contacts per day for a person \\(i\\) and the shape parameter of the negative binomial distribution, respectively. \\(\\Gamma (\\cdot)\\) denotes a gamma function.\n\n\nImplemetation\nLoad packages\n\n# library(posterior)\n# options(pillar.neg = FALSE, pillar.subtle=FALSE, pillar.sigfig=2)\nlibrary(brms)\nlibrary(tidyverse)\n# library(bayesplot)\n# theme_set(bayesplot::theme_default(base_family = \"sans\"))\nset1 &lt;- RColorBrewer::brewer.pal(7, \"Set1\")\n\nGenerate the fake data\nWe take the fake-data simulation approach, which can help us understand our data model and priors, what can be learned from an experiment, and the validity of the applied inference methods. Parameters of interest are the ratios of number of contacts per day (i.e., rate ratios or relative risks) by age, sex, and household size. Plugging Equation 2 into Equation 1 becomes our generative model."
  },
  {
    "objectID": "blog/posts/behavior_change/index.html#epidemiology",
    "href": "blog/posts/behavior_change/index.html#epidemiology",
    "title": "Modeling human behavior: Infectious disease transmission modeling perspective",
    "section": "",
    "text": "감염병의 전파 양상을 연구하는 감염병 역학 (Epidemiology of Infectious Diseases ; EID)에서는 감염병의 발생 양샹을 세 가지 요소로 해석하고자 한다. 하나는 병을 일으키는 감염원 (infectious agent)이고, 다른 하나는 사람이 살아가고 있는 주변 환경 (environment)이며 마지막은 감염되는 대상, 즉 숙주 (host) 이다[Barreto et al.].\nSebastian Funk et al. 의 논문 Modelling the influence of human behaviour on the spread of infectious diseases: a review은 을 살펴보고자 한다.\n\n“… After all, individual self-initiated behaviour can change the fate of an outbreak, and its interaction with disease dynamics requires proper understanding if we are to fully comprehend what happens when a disease spreads through human populations Ferguson et al. Capturing human behaviour. …”"
  },
  {
    "objectID": "blog/posts/distribution_matching/index.html",
    "href": "blog/posts/distribution_matching/index.html",
    "title": "Distribution Matching Methods: From Theory to Practice",
    "section": "",
    "text": "# Load required packages\nlibrary(MASS)\nlibrary(ks)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(tidyr)\nlibrary(dplyr)"
  },
  {
    "objectID": "blog/posts/distribution_matching/index.html#quantile-matching",
    "href": "blog/posts/distribution_matching/index.html#quantile-matching",
    "title": "Distribution Matching Methods: From Theory to Practice",
    "section": "2.1 Quantile Matching",
    "text": "2.1 Quantile Matching\nQuantile matching transforms data by mapping corresponding quantiles between distributions. It’s a non-parametric approach that preserves the rank order of the original data.\n\n\nCode\nquantile_match &lt;- function(A, B) {\n  probs &lt;- seq(0, 1, length.out = length(A))\n  sorted_A &lt;- sort(A)\n  \n  B_transformed &lt;- approx(x = probs,\n                         y = sorted_A,\n                         xout = rank(B)/(length(B) + 1),\n                         method = \"linear\")$y\n  return(B_transformed)\n}"
  },
  {
    "objectID": "blog/posts/distribution_matching/index.html#box-cox-transformation",
    "href": "blog/posts/distribution_matching/index.html#box-cox-transformation",
    "title": "Distribution Matching Methods: From Theory to Practice",
    "section": "2.2 Box-Cox Transformation",
    "text": "2.2 Box-Cox Transformation\nThe Box-Cox transformation is particularly useful when you want to transform data to approximate normality before matching moments.\n\n\nCode\nboxcox_match &lt;- function(A, B) {\n  # Find optimal lambda for both distributions\n  bc_A &lt;- boxcox(A ~ 1, plotit = FALSE)\n  lambda_A &lt;- bc_A$x[which.max(bc_A$y)]\n  \n  # Transform to normality\n  transform_boxcox &lt;- function(x, lambda) {\n    if (abs(lambda) &lt; 1e-4) {\n      log(x)\n    } else {\n      (x^lambda - 1) / lambda\n    }\n  }\n  \n  # Transform both samples\n  A_transformed &lt;- transform_boxcox(A, lambda_A)\n  B_transformed &lt;- transform_boxcox(B, lambda_A)  # Use same lambda\n  \n  # Match moments\n  B_standardized &lt;- (B_transformed - mean(B_transformed)) / sd(B_transformed)\n  B_matched &lt;- B_standardized * sd(A_transformed) + mean(A_transformed)\n  \n  # Inverse transform\n  inverse_boxcox &lt;- function(x, lambda) {\n    if (abs(lambda) &lt; 1e-4) {\n      exp(x)\n    } else {\n      (lambda * x + 1)^(1/lambda)\n    }\n  }\n  \n  B_final &lt;- inverse_boxcox(B_matched, lambda_A)\n  return(B_final)\n}"
  },
  {
    "objectID": "blog/posts/distribution_matching/index.html#kernel-density-based-transformation",
    "href": "blog/posts/distribution_matching/index.html#kernel-density-based-transformation",
    "title": "Distribution Matching Methods: From Theory to Practice",
    "section": "2.3 Kernel Density-Based Transformation",
    "text": "2.3 Kernel Density-Based Transformation\nThis method uses kernel density estimation to transform the distributions.\n\n\nCode\nkernel_density_match &lt;- function(A, B, bw = \"nrd0\") {\n  # Estimate densities\n  density_A &lt;- kde(A, h = bw.nrd0(A))\n  density_B &lt;- kde(B, h = bw.nrd0(B))\n  \n  # Calculate CDFs using numerical integration\n  cdf_A &lt;- function(x) {\n    sapply(x, function(xi) {\n      mean(pnorm(xi, A, density_A$h))\n    })\n  }\n  \n  cdf_B &lt;- function(x) {\n    sapply(x, function(xi) {\n      mean(pnorm(xi, B, density_B$h))\n    })\n  }\n  \n  # Transform B to match A's distribution\n  B_probs &lt;- cdf_B(B)\n  \n  # Create quantile function for A using interpolation\n  A_sorted &lt;- sort(A)\n  A_probs &lt;- cdf_A(A_sorted)\n  \n  B_transformed &lt;- approx(x = A_probs,\n                         y = A_sorted,\n                         xout = B_probs,\n                         yleft = min(A),\n                         yright = max(A))$y\n  \n  return(B_transformed)\n}"
  },
  {
    "objectID": "blog/posts/distribution_matching/index.html#moment-matching",
    "href": "blog/posts/distribution_matching/index.html#moment-matching",
    "title": "Distribution Matching Methods: From Theory to Practice",
    "section": "2.4 Moment Matching",
    "text": "2.4 Moment Matching\nA simpler approach that focuses on matching the first two moments of the distributions.\n\n\nCode\nmoment_match &lt;- function(A, B) {\n  # Standardize B\n  B_std &lt;- (B - mean(B)) / sd(B)\n  \n  # Transform to match A's moments\n  B_transformed &lt;- B_std * sd(A) + mean(A)\n  \n  return(B_transformed)\n}"
  },
  {
    "objectID": "blog/posts/distribution_matching/index.html#visual-comparison",
    "href": "blog/posts/distribution_matching/index.html#visual-comparison",
    "title": "Distribution Matching Methods: From Theory to Practice",
    "section": "3.1 Visual Comparison",
    "text": "3.1 Visual Comparison\nLet’s create a more elegant visualization using ggplot2:\n\n# Create a data frame for plotting\ndf &lt;- data.frame(\n  Original_A = A,\n  Original_B = B,\n  Quantile = B_quantile,\n  BoxCox = B_boxcox,\n  Kernel = B_kernel,\n  Moment = B_moment\n)\n\n# Convert to long format\ndf_long &lt;- pivot_longer(df, \n                       cols = everything(),\n                       names_to = \"Method\",\n                       values_to = \"Value\")\n\n# Create the plot\nggplot(df_long, aes(x = Value, fill = Method)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~Method, scales = \"free_y\", ncol = 2) +\n  theme_minimal() +\n  labs(x = \"Value\",\n       y = \"Density\",\n       title = \"Comparison of Distribution Matching Methods\",\n       subtitle = \"Original A (target) vs Original B and transformed distributions\") +\n  theme(legend.position = \"none\")\n\n\n\n\nComparison of Distribution Matching Methods"
  },
  {
    "objectID": "blog/posts/distribution_matching/index.html#numerical-comparison",
    "href": "blog/posts/distribution_matching/index.html#numerical-comparison",
    "title": "Distribution Matching Methods: From Theory to Practice",
    "section": "3.2 Numerical Comparison",
    "text": "3.2 Numerical Comparison\nLet’s compare some summary statistics:\n\n# Function to calculate summary statistics\nget_stats &lt;- function(x) {\n  c(Mean = mean(x),\n    SD = sd(x),\n    Median = median(x),\n    Skewness = mean((x - mean(x))^3) / sd(x)^3,\n    Kurtosis = mean((x - mean(x))^4) / sd(x)^4)\n}\n\n# Calculate statistics for all distributions\nstats_df &lt;- data.frame(\n  Original_A = get_stats(A),\n  Original_B = get_stats(B),\n  Quantile = get_stats(B_quantile),\n  BoxCox = get_stats(B_boxcox),\n  Kernel = get_stats(B_kernel),\n  Moment = get_stats(B_moment)\n)\n\n# Display the results\nknitr::kable(round(stats_df, 3))\n\n\nSummary Statistics Comparison \n\n\n\nOriginal_A\nOriginal_B\nQuantile\nBoxCox\nKernel\nMoment\n\n\n\n\nMean\n10.032\n9.837\n10.031\n10.031\n10.093\n10.032\n\n\nSD\n1.983\n9.720\n1.967\n2.011\n1.872\n1.983\n\n\nMedian\n10.018\n6.684\n10.018\n9.424\n9.989\n9.389\n\n\nSkewness\n0.065\n1.686\n0.057\n1.532\n0.444\n1.686\n\n\nKurtosis\n2.920\n6.075\n2.842\n5.459\n2.736\n6.075"
  },
  {
    "objectID": "blog/posts/distribution_matching/index.html#quantile-matching-1",
    "href": "blog/posts/distribution_matching/index.html#quantile-matching-1",
    "title": "Distribution Matching Methods: From Theory to Practice",
    "section": "4.1 Quantile Matching",
    "text": "4.1 Quantile Matching\n\nPros: Preserves rank order, works with any distribution\nCons: May not extrapolate well\nBest for: General-purpose distribution matching"
  },
  {
    "objectID": "blog/posts/distribution_matching/index.html#box-cox-transformation-1",
    "href": "blog/posts/distribution_matching/index.html#box-cox-transformation-1",
    "title": "Distribution Matching Methods: From Theory to Practice",
    "section": "4.2 Box-Cox Transformation",
    "text": "4.2 Box-Cox Transformation\n\nPros: Works well for skewed data, preserves relationships\nCons: Requires positive data, assumes underlying normality\nBest for: Right-skewed positive data"
  },
  {
    "objectID": "blog/posts/distribution_matching/index.html#kernel-density-based",
    "href": "blog/posts/distribution_matching/index.html#kernel-density-based",
    "title": "Distribution Matching Methods: From Theory to Practice",
    "section": "4.3 Kernel Density-Based",
    "text": "4.3 Kernel Density-Based\n\nPros: Highly flexible, handles multimodal distributions\nCons: Computationally intensive, sensitive to bandwidth selection\nBest for: Complex, multimodal distributions"
  },
  {
    "objectID": "blog/posts/distribution_matching/index.html#moment-matching-1",
    "href": "blog/posts/distribution_matching/index.html#moment-matching-1",
    "title": "Distribution Matching Methods: From Theory to Practice",
    "section": "4.4 Moment Matching",
    "text": "4.4 Moment Matching\n\nPros: Simple, fast, preserves linear relationships\nCons: Only matches first two moments, assumes similar shapes\nBest for: Nearly normal distributions or quick approximations"
  },
  {
    "objectID": "blog/posts/correlation/index.html",
    "href": "blog/posts/correlation/index.html",
    "title": "Understanding correlation, covariation, and least squares regression",
    "section": "",
    "text": "Understanding relationships between variables is a fundamental aspect of statistical analysis. Three key concepts in this domain are correlation, covariation, and least squares regression. In this post, we will explore how these concepts interrelate and use R for demonstration."
  },
  {
    "objectID": "blog/posts/correlation/index.html#r-code-example",
    "href": "blog/posts/correlation/index.html#r-code-example",
    "title": "Understanding correlation, covariation, and least squares regression",
    "section": "R code example",
    "text": "R code example\n# Generate sample data\nset.seed(123)\nx &lt;- rnorm(100, mean = 50, sd = 10)\ny &lt;- 2 * x + rnorm(100, mean = 0, sd = 5)\n\n# Compute covariance\ncov_xy &lt;- cov(x, y)\nprint(cov_xy)"
  },
  {
    "objectID": "blog/posts/correlation/index.html#r-code-example-1",
    "href": "blog/posts/correlation/index.html#r-code-example-1",
    "title": "Understanding correlation, covariation, and least squares regression",
    "section": "R Code Example",
    "text": "R Code Example\n# Compute correlation\ncor_xy &lt;- cor(x, y)\nprint(cor_xy)"
  },
  {
    "objectID": "blog/posts/correlation/index.html#r-code-example-2",
    "href": "blog/posts/correlation/index.html#r-code-example-2",
    "title": "Understanding correlation, covariation, and least squares regression",
    "section": "R code Example",
    "text": "R code Example\n# Fit a linear model\nmodel &lt;- lm(y ~ x)\nsummary(model)\n\n# Plot regression\nplot(x, y, main = \"Least Squares Regression\", xlab = \"X\", ylab = \"Y\", pch = 19)\nabline(model, col = \"red\", lwd = 2)"
  },
  {
    "objectID": "blog/posts/correlation/index.html#summary-table",
    "href": "blog/posts/correlation/index.html#summary-table",
    "title": "Understanding correlation, covariation, and least squares regression",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\nConcept\nDefinition & Formula\nInterpretation\n\n\n\n\nCovariation\n\\(\\text{Cov}(X, Y) = \\frac{1}{n} \\sum (X_i - \\bar{X})(Y_i - \\bar{Y})\\)\nMeasures the direction of relationship but not the scale.\n\n\nCorrelation\n\\(\\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{s_X s_Y}\\)\nStandardized measure of association between -1 and 1.\n\n\nRegression\n\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\nUses covariance to estimate \\(\\beta_1\\) and predict \\(Y\\)."
  },
  {
    "objectID": "blog/posts/correlation/index.html#introduction",
    "href": "blog/posts/correlation/index.html#introduction",
    "title": "Understanding correlation, covariation, and least squares regression",
    "section": "",
    "text": "Understanding relationships between variables is a fundamental aspect of statistical analysis. Three key concepts in this domain are correlation, covariation, and least squares regression. In this post, we will explore how these concepts interrelate and use R for demonstration."
  },
  {
    "objectID": "blog/posts/correlation/index.html#covariance",
    "href": "blog/posts/correlation/index.html#covariance",
    "title": "Understanding correlation, covariation, and least squares regression",
    "section": "Covariance",
    "text": "Covariance\nCovariance measures how two variables move together. The covariance between two variables \\(X\\) and \\(Y\\) is given by:\n\\[\n\\text{Cov}(X, Y) = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})\n\\]\nwhere: - \\(X_i, Y_i\\) are individual observations, - \\(\\bar{X}, \\bar{Y}\\) are the means of \\(X\\) and \\(Y\\).\nA positive covariance indicates that as \\(X\\) increases, \\(Y\\) tends to increase. A negative covariance suggests an inverse relationship.\n\nR code example\n# Generate sample data\nset.seed(123)\nx &lt;- rnorm(100, mean = 50, sd = 10)\ny &lt;- 2 * x + rnorm(100, mean = 0, sd = 5)\n\n# Compute covariance\ncov_xy &lt;- cov(x, y)\nprint(cov_xy)"
  },
  {
    "objectID": "blog/posts/correlation/index.html#correlation",
    "href": "blog/posts/correlation/index.html#correlation",
    "title": "Understanding correlation, covariation, and least squares regression",
    "section": "Correlation",
    "text": "Correlation\nCorrelation standardizes covariance by dividing it by the product of standard deviations of \\(X\\) and \\(Y\\). The Pearson correlation coefficient is given by:\n\\[\n\\rho_{X,Y} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{X_i - \\bar{X}}{s_X}\\right) \\left(\\frac{Y_i - \\bar{Y}}{s_Y}\\right) = \\frac{\\text{Cov}(X, Y)}{s_X s_Y}\n\\] where \\(s_X\\) and \\(s_Y\\) are standard deviations of \\(X\\) and \\(Y\\). The correlation coefficient ranges from -1 to 1: - +1: Perfect positive correlation - -1: Perfect negative correlation - 0: No linear relationship\n\nR Code Example\n# Compute correlation\ncor_xy &lt;- cor(x, y)\nprint(cor_xy)"
  },
  {
    "objectID": "blog/posts/correlation/index.html#least-squares-regression",
    "href": "blog/posts/correlation/index.html#least-squares-regression",
    "title": "Understanding correlation, covariation, and least squares regression",
    "section": "Least squares regression",
    "text": "Least squares regression\nLeast squares regression models the relationship between \\(X\\) and \\(Y\\) by fitting a line that minimizes the sum of squared residuals:\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\]\nwhere: - \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope) are estimated using:\nThe equation for the \\(\\beta_1\\) can be derived by minimizing the error sum of squares (SSE) \\[\nSSE = \\sum_{i=1}^{n} (Y_i - (\\beta_0 + \\beta_1 X_i))^2\n\\] , which in turn can be solved using the following equation. \\[\n\\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2 = 0\n\\]\n\\[\n\\beta_1 = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}\n\\]\nThe intuition behind this formula is as follows. Covariance \\(\\text{Cov}(X,Y)\\) measures how \\(X\\) and \\(Y\\) vary together. Variance \\(\\text{Var}(X)\\) measures how spread out \\(X\\) is.\nThe ratio \\(\\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\\) tells us the change in \\(Y\\) for a one-unit change in \\(X\\). This makes sense because we are essentially scaling the covariance by the variability of \\(X\\)–ensuring that our slope correctly reflects how much \\(Y\\) changes per unit of \\(X\\).\n\\[\n\\beta_0 = \\bar{Y} - \\beta_1 \\bar{X}\n\\]\n\n\\(\\epsilon\\) represents the residual error.\n\n\nR code Example\n# Fit a linear model\nmodel &lt;- lm(y ~ x)\nsummary(model)\n\n# Plot regression\nplot(x, y, main = \"Least Squares Regression\", xlab = \"X\", ylab = \"Y\", pch = 19)\nabline(model, col = \"red\", lwd = 2)"
  },
  {
    "objectID": "blog/posts/correlation/index.html#relationship-between-concepts",
    "href": "blog/posts/correlation/index.html#relationship-between-concepts",
    "title": "Understanding correlation, covariation, and least squares regression",
    "section": "Relationship between concepts",
    "text": "Relationship between concepts\n\nCovariation quantifies the direction and strength of a linear relationship but does not standardize it.\nCorrelation standardizes covariance to provide a measure that is independent of the units of \\(X\\) and \\(Y\\).\nLeast squares regression uses covariance to estimate the slope (\\(\\beta_1\\)) and define the best-fit line.\n\n\nSummary Table\n\n\n\n\n\n\n\n\nConcept\nDefinition & Formula\nInterpretation\n\n\n\n\nCovariation\n\\(\\text{Cov}(X, Y) = \\frac{1}{n} \\sum (X_i - \\bar{X})(Y_i - \\bar{Y})\\)\nMeasures the direction of relationship but not the scale.\n\n\nCorrelation\n\\(\\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{s_X s_Y}\\)\nStandardized measure of association between -1 and 1.\n\n\nRegression\n\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\nUses covariance to estimate \\(\\beta_1\\) and predict \\(Y\\)."
  },
  {
    "objectID": "blog/posts/correlation/index.html#conclusion",
    "href": "blog/posts/correlation/index.html#conclusion",
    "title": "Understanding correlation, covariation, and least squares regression",
    "section": "Conclusion",
    "text": "Conclusion\nUnderstanding covariance, correlation, and regression allows for deeper insights into data relationships. Covariance provides directionality, correlation offers a unit-free measure of strength, and regression predicts outcomes. By applying these techniques in R, we can uncover meaningful patterns in data!"
  },
  {
    "objectID": "blog/posts/regularization/index.html",
    "href": "blog/posts/regularization/index.html",
    "title": "Regularization in statistical models",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(caret)\nlibrary(recipes)\nset.seed(123)"
  },
  {
    "objectID": "blog/posts/regularization/index.html#introduction",
    "href": "blog/posts/regularization/index.html#introduction",
    "title": "Regularization in statistical models",
    "section": "Introduction",
    "text": "Introduction\nRegularization is a fundamental technique in statistical modeling that helps prevent overfitting and improves model generalization. This blog post explores different regularization methods, their implementation in R, and their practical applications, with a special focus on scenarios where the number of parameters exceeds the number of observations (p &gt; n)."
  },
  {
    "objectID": "blog/posts/regularization/index.html#understanding-regularization",
    "href": "blog/posts/regularization/index.html#understanding-regularization",
    "title": "Regularization in statistical models",
    "section": "Understanding Regularization",
    "text": "Understanding Regularization\nRegularization works by adding a penalty term to the model’s loss function, effectively constraining the magnitude of model coefficients. The three most common types are:\n\nRidge (L2)\nLasso (L1)\nElastic Net (combination of L1 and L2)"
  },
  {
    "objectID": "blog/posts/regularization/index.html#simulation-study-when-p-n",
    "href": "blog/posts/regularization/index.html#simulation-study-when-p-n",
    "title": "Regularization in statistical models",
    "section": "Simulation Study: When p > n",
    "text": "Simulation Study: When p &gt; n\nLet’s first create a scenario where we have more parameters than observations:\n\n\nCode\n# Generate synthetic data\nn &lt;- 50  # number of observations\np &lt;- 100 # number of predictors\n\n# Create sparse true coefficients (most are zero)\ntrue_beta &lt;- rep(0, p)\ntrue_beta[1:5] &lt;- c(1.5, -0.8, 1.2, -0.5, 0.9)\n\n# Generate predictor matrix\nX &lt;- matrix(rnorm(n * p), nrow = n)\n# Generate response with some noise\ny &lt;- X %*% true_beta + rnorm(n, sd = 0.5)\n\n# Convert to data frame\ndata &lt;- as.data.frame(X)\ndata$y &lt;- y"
  },
  {
    "objectID": "blog/posts/regularization/index.html#ridge-regression-l2",
    "href": "blog/posts/regularization/index.html#ridge-regression-l2",
    "title": "Regularization in statistical models",
    "section": "Ridge Regression (L2)",
    "text": "Ridge Regression (L2)\nRidge regression adds the sum of squared coefficients to the loss function:\n\n\nCode\n# Prepare data for glmnet\nx_matrix &lt;- as.matrix(data[, -ncol(data)])\ny_vector &lt;- data$y\n\n# Fit ridge regression\nridge_fit &lt;- glmnet(x_matrix, y_vector, alpha = 0, \n                    lambda = exp(seq(-3, 5, length.out = 100)))\n\n# Cross-validation to find optimal lambda\ncv_ridge &lt;- cv.glmnet(x_matrix, y_vector, alpha = 0)\n\n# Plot cross-validation results\nplot(cv_ridge)"
  },
  {
    "objectID": "blog/posts/regularization/index.html#lasso-regression-l1",
    "href": "blog/posts/regularization/index.html#lasso-regression-l1",
    "title": "Regularization in statistical models",
    "section": "Lasso Regression (L1)",
    "text": "Lasso Regression (L1)\nLasso regression uses absolute value penalties and can perform variable selection:\n\n\nCode\n# Fit lasso regression\nlasso_fit &lt;- glmnet(x_matrix, y_vector, alpha = 1,\n                    lambda = exp(seq(-3, 5, length.out = 100)))\n\n# Cross-validation\ncv_lasso &lt;- cv.glmnet(x_matrix, y_vector, alpha = 1)\n\n# Plot solution path\nplot(lasso_fit, xvar = \"lambda\", label = TRUE)"
  },
  {
    "objectID": "blog/posts/regularization/index.html#elastic-net",
    "href": "blog/posts/regularization/index.html#elastic-net",
    "title": "Regularization in statistical models",
    "section": "Elastic Net",
    "text": "Elastic Net\nElastic Net combines both L1 and L2 penalties:\n\n\nCode\n# Fit elastic net with alpha = 0.5\nenet_fit &lt;- glmnet(x_matrix, y_vector, alpha = 0.5,\n                   lambda = exp(seq(-3, 5, length.out = 100)))\n\n# Cross-validation\ncv_enet &lt;- cv.glmnet(x_matrix, y_vector, alpha = 0.5)\n\n# Compare coefficients at optimal lambda\ncoef_comparison &lt;- data.frame(\n  true = true_beta,\n  ridge = as.vector(coef(cv_ridge, s = \"lambda.min\"))[-1],\n  lasso = as.vector(coef(cv_lasso, s = \"lambda.min\"))[-1],\n  elastic = as.vector(coef(cv_enet, s = \"lambda.min\"))[-1]\n)\n\n# Plot first 20 coefficients\nhead(coef_comparison, 20)\n\n\n   true        ridge      lasso       elastic\n1   1.5  0.168206623  1.4215046  1.3290087822\n2  -0.8 -0.136466379 -0.7070713 -0.7202401708\n3   1.2  0.159580168  1.1339067  1.0787191944\n4  -0.5 -0.036699418 -0.2745075 -0.2490259735\n5   0.9  0.074879809  0.8026272  0.7517043315\n6   0.0 -0.023896821  0.0000000  0.0000000000\n7   0.0 -0.009236366  0.0000000  0.0000000000\n8   0.0 -0.015842275  0.0000000  0.0000000000\n9   0.0 -0.104338887  0.0000000 -0.0146134033\n10  0.0 -0.011447505  0.0000000  0.0000000000\n11  0.0  0.044632265  0.0000000  0.0000000000\n12  0.0  0.005038114  0.0000000  0.0000000000\n13  0.0 -0.008216405  0.0000000  0.0000000000\n14  0.0  0.028400026  0.0000000  0.0001638061\n15  0.0  0.011236623  0.0000000  0.0000000000\n16  0.0  0.003305804  0.0000000  0.0000000000\n17  0.0  0.017521241  0.0000000  0.0000000000\n18  0.0  0.021065650  0.0000000  0.0000000000\n19  0.0 -0.005236160  0.0000000  0.0000000000\n20  0.0  0.039430620  0.0000000  0.0000000000"
  },
  {
    "objectID": "blog/posts/regularization/index.html#regularization-in-p-n-scenarios",
    "href": "blog/posts/regularization/index.html#regularization-in-p-n-scenarios",
    "title": "Regularization in statistical models",
    "section": "Regularization in p > n Scenarios",
    "text": "Regularization in p &gt; n Scenarios\nWhen the number of parameters (p) exceeds the number of observations (n), regularization becomes not just useful but essential. Here’s why:\n\nMathematical Necessity: Without regularization, the problem is ill-posed as there are infinitely many solutions that perfectly fit the training data.\nVariance Reduction: Regularization helps reduce the variance of parameter estimates, which is particularly important when we have limited data.\nFeature Selection: Lasso and Elastic Net can help identify the most important features, which is crucial when dealing with high-dimensional data.\n\nLet’s demonstrate this with a comparison of prediction errors:\n\n\nCode\n# Generate test data\nX_test &lt;- matrix(rnorm(n * p), nrow = n)\ny_test &lt;- X_test %*% true_beta + rnorm(n, sd = 0.5)\n\n# Calculate MSE for each method\nmse &lt;- data.frame(\n  Method = c(\"Ridge\", \"Lasso\", \"Elastic Net\"),\n  MSE = c(\n    mean((predict(cv_ridge, newx = X_test) - y_test)^2),\n    mean((predict(cv_lasso, newx = X_test) - y_test)^2),\n    mean((predict(cv_enet, newx = X_test) - y_test)^2)\n  )\n)\n\nprint(mse)\n\n\n       Method       MSE\n1       Ridge 6.3827969\n2       Lasso 0.4183443\n3 Elastic Net 0.5025287"
  },
  {
    "objectID": "blog/posts/regularization/index.html#best-practices-and-recommendations",
    "href": "blog/posts/regularization/index.html#best-practices-and-recommendations",
    "title": "Regularization in statistical models",
    "section": "Best Practices and Recommendations",
    "text": "Best Practices and Recommendations\nWhen working with p &gt; n scenarios:\n\nStart with Cross-Validation: Always use cross-validation to select the optimal regularization parameter (λ).\nConsider Multiple Methods: Try different regularization approaches as their performance can vary depending on the underlying data structure.\nScale Features: Standardize predictors before applying regularization to ensure fair penalization.\nMonitor Sparsity: If interpretability is important, prefer Lasso or Elastic Net as they can produce sparse solutions."
  },
  {
    "objectID": "blog/posts/regularization/index.html#conclusion",
    "href": "blog/posts/regularization/index.html#conclusion",
    "title": "Regularization in statistical models",
    "section": "Conclusion",
    "text": "Conclusion\nRegularization is particularly crucial in p &gt; n scenarios, where it helps: - Prevent perfect but meaningless fits to training data - Reduce model complexity - Identify important features - Improve prediction accuracy on new data\nThe choice between Ridge, Lasso, and Elastic Net depends on your specific needs regarding sparsity, prediction accuracy, and interpretability."
  },
  {
    "objectID": "blog/posts/regularization/index.html#references",
    "href": "blog/posts/regularization/index.html#references",
    "title": "Regularization in statistical models",
    "section": "References",
    "text": "References\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning.\nZou, H., & Hastie, T. (2005). Regularization and variable selection via the elastic net.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning."
  },
  {
    "objectID": "blog/posts/Ross-Macdonald/index.html",
    "href": "blog/posts/Ross-Macdonald/index.html",
    "title": "The Ross-Macdonald Framework: Foundational Mathematical Models in Vector-Borne Disease Epidemiology",
    "section": "",
    "text": "The mathematical modeling of infectious diseases has become an indispensable tool in modern epidemiology, guiding intervention strategies and policy decisions. While contemporary models exhibit increasing complexity, many fundamental principles trace back to the pioneering work of Sir Ronald Ross, who established the theoretical foundation for understanding vector-borne disease transmission dynamics at the turn of the 20th century.\nThis article examines Ross’s seminal contributions to mathematical epidemiology, the subsequent refinements by George Macdonald, and how these models continue to influence contemporary approaches to vector-borne disease control. We will focus on the mathematical formulations that undergird the Ross-Macdonald framework and their implications for disease control thresholds."
  },
  {
    "objectID": "blog/posts/Ross-Macdonald/index.html#introduction",
    "href": "blog/posts/Ross-Macdonald/index.html#introduction",
    "title": "The Ross-Macdonald Framework: Foundational Mathematical Models in Vector-Borne Disease Epidemiology",
    "section": "",
    "text": "The mathematical modeling of infectious diseases has become an indispensable tool in modern epidemiology, guiding intervention strategies and policy decisions. While contemporary models exhibit increasing complexity, many fundamental principles trace back to the pioneering work of Sir Ronald Ross, who established the theoretical foundation for understanding vector-borne disease transmission dynamics at the turn of the 20th century.\nThis article examines Ross’s seminal contributions to mathematical epidemiology, the subsequent refinements by George Macdonald, and how these models continue to influence contemporary approaches to vector-borne disease control. We will focus on the mathematical formulations that undergird the Ross-Macdonald framework and their implications for disease control thresholds."
  },
  {
    "objectID": "blog/posts/Ross-Macdonald/index.html#rosss-initial-mathematical-framework",
    "href": "blog/posts/Ross-Macdonald/index.html#rosss-initial-mathematical-framework",
    "title": "The Ross-Macdonald Framework: Foundational Mathematical Models in Vector-Borne Disease Epidemiology",
    "section": "2 Ross’s Initial Mathematical Framework",
    "text": "2 Ross’s Initial Mathematical Framework\nAfter establishing the mosquito-malaria parasite life cycle, for which he received the Nobel Prize in Medicine in 1902, Ronald Ross turned his attention to developing mathematical models to guide control strategies. His first models, published in 1908 in “Report on the Prevention of Malaria in Mauritius” (Ross 1908) and expanded in his 1911 book “The Prevention of Malaria” (Ross 1911), represented a paradigm shift in epidemiological thinking.\nRoss later formalized these mathematical principles in his landmark papers “Some a priori pathometric equations” (Ross 1915) and “An application of the theory of probabilities to the study of a priori pathometry” (Ross 1916), which laid the groundwork for the entire field of mathematical epidemiology.\nRoss’s initial model was remarkably straightforward yet profound. If we denote the proportion of infected humans as \\(p\\) and the proportion of infected mosquitoes as \\(P\\), his first model can be expressed as:\n\\[\\frac{dp}{dt} = abmP(1-p) - rp\\] \\[\\frac{dP}{dt} = acp(1-P) - \\mu P\\]\nWhere: - \\(a\\) represents the mosquito biting rate - \\(b\\) is the probability of parasite transmission from mosquito to human per bite - \\(c\\) is the probability of parasite transmission from human to mosquito per bite - \\(m\\) is the mosquito density per human - \\(r\\) is the human recovery rate - \\(\\mu\\) is the mosquito mortality rate"
  },
  {
    "objectID": "blog/posts/Ross-Macdonald/index.html#the-basic-reproduction-number",
    "href": "blog/posts/Ross-Macdonald/index.html#the-basic-reproduction-number",
    "title": "The Ross-Macdonald Framework: Foundational Mathematical Models in Vector-Borne Disease Epidemiology",
    "section": "3 The Basic Reproduction Number",
    "text": "3 The Basic Reproduction Number\nRoss’s most significant contribution was demonstrating that malaria transmission could be interrupted without eliminating the entire vector population (Smith et al. 2007). This led to the concept of a critical threshold, later formalized as the basic reproduction number \\(R_0\\).\nFor Ross’s model, \\(R_0\\) can be derived as:\n\\[R_0 = \\frac{a^2bcm}{\\mu r}\\]\nThis quantity represents the expected number of secondary infections arising from a single infected individual in a completely susceptible population. When \\(R_0 &lt; 1\\), the disease will eventually die out; when \\(R_0 &gt; 1\\), the disease can persist in the population (Smith et al. 2012).\n\n\nCode\n# Function to calculate R0 from model parameters\ncalculate_R0 &lt;- function(a, b, c, m, mu, r) {\n  R0 &lt;- (a^2 * b * c * m) / (mu * r)\n  return(R0)\n}\n\n# Example parameter values\na &lt;- 0.3    # Biting rate (bites per day)\nb &lt;- 0.5    # Transmission probability: mosquito to human\nc &lt;- 0.5    # Transmission probability: human to mosquito\nm &lt;- 10     # Mosquito density per human\nmu &lt;- 0.1   # Mosquito mortality rate\nr &lt;- 0.01   # Human recovery rate\n\nR0 &lt;- calculate_R0(a, b, c, m, mu, r)\ncat(\"With these parameters, R0 =\", round(R0, 2))\n\n\nWith these parameters, R0 = 225"
  },
  {
    "objectID": "blog/posts/Ross-Macdonald/index.html#the-macdonald-refinements",
    "href": "blog/posts/Ross-Macdonald/index.html#the-macdonald-refinements",
    "title": "The Ross-Macdonald Framework: Foundational Mathematical Models in Vector-Borne Disease Epidemiology",
    "section": "4 The Macdonald Refinements",
    "text": "4 The Macdonald Refinements\nGeorge Macdonald expanded Ross’s work in the 1950s, incorporating additional biological complexity (Macdonald 1952). His key modifications included:\n\nAccounting for the latent period in the mosquito (\\(n\\) days), during which the parasite develops\nRecognition of mosquito survival as exponential, with daily survival probability \\(p\\)\n\nMacdonald’s version of the reproduction number became:\n\\[R_0 = \\frac{ma^2bcp^n}{-r\\ln(p)}\\]\nWhere \\(-\\ln(p)\\) replaces \\(\\mu\\) as the mosquito mortality rate.\nA crucial insight from Macdonald was that malaria transmission is particularly sensitive to adult mosquito longevity, as expressed by the exponent \\(n\\) in the equation. This led to the strategic emphasis on adult mosquito control using residual insecticides (Garrett-Jones 1964).\n\n\nCode\n# Macdonald's R0 calculation\ncalculate_macdonald_R0 &lt;- function(m, a, b, c, p, n, r) {\n  R0 &lt;- (m * a^2 * b * c * p^n) / (-r * log(p))\n  return(R0)\n}\n\n# Example parameter values\nm &lt;- 10     # Mosquito density per human\na &lt;- 0.3    # Biting rate (bites per day)\nb &lt;- 0.5    # Transmission probability: mosquito to human\nc &lt;- 0.5    # Transmission probability: human to mosquito\np &lt;- 0.9    # Daily mosquito survival probability\nn &lt;- 10     # Extrinsic incubation period (days)\nr &lt;- 0.01   # Human recovery rate\n\nmacdonald_R0 &lt;- calculate_macdonald_R0(m, a, b, c, p, n, r)\ncat(\"With Macdonald's formulation, R0 =\", round(macdonald_R0, 2))\n\n\nWith Macdonald's formulation, R0 = 74.46"
  },
  {
    "objectID": "blog/posts/Ross-Macdonald/index.html#equilibrium-analysis-and-threshold-conditions",
    "href": "blog/posts/Ross-Macdonald/index.html#equilibrium-analysis-and-threshold-conditions",
    "title": "The Ross-Macdonald Framework: Foundational Mathematical Models in Vector-Borne Disease Epidemiology",
    "section": "5 Equilibrium Analysis and Threshold Conditions",
    "text": "5 Equilibrium Analysis and Threshold Conditions\nRoss’s mathematical analysis demonstrated that the system has two equilibrium points (Bailey 1982):\n\nDisease-free equilibrium: \\((p, P) = (0, 0)\\)\nEndemic equilibrium: \\((p, P) = (p^*, P^*)\\) where:\n\n\\[p^* = \\frac{abmP^* - rp^*}{abmP^*}\\] \\[P^* = \\frac{acp^* - \\mu P^*}{acp^*}\\]\nThe stability of these equilibria depends on the value of \\(R_0\\). When \\(R_0 &lt; 1\\), the disease-free equilibrium is stable and the endemic equilibrium doesn’t exist. When \\(R_0 &gt; 1\\), the disease-free equilibrium is unstable and the endemic equilibrium is stable (Dietz, Molineaux, and Thomas 1974).\n\n\nCode\n# Simple SIS model with vector-host interaction\nross_model &lt;- function(time, state, parameters) {\n  with(as.list(c(state, parameters)), {\n    dp_dt &lt;- a * b * m * P * (1 - p) - r * p\n    dP_dt &lt;- a * c * p * (1 - P) - mu * P\n    \n    return(list(c(dp_dt, dP_dt)))\n  })\n}\n\n# Parameter values\nparams &lt;- c(\n  a = 0.3,    # Biting rate\n  b = 0.5,    # Transmission probability: mosquito to human\n  c = 0.5,    # Transmission probability: human to mosquito\n  m = 10,     # Mosquito density per human\n  r = 0.01,   # Human recovery rate\n  mu = 0.1    # Mosquito mortality rate\n)\n\n# Initial conditions\ninit_state &lt;- c(p = 0.01, P = 0.005)\n\n# Time points\ntimes &lt;- seq(0, 365, by = 1)\n\n# Solve the model\nsolution &lt;- ode(y = init_state, times = times, func = ross_model, parms = params)\n\n# Convert to data frame\nresults &lt;- as.data.frame(solution)\nresults_long &lt;- pivot_longer(results, cols = c(p, P), \n                           names_to = \"Compartment\", \n                           values_to = \"Proportion\")\n\n# Plot\nggplot(results_long, aes(x = time, y = Proportion, color = Compartment)) +\n  geom_line(size = 1) +\n  scale_color_manual(values = c(\"p\" = \"darkred\", \"P\" = \"darkblue\"),\n                    labels = c(\"p\" = \"Infected Humans\", \"P\" = \"Infected Mosquitoes\")) +\n  labs(title = \"Ross Model Dynamics\",\n       x = \"Time (days)\",\n       y = \"Proportion Infected\") +\n  theme_minimal()"
  },
  {
    "objectID": "blog/posts/Ross-Macdonald/index.html#vector-control-implications",
    "href": "blog/posts/Ross-Macdonald/index.html#vector-control-implications",
    "title": "The Ross-Macdonald Framework: Foundational Mathematical Models in Vector-Borne Disease Epidemiology",
    "section": "6 Vector Control Implications",
    "text": "6 Vector Control Implications\nThe Ross-Macdonald framework provides clear guidance on vector control strategies. From the expression for \\(R_0\\), we can derive critical thresholds for various control parameters (Smith et al. 2014). For example, the critical mosquito density \\(m_c\\) below which transmission cannot be sustained is:\n\\[m_c = \\frac{\\mu r}{a^2bc}\\]\nSimilarly, the required efficacy of insecticides to achieve control can be calculated. If we denote the proportional reduction in mosquito density as \\(\\Delta m\\), then for disease elimination:\n\\[\\Delta m &gt; 1 - \\frac{1}{R_0}\\]\nThis concept is directly related to the herd immunity threshold in direct transmission diseases (Fine 1993).\n\n\nCode\n# Calculate critical mosquito density\ncalculate_critical_m &lt;- function(a, b, c, mu, r) {\n  m_c &lt;- (mu * r) / (a^2 * b * c)\n  return(m_c)\n}\n\nm_c &lt;- calculate_critical_m(a, b, c, mu, r)\ncat(\"Critical mosquito density (m_c) =\", round(m_c, 2), \"mosquitoes per human\\n\")\n\n\nCritical mosquito density (m_c) = 0.04 mosquitoes per human\n\n\nCode\n# Calculate required control effectiveness\nrequired_control &lt;- 1 - (1/R0)\ncat(\"Required proportional reduction in mosquito density =\", round(required_control, 4) * 100, \"%\")\n\n\nRequired proportional reduction in mosquito density = 99.56 %"
  },
  {
    "objectID": "blog/posts/Ross-Macdonald/index.html#sensitivity-analysis",
    "href": "blog/posts/Ross-Macdonald/index.html#sensitivity-analysis",
    "title": "The Ross-Macdonald Framework: Foundational Mathematical Models in Vector-Borne Disease Epidemiology",
    "section": "7 Sensitivity Analysis",
    "text": "7 Sensitivity Analysis\nA key insight from the Ross-Macdonald framework is the differential sensitivity of \\(R_0\\) to various parameters. This can be formally analyzed through partial derivatives or elasticity analysis (Brady et al. 2016).\nFor example, the elasticity of \\(R_0\\) with respect to parameter \\(\\theta\\) is:\n\\[E_{\\theta} = \\frac{\\theta}{R_0} \\frac{\\partial R_0}{\\partial \\theta}\\]\nFor the mosquito mortality rate \\(\\mu\\), this becomes:\n\\[E_{\\mu} = -1\\]\nThis indicates that a 1% increase in mosquito mortality leads to approximately a 1% decrease in \\(R_0\\).\nFor the parameter \\(a\\) (biting rate), the elasticity is:\n\\[E_a = 2\\]\nThis reflects Macdonald’s important observation that interventions targeting mosquito biting behavior have a squared effect on transmission, since \\(a\\) appears as \\(a^2\\) in the \\(R_0\\) expression (Smith et al. 2012).\n\n\nCode\n# Function to calculate elasticity for different parameters\ncalculate_elasticities &lt;- function(a, b, c, m, mu, r) {\n  E_a &lt;- 2                # Elasticity for biting rate\n  E_b &lt;- 1                # Elasticity for transmission probability (mosquito to human)\n  E_c &lt;- 1                # Elasticity for transmission probability (human to mosquito)\n  E_m &lt;- 1                # Elasticity for mosquito density\n  E_mu &lt;- -1              # Elasticity for mosquito mortality rate\n  E_r &lt;- -1               # Elasticity for human recovery rate\n  \n  elasticities &lt;- data.frame(\n    Parameter = c(\"Biting rate (a)\", \"Transmission: mosquito→human (b)\", \n                  \"Transmission: human→mosquito (c)\", \"Mosquito density (m)\",\n                  \"Mosquito mortality (μ)\", \"Human recovery rate (r)\"),\n    Elasticity = c(E_a, E_b, E_c, E_m, E_mu, E_r)\n  )\n  \n  return(elasticities)\n}\n\nelasticities &lt;- calculate_elasticities(a, b, c, m, mu, r)\nelasticities\n\n\n                         Parameter Elasticity\n1                  Biting rate (a)          2\n2 Transmission: mosquito→human (b)          1\n3 Transmission: human→mosquito (c)          1\n4             Mosquito density (m)          1\n5           Mosquito mortality (μ)         -1\n6          Human recovery rate (r)         -1"
  },
  {
    "objectID": "blog/posts/Ross-Macdonald/index.html#contemporary-extensions-and-applications",
    "href": "blog/posts/Ross-Macdonald/index.html#contemporary-extensions-and-applications",
    "title": "The Ross-Macdonald Framework: Foundational Mathematical Models in Vector-Borne Disease Epidemiology",
    "section": "8 Contemporary Extensions and Applications",
    "text": "8 Contemporary Extensions and Applications\nWhile the Ross-Macdonald framework remains foundational, modern approaches have extended these models in several directions (Reiner et al. 2013; Mandal, Sarkar, and Sinha 2011):\n\nSpatial heterogeneity: Incorporating spatial structure and human mobility\nStochasticity: Accounting for random events, particularly important near elimination\nVector bionomics: More detailed entomological parameters\nImmunity dynamics: Accounting for acquired immunity in human populations\nDrug resistance: Modeling the spread of antimalarial resistance\n\nThe mathematical expression for \\(R_0\\) in these more complex models becomes correspondingly more intricate. For example, in a spatial metapopulation model with \\(n\\) patches, \\(R_0\\) can be expressed as the dominant eigenvalue of the next-generation matrix:\n\\[R_0 = \\rho(FV^{-1})\\]\nWhere \\(F\\) represents new infections and \\(V\\) represents transitions between compartments."
  },
  {
    "objectID": "blog/posts/Ross-Macdonald/index.html#practical-applications-in-disease-control-programs",
    "href": "blog/posts/Ross-Macdonald/index.html#practical-applications-in-disease-control-programs",
    "title": "The Ross-Macdonald Framework: Foundational Mathematical Models in Vector-Borne Disease Epidemiology",
    "section": "9 Practical Applications in Disease Control Programs",
    "text": "9 Practical Applications in Disease Control Programs\nThe threshold concept derived from Ross’s work remains central to modern malaria control programs (Smith et al. 2007). The World Health Organization’s Global Technical Strategy for Malaria 2016-2030 implicitly relies on these mathematical principles when setting targets for vector control coverage and intervention effectiveness.\nKey applications include:\n\nIndoor residual spraying (IRS): Targeting the adult mosquito longevity parameter\nInsecticide-treated nets (ITNs): Affecting both mosquito mortality and biting rate\nLarval source management: Reducing the mosquito density parameter \\(m\\)\n\nMathematical models help quantify the coverage levels required for these interventions to bring \\(R_0\\) below 1 (Brady et al. 2016)."
  },
  {
    "objectID": "blog/posts/Ross-Macdonald/index.html#discussion",
    "href": "blog/posts/Ross-Macdonald/index.html#discussion",
    "title": "The Ross-Macdonald Framework: Foundational Mathematical Models in Vector-Borne Disease Epidemiology",
    "section": "10 Discussion",
    "text": "10 Discussion\nRoss’s mathematical insights have withstood the test of time remarkably well. The threshold concept, formalized as \\(R_0\\), remains central to infectious disease epidemiology across all pathogens, not just vector-borne diseases.\nModern computational capabilities allow for increasingly complex simulations, but the core mathematical principles established by Ross (Ross 1911) and refined by Macdonald (Macdonald 1952) continue to guide our understanding of transmission dynamics. The elegance of these models lies in their ability to capture essential dynamics with relatively simple formulations.\nAs we face challenges like climate change, insecticide resistance, and drug resistance, these mathematical frameworks are being adapted to address increasingly complex scenarios (Smith et al. 2014). Yet the threshold principle - that disease transmission can be interrupted without eliminating every vector - remains a profound insight that continues to guide public health strategies worldwide."
  },
  {
    "objectID": "blog/posts/Ross-Macdonald/index.html#references",
    "href": "blog/posts/Ross-Macdonald/index.html#references",
    "title": "The Ross-Macdonald Framework: Foundational Mathematical Models in Vector-Borne Disease Epidemiology",
    "section": "11 References",
    "text": "11 References\n\nRoss, R. (1908). Report on the Prevention of Malaria in Mauritius. London: Waterlow & Sons Limited.\nRoss, R. (1911). The Prevention of Malaria. London: John Murray.\nMacdonald, G. (1952). The analysis of equilibrium in malaria. Tropical Diseases Bulletin, 49(9), 813-829.\nSmith, D. L., Battle, K. E., Hay, S. I., Barker, C. M., Scott, T. W., & McKenzie, F. E. (2012). Ross, Macdonald, and a theory for the dynamics and control of mosquito-transmitted pathogens. PLOS Pathogens, 8(4), e1002588.\nSmith, D. L., Perkins, T. A., Reiner, R. C., Barker, C. M., Niu, T., Chaves, L. F., … & Scott, T. W. (2014). Recasting the theory of mosquito-borne pathogen transmission dynamics and control. Transactions of the Royal Society of Tropical Medicine and Hygiene, 108(4), 185-197.\nReiner, R. C., Perkins, T. A., Barker, C. M., Niu, T., Chaves, L. F., Ellis, A. M., … & Smith, D. L. (2013). A systematic review of mathematical models of mosquito-borne pathogen transmission: 1970–2010. Journal of the Royal Society Interface, 10(81), 20120921.\nBrady, O. J., Godfray, H. C. J., Tatem, A. J., Gething, P. W., Cohen, J. M., McKenzie, F. E., … & Smith, D. L. (2016). Vectorial capacity and vector control: reconsidering sensitivity to parameters for malaria elimination. Transactions of the Royal Society of Tropical Medicine and Hygiene, 110(2), 107-117.\nMandal, S., Sarkar, R. R., & Sinha, S. (2011). Mathematical models of malaria-a review. Malaria Journal, 10(1), 1-19."
  },
  {
    "objectID": "blog/posts/Daniel_Bernoulli/index.html",
    "href": "blog/posts/Daniel_Bernoulli/index.html",
    "title": "The Birth of Epidemic Modeling: Daniel Bernoulli and the Smallpox Inoculation Controversy",
    "section": "",
    "text": "Mathematical modeling of infectious diseases is now a cornerstone of public health decision-making. During COVID-19, we all became familiar with concepts like “flattening the curve” and reproduction numbers. But did you know that the first mathematical model for infectious disease control was developed nearly 300 years ago? In 1760, a Swiss mathematician named Daniel Bernoulli created a revolutionary approach to evaluate a controversial medical procedure: smallpox inoculation (Bernoulli 1766). His work represents the birth of what we now call mathematical epidemiology (Dietz and Heesterbeek 2002)."
  },
  {
    "objectID": "blog/posts/Daniel_Bernoulli/index.html#introduction",
    "href": "blog/posts/Daniel_Bernoulli/index.html#introduction",
    "title": "The Birth of Epidemic Modeling: Daniel Bernoulli and the Smallpox Inoculation Controversy",
    "section": "",
    "text": "Mathematical modeling of infectious diseases is now a cornerstone of public health decision-making. During COVID-19, we all became familiar with concepts like “flattening the curve” and reproduction numbers. But did you know that the first mathematical model for infectious disease control was developed nearly 300 years ago? In 1760, a Swiss mathematician named Daniel Bernoulli created a revolutionary approach to evaluate a controversial medical procedure: smallpox inoculation (Bernoulli 1766). His work represents the birth of what we now call mathematical epidemiology (Dietz and Heesterbeek 2002)."
  },
  {
    "objectID": "blog/posts/Daniel_Bernoulli/index.html#who-was-daniel-bernoulli",
    "href": "blog/posts/Daniel_Bernoulli/index.html#who-was-daniel-bernoulli",
    "title": "The Birth of Epidemic Modeling: Daniel Bernoulli and the Smallpox Inoculation Controversy",
    "section": "2 Who Was Daniel Bernoulli?",
    "text": "2 Who Was Daniel Bernoulli?\nDaniel Bernoulli (1700-1782) came from a family of brilliant mathematicians. While his relatives focused on pure mathematics, Daniel pursued medicine alongside his mathematical studies (Hald 2003). This unique combination placed him perfectly to bridge these two worlds when a major public health controversy erupted in 18th century Europe.\n\n\n\nDaniel Bernoulli"
  },
  {
    "objectID": "blog/posts/Daniel_Bernoulli/index.html#the-smallpox-crisis",
    "href": "blog/posts/Daniel_Bernoulli/index.html#the-smallpox-crisis",
    "title": "The Birth of Epidemic Modeling: Daniel Bernoulli and the Smallpox Inoculation Controversy",
    "section": "3 The Smallpox Crisis",
    "text": "3 The Smallpox Crisis\nToday, we celebrate smallpox as the first human disease to be completely eradicated through vaccination. But in Bernoulli’s time, smallpox was a devastating illness that:\n\nKilled approximately 400,000 Europeans annually\nCaused death in 1 out of every 7-10 infected individuals\nLeft survivors permanently scarred and sometimes blind\nAffected nearly everyone, with most contracting it during childhood"
  },
  {
    "objectID": "blog/posts/Daniel_Bernoulli/index.html#the-controversial-solution-variolation",
    "href": "blog/posts/Daniel_Bernoulli/index.html#the-controversial-solution-variolation",
    "title": "The Birth of Epidemic Modeling: Daniel Bernoulli and the Smallpox Inoculation Controversy",
    "section": "4 The Controversial Solution: Variolation",
    "text": "4 The Controversial Solution: Variolation\nIn the early 18th century, a precursor to vaccination called “variolation” (or inoculation) was introduced to Europe (Blower and Bernoulli 2004). This procedure involved:\n\nTaking a small amount of pus from a person with a mild case of smallpox\nInserting it under the skin of a healthy person\nInducing a usually milder form of the disease\nConferring lifelong immunity if the person survived\n\nThis technique reduced the mortality rate from smallpox from about 15% to approximately 1-2%. However, it was extremely controversial because:\n\nIt deliberately infected healthy people with a deadly disease\nSome people died from the procedure itself\nInoculated individuals could spread smallpox to others\nReligious and ethical objections were raised against “interfering with God’s will”"
  },
  {
    "objectID": "blog/posts/Daniel_Bernoulli/index.html#bernoullis-breakthrough-the-first-disease-model",
    "href": "blog/posts/Daniel_Bernoulli/index.html#bernoullis-breakthrough-the-first-disease-model",
    "title": "The Birth of Epidemic Modeling: Daniel Bernoulli and the Smallpox Inoculation Controversy",
    "section": "5 Bernoulli’s Breakthrough: The First Disease Model",
    "text": "5 Bernoulli’s Breakthrough: The First Disease Model\nBernoulli recognized that this controversy needed more than opinions—it needed data and mathematical analysis. In 1760, he presented his work “Essai d’une nouvelle analyse de la mortalité causée par la petite vérole” (Essay on a new analysis of the mortality caused by smallpox) to the Royal Academy of Sciences in Paris (Bernoulli 1766).\nHis approach was revolutionary because:\n\nIt was the first mathematical model specifically designed to evaluate a public health intervention\nIt separated the risk of dying from smallpox from other causes of death\nIt calculated potential years of life gained through inoculation\nIt used real demographic data from the city of London"
  },
  {
    "objectID": "blog/posts/Daniel_Bernoulli/index.html#the-model-explained-simply",
    "href": "blog/posts/Daniel_Bernoulli/index.html#the-model-explained-simply",
    "title": "The Birth of Epidemic Modeling: Daniel Bernoulli and the Smallpox Inoculation Controversy",
    "section": "6 The Model Explained Simply",
    "text": "6 The Model Explained Simply\nBernoulli’s approach can be understood through a simple analogy:\nImagine 1,000 children born in the same year. Without inoculation, these children face two risks as they age: - The risk of contracting and possibly dying from smallpox - The risk of dying from any other cause\nBernoulli used available data to estimate: - How many people would get smallpox each year - How many would die from it - How many would die from other causes\nHe then compared this natural scenario with one where everyone was inoculated against smallpox. Although inoculation carried its own small risk of death, Bernoulli’s calculations showed that widespread inoculation would:\n\nIncrease life expectancy by about 3 years (significant for that time period)\nSave thousands of lives across a population"
  },
  {
    "objectID": "blog/posts/Daniel_Bernoulli/index.html#mathematical-innovation-the-life-table-approach",
    "href": "blog/posts/Daniel_Bernoulli/index.html#mathematical-innovation-the-life-table-approach",
    "title": "The Birth of Epidemic Modeling: Daniel Bernoulli and the Smallpox Inoculation Controversy",
    "section": "7 Mathematical Innovation: The Life Table Approach",
    "text": "7 Mathematical Innovation: The Life Table Approach\nBernoulli’s work was mathematically innovative because he:\n\nCreated what we now call a “dynamic life table” - tracking population changes over time\nSeparated specific causes of death (what epidemiologists now call “competing risks”)\nIntroduced differential equations to model population changes due to disease (Dietz and Heesterbeek 2002)\n\nIn modern mathematical notation, his core equation looked something like:\n\\[\\frac{dp(a)}{da} = -\\mu(a)p(a) - \\lambda p_s(a)\\]\nWhere: - \\(p(a)\\) represents the proportion of people surviving to age \\(a\\) - \\(\\mu(a)\\) is the force of mortality from causes other than smallpox - \\(\\lambda\\) is the rate of smallpox infection - \\(p_s(a)\\) is the proportion of susceptible people at age \\(a\\)\nWhile this may look intimidating, the concept is straightforward: Bernoulli tracked how many people would die from different causes under different scenarios (J. A. P. Heesterbeek and Dietz 1996)."
  },
  {
    "objectID": "blog/posts/Daniel_Bernoulli/index.html#impact-and-legacy",
    "href": "blog/posts/Daniel_Bernoulli/index.html#impact-and-legacy",
    "title": "The Birth of Epidemic Modeling: Daniel Bernoulli and the Smallpox Inoculation Controversy",
    "section": "8 Impact and Legacy",
    "text": "8 Impact and Legacy\nBernoulli’s mathematical analysis had profound effects:\n\nQuantified benefits: He showed that inoculation would increase life expectancy by approximately 3 years - remarkable in an era when life expectancy was around 30 years (Blower and Bernoulli 2004).\nRisk comparison: He demonstrated that the 1-2% risk of death from inoculation was far smaller than the lifetime risk of dying from natural smallpox infection.\nPolicy influence: His work helped shift public opinion and policy toward accepting inoculation as a worthwhile public health measure.\nMethodological innovation: He established mathematical modeling as a valid approach to public health questions (Hethcote 2000)."
  },
  {
    "objectID": "blog/posts/Daniel_Bernoulli/index.html#the-birth-of-mathematical-epidemiology",
    "href": "blog/posts/Daniel_Bernoulli/index.html#the-birth-of-mathematical-epidemiology",
    "title": "The Birth of Epidemic Modeling: Daniel Bernoulli and the Smallpox Inoculation Controversy",
    "section": "9 The Birth of Mathematical Epidemiology",
    "text": "9 The Birth of Mathematical Epidemiology\nBernoulli’s approach contained the seeds of modern infectious disease modeling:\n\nIt used mathematics to predict disease outcomes\nIt quantified the impact of interventions\nIt separated competing causes of death\nIt estimated population-level effects from individual risks\n\nThe field that Bernoulli pioneered would later be expanded by other mathematical innovators like Ronald Ross (Ross 1908, 1911) and the team of Kermack and McKendrick (Kermack and McKendrick 1927), eventually leading to the sophisticated models we use today (Anderson and May 1991)."
  },
  {
    "objectID": "blog/posts/Daniel_Bernoulli/index.html#modern-applications-of-bernoullis-approach",
    "href": "blog/posts/Daniel_Bernoulli/index.html#modern-applications-of-bernoullis-approach",
    "title": "The Birth of Epidemic Modeling: Daniel Bernoulli and the Smallpox Inoculation Controversy",
    "section": "10 Modern Applications of Bernoulli’s Approach",
    "text": "10 Modern Applications of Bernoulli’s Approach\nThe principles Bernoulli established are still used to answer questions like:\n\nHow many lives will a new vaccine save? (Ferguson et al. 2006)\nIs a screening program worth its costs?\nWhich age groups should receive priority for limited medical resources?\nWhat is the optimal strategy for controlling an epidemic? (Riley et al. 2003)\n\nModern epidemic models have incorporated additional mathematical sophistication, particularly around the concept of the basic reproduction number (R₀) and the construction of next-generation matrices (Diekmann, Heesterbeek, and Roberts 2010; Wallinga and Lipsitch 2007), but they still follow Bernoulli’s fundamental approach of using mathematics to guide public health decisions."
  },
  {
    "objectID": "blog/posts/Daniel_Bernoulli/index.html#conclusion",
    "href": "blog/posts/Daniel_Bernoulli/index.html#conclusion",
    "title": "The Birth of Epidemic Modeling: Daniel Bernoulli and the Smallpox Inoculation Controversy",
    "section": "11 Conclusion",
    "text": "11 Conclusion\nDaniel Bernoulli’s 1760 smallpox model represents a pivotal moment in the history of public health (Bernoulli 1766). By applying mathematical reasoning to a contentious medical issue, he demonstrated that data and analysis could guide decisions more effectively than opinions alone.\nAs we continue to face infectious disease challenges in the modern world, from emerging pathogens to vaccine hesitancy, Bernoulli’s approach remains relevant (Keeling and Rohani 2008). His work reminds us that mathematical modeling is not just an academic exercise but a powerful tool for saving lives and informing policy.\nThe next time you hear about disease models predicting the course of an epidemic or evaluating the impact of interventions, remember that this approach began with a mathematician applying his skills to a pressing public health problem nearly three centuries ago (H. Heesterbeek and Roberts 2015)."
  },
  {
    "objectID": "blog/posts/model_and_reality/index.html",
    "href": "blog/posts/model_and_reality/index.html",
    "title": "The Model and Reality: Bridging Mathematical Frameworks with Real-World Infectious Disease Dynamics",
    "section": "",
    "text": "As infectious disease modelers, we face a fundamental epistemological question: How can abstract mathematical constructs help us understand and predict the complex dynamics of disease transmission in the real world? This question lies at the intersection of epidemiology, mathematics, and philosophy of science.\nDrawing from the Stanford Encyclopedia of Philosophy’s entry on scientific models (Frigg and Hartmann 2020), I’d like to explore how we connect our mathematical representations to reality, particularly in the context of infectious disease modeling."
  },
  {
    "objectID": "blog/posts/model_and_reality/index.html#introduction",
    "href": "blog/posts/model_and_reality/index.html#introduction",
    "title": "The Model and Reality: Bridging Mathematical Frameworks with Real-World Infectious Disease Dynamics",
    "section": "",
    "text": "As infectious disease modelers, we face a fundamental epistemological question: How can abstract mathematical constructs help us understand and predict the complex dynamics of disease transmission in the real world? This question lies at the intersection of epidemiology, mathematics, and philosophy of science.\nDrawing from the Stanford Encyclopedia of Philosophy’s entry on scientific models (Frigg and Hartmann 2020), I’d like to explore how we connect our mathematical representations to reality, particularly in the context of infectious disease modeling."
  },
  {
    "objectID": "blog/posts/model_and_reality/index.html#surrogate-reasoning-how-models-help-us-understand-reality",
    "href": "blog/posts/model_and_reality/index.html#surrogate-reasoning-how-models-help-us-understand-reality",
    "title": "The Model and Reality: Bridging Mathematical Frameworks with Real-World Infectious Disease Dynamics",
    "section": "2 Surrogate Reasoning: How Models Help Us Understand Reality",
    "text": "2 Surrogate Reasoning: How Models Help Us Understand Reality\nPhilosophers describe our relationship with models as a process of “surrogate reasoning” or “model-based reasoning.” Through building, manipulating, applying, and evaluating models, we can make inferences about reality without directly experimenting on it—an especially crucial approach when dealing with infectious diseases.\nThis reasoning process can be divided into two phases:\n\nUnderstanding the model itself\nTranslating that understanding to the real-world target system\n\nWhen building a model, we learn how different components interact mathematically. For instance, in an SIR model, we define relationships between susceptible, infected, and recovered populations (Anderson and May 1991). Through manipulation—changing parameters, running simulations, and analyzing outputs—we develop intuition about the system’s behavior under different conditions.\nBut the critical challenge remains: How do we translate knowledge gained from manipulating abstract equations to meaningful insights about actual disease transmission?"
  },
  {
    "objectID": "blog/posts/model_and_reality/index.html#the-representation-problem",
    "href": "blog/posts/model_and_reality/index.html#the-representation-problem",
    "title": "The Model and Reality: Bridging Mathematical Frameworks with Real-World Infectious Disease Dynamics",
    "section": "3 The Representation Problem",
    "text": "3 The Representation Problem\nFor a model to be useful, it must represent relevant aspects of reality. If we’re interested in how vaccination might reduce infection rates, our model must incorporate mechanisms that capture both vaccination dynamics and disease transmission accurately.\nYet how can we be sure our mathematical representations adequately reflect reality? This is where empirical validation becomes essential. We compare model predictions against observed data—testing whether our models actually predict what happens in outbreaks. When discrepancies arise, we refine our models, creating an iterative cycle of improvement.\n\n\nWarning: package 'DiagrammeR' was built under R version 4.3.3\n\n\n\n\nSimple visualization of the model validation cycle"
  },
  {
    "objectID": "blog/posts/model_and_reality/index.html#models-as-tools-for-knowledge-rather-than-perfect-representations",
    "href": "blog/posts/model_and_reality/index.html#models-as-tools-for-knowledge-rather-than-perfect-representations",
    "title": "The Model and Reality: Bridging Mathematical Frameworks with Real-World Infectious Disease Dynamics",
    "section": "4 Models as Tools for Knowledge Rather Than Perfect Representations",
    "text": "4 Models as Tools for Knowledge Rather Than Perfect Representations\nIt’s important to recognize that all models are simplifications. As statistician George Box famously noted, “All models are wrong, but some are useful” (Box 1987). The value of a model isn’t in its perfect replication of reality but in its ability to provide useful insights despite its limitations.\nIn infectious disease modeling, we often make simplifying assumptions—homogeneous mixing of populations, identical susceptibility across age groups, or simplified immune responses. These assumptions make the mathematics tractable but create distance from biological reality."
  },
  {
    "objectID": "blog/posts/model_and_reality/index.html#case-study-covid-19-modeling",
    "href": "blog/posts/model_and_reality/index.html#case-study-covid-19-modeling",
    "title": "The Model and Reality: Bridging Mathematical Frameworks with Real-World Infectious Disease Dynamics",
    "section": "5 Case Study: COVID-19 Modeling",
    "text": "5 Case Study: COVID-19 Modeling\nThe COVID-19 pandemic provided a dramatic illustration of both the power and limitations of epidemiological modeling. Early models helped forecast hospital capacity needs and evaluate potential intervention strategies (Adam 2020; Ferguson et al. 2020). However, they also demonstrated how sensitive predictions can be to underlying assumptions about transmission rates, asymptomatic spread, and behavioral responses (Ioannidis, Cripps, and Tanner 2020).\nWhen models failed to accurately predict outcomes, this wasn’t necessarily a failure of modeling itself but often reflected limitations in our understanding of the virus or changes in human behavior that models couldn’t anticipate (Holmdahl and Buckee 2020). Each discrepancy became an opportunity to refine both our models and our understanding of disease dynamics.\n\n\n\n\n\nExample SIR model simulation showing potential COVID-19 scenarios with different intervention strategies"
  },
  {
    "objectID": "blog/posts/model_and_reality/index.html#bridging-the-gap-model-validation-and-refinement",
    "href": "blog/posts/model_and_reality/index.html#bridging-the-gap-model-validation-and-refinement",
    "title": "The Model and Reality: Bridging Mathematical Frameworks with Real-World Infectious Disease Dynamics",
    "section": "6 Bridging the Gap: Model Validation and Refinement",
    "text": "6 Bridging the Gap: Model Validation and Refinement\nHow can we strengthen the connection between our models and reality? Several approaches are crucial:\n\nEmpirical validation: Continuously comparing model outputs with real-world data\nSensitivity analysis: Understanding how changes in parameters affect outcomes\nEnsemble modeling: Using multiple modeling approaches to gain more robust insights\nIncorporating heterogeneity: Adding complexity to capture more realistic population structures\nInterdisciplinary collaboration: Working with behavioral scientists, immunologists, and other experts to better represent complex dynamics"
  },
  {
    "objectID": "blog/posts/model_and_reality/index.html#conclusion",
    "href": "blog/posts/model_and_reality/index.html#conclusion",
    "title": "The Model and Reality: Bridging Mathematical Frameworks with Real-World Infectious Disease Dynamics",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nAs infectious disease modelers, we navigate a fascinating epistemological landscape. Our models serve as cognitive tools that allow us to explore complex dynamics in ways that would be impossible through direct observation alone. They help us generate hypotheses, evaluate potential interventions, and develop intuition about non-linear systems.\nWhile the gap between model and reality can never be completely closed, the iterative process of building, testing, and refining models brings us progressively closer to useful representations of disease transmission. The philosophical challenge of connecting abstract mathematical structures to biological reality remains at the heart of our discipline—a challenge that makes epidemiological modeling both intellectually stimulating and practically valuable for public health."
  },
  {
    "objectID": "blog/posts/model_and_reality/index.html#references",
    "href": "blog/posts/model_and_reality/index.html#references",
    "title": "The Model and Reality: Bridging Mathematical Frameworks with Real-World Infectious Disease Dynamics",
    "section": "References",
    "text": "References\n\n\nAdam, David. 2020. “Special Report: The Simulations Driving the World’s Response to COVID-19.” Nature 580 (7803): 316–18. https://doi.org/10.1038/d41586-020-01003-6.\n\n\nAnderson, Roy M., and Robert M. May. 1991. Infectious Diseases of Humans: Dynamics and Control. Oxford: Oxford University Press.\n\n\nBox, George E. P. 1987. Empirical Model-Building and Response Surfaces. New York: Wiley.\n\n\nFerguson, Neil M., Daniel Laydon, Gemma Nedjati-Gilani, Natsuko Imai, Kylie Ainslie, Marc Baguelin, Sangeeta Bhatia, et al. 2020. “Impact of Non-Pharmaceutical Interventions (NPIs) to Reduce COVID-19 Mortality and Healthcare Demand.” Imperial College COVID-19 Response Team. https://doi.org/10.25561/77482.\n\n\nFrigg, Roman, and Stephan Hartmann. 2020. “Models in Science.” Edited by Edward N. Zalta. The Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/entries/models-science/.\n\n\nHolmdahl, Inga, and Caroline Buckee. 2020. “Wrong but Useful — What Covid-19 Epidemiologic Models Can and Cannot Tell Us.” New England Journal of Medicine 383 (4): 303–5. https://doi.org/10.1056/NEJMp2016822.\n\n\nIoannidis, John P. A., Sally Cripps, and Martin A. Tanner. 2020. “Forecasting for COVID-19 Has Failed.” International Journal of Forecasting. https://doi.org/10.1016/j.ijforecast.2020.08.004."
  },
  {
    "objectID": "blog/posts/model_and_reality_kor/index.html",
    "href": "blog/posts/model_and_reality_kor/index.html",
    "title": "모델과 현실: 수학으로 감염병 확산을 읽다",
    "section": "",
    "text": "수학 모델로 감염병 확산을 예측한다. 현실과 얼마나 맞을까? 코로나19는 이 질문의 정답을 알려줬다."
  },
  {
    "objectID": "blog/posts/model_and_reality_kor/index.html#모델은-단순화된-현실이다",
    "href": "blog/posts/model_and_reality_kor/index.html#모델은-단순화된-현실이다",
    "title": "모델과 현실: 수학으로 감염병 확산을 읽다",
    "section": "모델은 단순화된 현실이다",
    "text": "모델은 단순화된 현실이다\n감염병 수학 모델은 현실의 복사본이 아니다. 그저 도구일 뿐이다.\n지하철 노선도를 생각해보자. 실제 지리를 정확히 반영하진 않는다. 하지만 길 찾기엔 충분하다. 감염병 모델도 마찬가지다. 복잡한 현실을 단순화해 핵심만 보여준다.\n가장 기본적인 감염병 모델은 SIR 모델이다. S는 감염 가능한 사람(Susceptible), I는 감염된 사람(Infected), R은 회복된 사람(Recovered)을 뜻한다. 이 세 집단이 시간에 따라 어떻게 변화하는지 보여준다.\n시간이 지날수록 S는 줄고, I는 늘었다가 줄고, R은 계속 늘어난다. 단순하다. 그러나 감염병 확산의 기본 패턴을 정확히 보여준다.\n\n\nCode\nlibrary(deSolve)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(hrbrthemes)\n\n# SIR 모델 정의\nsir_model &lt;- function(time, state, parameters) {\n  with(as.list(c(state, parameters)), {\n    dS &lt;- -beta * S * I\n    dI &lt;- beta * S * I - gamma * I\n    dR &lt;- gamma * I\n    return(list(c(dS, dI, dR)))\n  })\n}\n\n# 매개변수 설정\nparameters &lt;- c(beta = 0.3, gamma = 0.1)\ninitial_state &lt;- c(S = 0.99, I = 0.01, R = 0.0)\ntimes &lt;- seq(0, 100, by = 1)\n\n# 모델 실행\noutput &lt;- as.data.frame(ode(y = initial_state, times = times, \n                          func = sir_model, parms = parameters))\n\n# 데이터 형식 변환\noutput_long &lt;- pivot_longer(output, cols = c(\"S\", \"I\", \"R\"), \n                           names_to = \"상태\", values_to = \"비율\")\n\n# 표시할 레이블 설정\noutput_long$상태 &lt;- factor(output_long$상태, \n                         levels = c(\"S\", \"I\", \"R\"),\n                         labels = c(\"감염 가능\", \"감염됨\", \"회복됨\"))\n\n# 그래프 그리기\nggplot(output_long, aes(x = time, y = 비율, color = 상태)) +\n  geom_line(size = 1.2) +\n  labs(title = \"전형적인 감염병 확산 패턴\",\n       subtitle = \"SIR 모델에 기반한 시뮬레이션\",\n       x = \"시간 (일)\", \n       y = \"인구 비율\",\n       caption = expression(paste(\"기본 재생산 지수(\", R[0], \") = 3.0\"))) +\n  scale_color_manual(values = c(\"감염 가능\" = \"#3498db\", \n                               \"감염됨\" = \"#e74c3c\", \n                               \"회복됨\" = \"#2ecc71\")) +\n  theme_ipsum_rc() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        plot.title = element_text(face = \"bold\", size = 16),\n        plot.subtitle = element_text(size = 12),\n        axis.title = element_text(size = 12)) +\n  annotate(\"text\", x = 22, y = 0.25, label = \"최대 감염자 수\", \n           color = \"#e74c3c\", fontface = \"bold\") +\n  annotate(\"segment\", x = 22, y = 0.23, xend = 18, yend = 0.19,\n           arrow = arrow(length = unit(0.3, \"cm\")), color = \"#e74c3c\")\n\n\n\n\n\n기본 SIR 모델: 시간에 따른 감염 가능(S), 감염됨(I), 회복됨(R) 인구의 변화"
  },
  {
    "objectID": "blog/posts/model_and_reality_kor/index.html#직접-실험할-수-없을-때-모델이-답한다",
    "href": "blog/posts/model_and_reality_kor/index.html#직접-실험할-수-없을-때-모델이-답한다",
    "title": "모델과 현실: 수학으로 감염병 확산을 읽다",
    "section": "직접 실험할 수 없을 때, 모델이 답한다",
    "text": "직접 실험할 수 없을 때, 모델이 답한다\n“학교를 2주 닫으면 코로나 확산이 얼마나 줄어들까?”\n이런 질문에 답하려면 어떻게 해야 할까? 실제로 학교를 닫았다 열었다 실험할 수는 없다. 위험하다. 비용도 많이 든다.\n모델은 이런 가상 실험을 가능하게 한다. 컴퓨터 속에서 다양한 상황을 시뮬레이션 할 수 있다. 철학자들은 이를 “대리 추론”(surrogate reasoning)이라고 부른다. 현실 대신 모델로 추론하는 것이다.\n\n\nCode\nlibrary(deSolve)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# 감염병 모델 (거리두기 효과 포함)\nintervention_model &lt;- function(time, state, parameters) {\n  with(as.list(c(state, parameters)), {\n    beta_t &lt;- beta * intervention_effect(time, int_start, int_duration, int_strength)\n    dS &lt;- -beta_t * S * I\n    dI &lt;- beta_t * S * I - gamma * I\n    dR &lt;- gamma * I\n    return(list(c(dS, dI, dR)))\n  })\n}\n\n# 개입 효과 함수\nintervention_effect &lt;- function(time, start, duration, strength) {\n  if (time &gt;= start && time &lt; start + duration) {\n    return(1 - strength)  # 개입 기간 중 효과\n  } else {\n    return(1)  # 개입 없음\n  }\n}\n\n# 시나리오 생성 함수\ngenerate_scenario &lt;- function(int_strength, label) {\n  parameters &lt;- c(beta = 0.3, gamma = 0.1, \n                 int_start = 20, int_duration = 30, \n                 int_strength = int_strength)\n  \n  initial_state &lt;- c(S = 0.99, I = 0.01, R = 0.0)\n  times &lt;- seq(0, 100, by = 1)\n  \n  output &lt;- as.data.frame(ode(y = initial_state, times = times, \n                            func = intervention_model, parms = parameters))\n  output$scenario &lt;- label\n  return(output)\n}\n\n# 시나리오 실행\nno_intervention &lt;- generate_scenario(0, \"조치 없음\")\nmild_intervention &lt;- generate_scenario(0.3, \"약한 거리두기\")\nmoderate_intervention &lt;- generate_scenario(0.6, \"중간 거리두기\")\nstrong_intervention &lt;- generate_scenario(0.8, \"강한 거리두기\")\n\n# 데이터 결합\nall_scenarios &lt;- bind_rows(no_intervention, mild_intervention,\n                          moderate_intervention, strong_intervention)\n\n# 그래프 그리기\nggplot(all_scenarios, aes(x = time, y = I, color = scenario, linetype = scenario)) +\n  geom_line(size = 1.2) +\n  labs(title = \"사회적 거리두기 강도에 따른 감염 곡선 변화\",\n       subtitle = \"거리두기 개입은 20일차에 시작하여 30일간 지속\",\n       x = \"시간 (일)\", \n       y = \"감염자 비율\",\n       caption = \"기본 재생산 지수(R₀) = 3.0\") +\n  scale_color_manual(values = c(\"조치 없음\" = \"#e74c3c\", \n                               \"약한 거리두기\" = \"#f39c12\", \n                               \"중간 거리두기\" = \"#3498db\", \n                               \"강한 거리두기\" = \"#2ecc71\")) +\n  scale_linetype_manual(values = c(\"조치 없음\" = \"solid\", \n                                  \"약한 거리두기\" = \"dashed\", \n                                  \"중간 거리두기\" = \"dotdash\", \n                                  \"강한 거리두기\" = \"longdash\")) +\n  theme_ipsum_rc() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        plot.title = element_text(face = \"bold\", size = 16),\n        plot.subtitle = element_text(size = 12),\n        axis.title = element_text(size = 12)) +\n  annotate(\"rect\", xmin = 20, xmax = 50, ymin = -Inf, ymax = Inf,\n           alpha = 0.1, fill = \"gray50\") +\n  annotate(\"text\", x = 35, y = 0.01, label = \"개입 기간\", \n           fontface = \"italic\", color = \"gray50\")\n\n\n\n\n\n코로나19 개입 효과 비교: 다른 강도의 사회적 거리두기가 감염 곡선에 미치는 영향"
  },
  {
    "objectID": "blog/posts/model_and_reality_kor/index.html#코로나19는-모델의-강점과-한계를-동시에-보여줬다",
    "href": "blog/posts/model_and_reality_kor/index.html#코로나19는-모델의-강점과-한계를-동시에-보여줬다",
    "title": "모델과 현실: 수학으로 감염병 확산을 읽다",
    "section": "코로나19는 모델의 강점과 한계를 동시에 보여줬다",
    "text": "코로나19는 모델의 강점과 한계를 동시에 보여줬다\n코로나19 초기, 모델은 중요한 역할을 했다. 병원 수용 능력을 예측했다. 사회적 거리두기(social distancing)의 효과를 추정했다. 정책 결정에 도움을 줬다.\n그러나 예측이 빗나간 경우도 많았다. 이유는 여러 가지다. 바이러스에 대한 정보가 부족했다. 사람들의 행동 변화를 예측하기 어려웠다. 새로운 변이가 등장했다.\n영국 임페리얼 칼리지(Imperial College)의 모델은 초기에 큰 주목을 받았다. 아무런 조치를 취하지 않으면 영국에서만 51만 명이 사망할 수 있다고 예측했다. 실제로는 그보다 훨씬 적은 사람이 사망했다. 정부의 개입과 사람들의 행동 변화 때문이었다.\n모델은 틀렸지만, 정책 결정에 영향을 미쳤다. 유용했다.\n\n\nCode\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(dplyr)\n\n# 가상 데이터 생성 (실제 데이터를 시뮬레이션)\nset.seed(123)\n\n# 날짜 생성\ndates &lt;- seq(as.Date(\"2020-03-01\"), as.Date(\"2020-06-30\"), by = \"day\")\n\n# 초기 예측 모델 (과대 예측)\ninitial_model &lt;- 100 * exp(0.15 * 1:length(dates))\ninitial_model[initial_model &gt; 20000] &lt;- 20000 + (initial_model[initial_model &gt; 20000] - 20000) * 0.1\ninitial_model &lt;- pmin(initial_model, 25000)\n\n# 업데이트된 모델 (더 정확함)\nupdated_model &lt;- 50 * exp(0.12 * 1:length(dates))\nupdated_model &lt;- pmin(updated_model, 15000)\n\n# 실제 사례 (락다운 효과 반영)\nactual_cases &lt;- 50 * exp(0.18 * 1:40)\nlockdown_effect &lt;- exp(-0.03 * 1:(length(dates) - 40))\nactual_cases &lt;- c(actual_cases, actual_cases[40] * lockdown_effect)\nactual_cases &lt;- actual_cases + rnorm(length(dates), 0, actual_cases * 0.05)\nactual_cases &lt;- pmax(0, actual_cases)\n\n# 데이터프레임 만들기\nmodel_data &lt;- data.frame(\n  date = rep(dates, 3),\n  cases = c(initial_model, updated_model, actual_cases),\n  type = factor(rep(c(\"초기 모델 (3월 1일)\", \"업데이트된 모델 (4월 1일)\", \"실제 사례\"), each = length(dates)))\n)\n\n# 락다운 시작일\nlockdown_date &lt;- as.Date(\"2020-04-10\")\n\n# 그래프 그리기\nggplot(model_data, aes(x = date, y = cases, color = type, linetype = type)) +\n  geom_line(size = 1.2) +\n  labs(title = \"코로나19 모델 예측과 실제 확진자 수 비교\",\n       subtitle = \"초기 모델은 과대 예측, 업데이트된 모델은 더 정확함\",\n       x = \"날짜\", \n       y = \"일일 확진자 수\",\n       caption = \"가상 데이터 기반 시뮬레이션\") +\n  scale_color_manual(values = c(\"초기 모델 (3월 1일)\" = \"#e74c3c\", \n                              \"업데이트된 모델 (4월 1일)\" = \"#3498db\", \n                              \"실제 사례\" = \"#2ecc71\")) +\n  scale_linetype_manual(values = c(\"초기 모델 (3월 1일)\" = \"dashed\", \n                                 \"업데이트된 모델 (4월 1일)\" = \"dotdash\", \n                                 \"실제 사례\" = \"solid\")) +\n  theme_ipsum_rc() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        plot.title = element_text(face = \"bold\", size = 16),\n        plot.subtitle = element_text(size = 12),\n        axis.title = element_text(size = 12)) +\n  geom_vline(xintercept = lockdown_date, linetype = \"dashed\", color = \"gray50\") +\n  annotate(\"text\", x = lockdown_date + days(5), y = 22000, \n           label = \"락다운 시작\", color = \"gray50\", fontface = \"italic\", hjust = 0)\n\n\n\n\n\n모델 예측과 실제 데이터 비교: 코로나19 첫 물결(가상 데이터)"
  },
  {
    "objectID": "blog/posts/model_and_reality_kor/index.html#더-나은-모델을-위한-다섯-가지-방법",
    "href": "blog/posts/model_and_reality_kor/index.html#더-나은-모델을-위한-다섯-가지-방법",
    "title": "모델과 현실: 수학으로 감염병 확산을 읽다",
    "section": "더 나은 모델을 위한 다섯 가지 방법",
    "text": "더 나은 모델을 위한 다섯 가지 방법\n모델을 현실에 더 가깝게 만들려면 어떻게 해야 할까?\n\n실증적 검증(empirical validation): 모델 결과를 실제 데이터와 지속적으로 비교한다.\n민감도 분석(sensitivity analysis): 매개변수(parameter) 변화가 결과에 어떤 영향을 미치는지 이해한다.\n앙상블 모델링(ensemble modeling): 여러 모델을 함께 사용해 더 균형 잡힌 전망을 얻는다.\n이질성 반영(incorporating heterogeneity): 연령, 지역, 행동 패턴 등 다양한 요소를 모델에 추가한다.\n학제 간 협력(interdisciplinary collaboration): 역학자(epidemiologist), 의사, 행동 과학자, 데이터 과학자가 함께 모델을 개발한다.\n\n현재 많은 연구팀이 이러한 방향으로 모델을 개선하고 있다. 하버드 대학의 연구팀은 모바일 데이터를 활용해 사람들의 이동 패턴을 모델에 반영했다. 더 정확한 예측이 가능해졌다.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggrepel)\n\n# 모델 유형별 데이터\nmodel_types &lt;- data.frame(\n  complexity = c(1, 2, 3, 5, 6, 8),\n  accuracy = c(0.4, 0.55, 0.7, 0.8, 0.83, 0.85),\n  type = c(\"SIR\", \"SEIR\", \"SEIR+\", \"연령 구조화 모델\", \"공간적 모델\", \"행동 적응형 모델\"),\n  features = c(\"기본 감염-회복 역학\", \n              \"잠복기 추가\", \n              \"무증상 감염자 추가\", \n              \"연령별 취약성과 접촉 패턴\", \n              \"지역 간 이동과 공간적 이질성\", \n              \"행동 변화와 정책 대응의 피드백 루프\")\n)\n\n# 과적합 곡선 데이터\nx_curve &lt;- seq(0.5, 10, length.out = 100)\naccuracy_curve &lt;- 0.9 * (1 - exp(-0.5 * x_curve))\noverfitting_curve &lt;- 0.9 * (1 - exp(-0.5 * x_curve)) - (0.3 * pmax(0, x_curve - 5)^2 / 30)\n\ncurve_data &lt;- data.frame(\n  complexity = c(x_curve, x_curve),\n  accuracy = c(accuracy_curve, overfitting_curve),\n  dataset = rep(c(\"이론적 정확도\", \"실제 성능\"), each = length(x_curve))\n)\n\n# 그래프 그리기\nggplot() +\n  # 곡선\n  geom_line(data = curve_data, \n           aes(x = complexity, y = accuracy, linetype = dataset, color = dataset),\n           size = 1) +\n  # 모델 유형\n  geom_point(data = model_types, \n            aes(x = complexity, y = accuracy),\n            size = 4, color = \"#3498db\") +\n  # 모델 레이블\n  geom_text_repel(data = model_types,\n                 aes(x = complexity, y = accuracy, label = type),\n                 box.padding = 0.5, point.padding = 0.5,\n                 force = 2, seed = 42) +\n  # 라벨과 제목\n  labs(title = \"감염병 모델의 복잡성과 정확도\",\n       subtitle = \"복잡한 모델이 항상 더 좋은 예측을 제공하지는 않음\",\n       x = \"모델 복잡성\", \n       y = \"예측 정확도\",\n       caption = \"실제 성능은 과적합으로 인해 복잡성이 증가함에 따라 감소할 수 있음\") +\n  scale_color_manual(values = c(\"이론적 정확도\" = \"#2ecc71\", \"실제 성능\" = \"#e74c3c\")) +\n  scale_linetype_manual(values = c(\"이론적 정확도\" = \"solid\", \"실제 성능\" = \"dashed\")) +\n  scale_x_continuous(breaks = c(1, 2, 3, 5, 6, 8)) +\n  scale_y_continuous(labels = scales::percent) +\n  theme_ipsum_rc() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        plot.title = element_text(face = \"bold\", size = 16),\n        plot.subtitle = element_text(size = 12),\n        axis.title = element_text(size = 12)) +\n  # 주석\n  annotate(\"text\", x = 7.5, y = 0.55, \n           label = \"과적합 위험 영역\", \n           color = \"#e74c3c\", fontface = \"italic\", hjust = 0) +\n  annotate(\"text\", x = 2, y = 0.8, \n           label = \"모델 복잡성에 따른\\n예측 정확도 향상\", \n           color = \"#2ecc71\", fontface = \"italic\")\n\n\n\n\n\n감염병 모델 개선 과정: 모델 복잡성과 정확도 사이의 관계"
  },
  {
    "objectID": "blog/posts/model_and_reality_kor/index.html#모델은-현실이-아니지만-현실을-이해하는-도구다",
    "href": "blog/posts/model_and_reality_kor/index.html#모델은-현실이-아니지만-현실을-이해하는-도구다",
    "title": "모델과 현실: 수학으로 감염병 확산을 읽다",
    "section": "모델은 현실이 아니지만, 현실을 이해하는 도구다",
    "text": "모델은 현실이 아니지만, 현실을 이해하는 도구다\n통계학자 조지 박스(George Box)는 말했다. “모든 모델은 틀렸지만, 일부는 유용하다 (All models are wrong, but some are useful).”\n감염병 모델도 마찬가지다. 완벽하지 않다. 그러나 복잡한 현실을 이해하는 데 도움을 준다. 가능한 미래를 탐색하게 해준다. 정책 결정에 정보를 제공한다.\n중요한 것은 모델의 한계를 인정하는 것이다. 모델 결과를 절대적 진리로 받아들이면 안 된다. 하나의 참고 자료로 봐야 한다.\n모델과 현실 사이의 간격은 항상 존재할 것이다. 그러나 그 간격을 좁히려는 노력은 계속되어야 한다. 다음 감염병이 찾아왔을 때, 우리는 더 준비되어 있을 수 있다.\n그때 우리는 다시 물을 것이다. “이 모델은 현실을 얼마나 잘 반영하고 있는가?”"
  }
]